{
  
    
        "post0": {
            "title": "Keras CNN - CIFAR-10 Multiple Classification",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt . from tensorflow.keras.datasets import cifar10 (X_train, y_train), (X_test, y_test) = cifar10.load_data() . X_train.shape . (50000, 32, 32, 3) . plt.imshow(X_train[4]); . Normalise Data . X_train.max(), . (255,) . X_train = X_train/255 X_test = X_test/255 . Label Data . from tensorflow.keras.utils import to_categorical . y_train.max() . 9 . y_train = to_categorical(y_train, 10) y_test = to_categorical(y_test , 10) . Build Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten . model = Sequential() model.add(Conv2D(filters=32, kernel_size=(4,4), activation=&#39;relu&#39;, input_shape=(32,32,3))) model.add(MaxPool2D(2,2)) model.add(Conv2D(filters=32, kernel_size=(4,4), activation=&#39;relu&#39;, input_shape=(32,32,3))) model.add(MaxPool2D(2,2)) model.add(Flatten()) model.add(Dense(256, activation=&#39;relu&#39;)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_4 (Conv2D) (None, 29, 29, 32) 1568 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 11, 11, 32) 16416 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 800) 0 _________________________________________________________________ dense_4 (Dense) (None, 256) 205056 _________________________________________________________________ dense_5 (Dense) (None, 10) 2570 ================================================================= Total params: 225,610 Trainable params: 225,610 Non-trainable params: 0 _________________________________________________________________ . from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor=&#39;val_loss&#39;, patience=4) . model.fit(X_train, y_train, epochs=20, batch_size=516, validation_data=(X_test, y_test), callbacks = [early_stop], verbose= 1) . Train on 50000 samples, validate on 10000 samples Epoch 1/20 50000/50000 [==============================] - 26s 526us/sample - loss: 1.9519 - accuracy: 0.3024 - val_loss: 2.1164 - val_accuracy: 0.2693 Epoch 2/20 50000/50000 [==============================] - 24s 471us/sample - loss: 1.6544 - accuracy: 0.4150 - val_loss: 1.6044 - val_accuracy: 0.4418 Epoch 3/20 50000/50000 [==============================] - 22s 450us/sample - loss: 1.5079 - accuracy: 0.4694 - val_loss: 1.3855 - val_accuracy: 0.5176 Epoch 4/20 50000/50000 [==============================] - 24s 479us/sample - loss: 1.4067 - accuracy: 0.5047 - val_loss: 1.3563 - val_accuracy: 0.5168 Epoch 5/20 50000/50000 [==============================] - 24s 474us/sample - loss: 1.3298 - accuracy: 0.5361 - val_loss: 1.2736 - val_accuracy: 0.5526 Epoch 6/20 50000/50000 [==============================] - 24s 475us/sample - loss: 1.2627 - accuracy: 0.5588 - val_loss: 1.2547 - val_accuracy: 0.5565 Epoch 7/20 50000/50000 [==============================] - 23s 460us/sample - loss: 1.2135 - accuracy: 0.5789 - val_loss: 1.1838 - val_accuracy: 0.5852 Epoch 8/20 50000/50000 [==============================] - 23s 460us/sample - loss: 1.1594 - accuracy: 0.5952 - val_loss: 1.1685 - val_accuracy: 0.5939 Epoch 9/20 50000/50000 [==============================] - 24s 471us/sample - loss: 1.1193 - accuracy: 0.6114 - val_loss: 1.1167 - val_accuracy: 0.6154 Epoch 10/20 50000/50000 [==============================] - 30s 597us/sample - loss: 1.0799 - accuracy: 0.6241 - val_loss: 1.0816 - val_accuracy: 0.6262 Epoch 11/20 50000/50000 [==============================] - 27s 540us/sample - loss: 1.0402 - accuracy: 0.6414 - val_loss: 1.0576 - val_accuracy: 0.6281 Epoch 12/20 50000/50000 [==============================] - 23s 468us/sample - loss: 1.0041 - accuracy: 0.6527 - val_loss: 1.0503 - val_accuracy: 0.6382 Epoch 13/20 50000/50000 [==============================] - 24s 471us/sample - loss: 0.9675 - accuracy: 0.6637 - val_loss: 1.0547 - val_accuracy: 0.6341 Epoch 14/20 50000/50000 [==============================] - 23s 470us/sample - loss: 0.9371 - accuracy: 0.6747 - val_loss: 1.0385 - val_accuracy: 0.6376 Epoch 15/20 50000/50000 [==============================] - 23s 465us/sample - loss: 0.8996 - accuracy: 0.6883 - val_loss: 1.0758 - val_accuracy: 0.6306 Epoch 16/20 50000/50000 [==============================] - 23s 459us/sample - loss: 0.8726 - accuracy: 0.7005 - val_loss: 1.1459 - val_accuracy: 0.6130 Epoch 17/20 50000/50000 [==============================] - 23s 469us/sample - loss: 0.8357 - accuracy: 0.7116 - val_loss: 1.0751 - val_accuracy: 0.6276 Epoch 18/20 50000/50000 [==============================] - 22s 440us/sample - loss: 0.8131 - accuracy: 0.7188 - val_loss: 0.9483 - val_accuracy: 0.6722 Epoch 19/20 50000/50000 [==============================] - 23s 466us/sample - loss: 0.7866 - accuracy: 0.7296 - val_loss: 0.9500 - val_accuracy: 0.6725 Epoch 20/20 50000/50000 [==============================] - 24s 471us/sample - loss: 0.7566 - accuracy: 0.7388 - val_loss: 1.0179 - val_accuracy: 0.6590 . &lt;tensorflow.python.keras.callbacks.History at 0x7f981b631510&gt; . losses = pd.DataFrame(model.history.history) . losses[[&#39;loss&#39;, &#39;val_loss&#39;]].plot() . &lt;AxesSubplot:&gt; . losses[[&#39;accuracy&#39;, &#39;val_accuracy&#39;]].plot() . &lt;AxesSubplot:&gt; . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . print(model.metrics_names) print(model.evaluate(X_test, y_test, verbose=0)) . [&#39;loss&#39;, &#39;accuracy&#39;] [1.0178816331863403, 0.659] . from sklearn.metrics import classification_report, confusion_matrix . X_test.shape . (10000, 32, 32, 3) . predictions = model.predict_classes(X_test) . predictions . array([3, 8, 8, ..., 5, 4, 7]) . y_test . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 0., 0., ..., 0., 1., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 1., 0., 0.]], dtype=float32) . y_test = y_test.argmax(axis=1) . y_test . array([3, 8, 8, ..., 5, 1, 7]) . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.83 0.66 0.74 1000 1 0.90 0.61 0.73 1000 2 0.68 0.44 0.53 1000 3 0.45 0.57 0.51 1000 4 0.53 0.71 0.61 1000 5 0.54 0.58 0.56 1000 6 0.67 0.81 0.74 1000 7 0.81 0.63 0.71 1000 8 0.79 0.76 0.78 1000 9 0.65 0.82 0.73 1000 accuracy 0.66 10000 macro avg 0.69 0.66 0.66 10000 weighted avg 0.69 0.66 0.66 10000 . confusion_matrix(y_test, predictions) . array([[660, 14, 41, 40, 49, 18, 25, 11, 77, 65], [ 15, 607, 9, 30, 17, 17, 37, 3, 50, 215], [ 46, 2, 437, 112, 156, 95, 99, 29, 10, 14], [ 5, 4, 41, 572, 98, 159, 76, 16, 10, 19], [ 6, 2, 40, 84, 714, 35, 62, 40, 11, 6], [ 5, 2, 18, 216, 86, 580, 46, 35, 3, 9], [ 0, 1, 24, 72, 55, 22, 810, 8, 2, 6], [ 7, 0, 13, 65, 131, 107, 13, 631, 4, 29], [ 34, 18, 13, 33, 32, 20, 17, 2, 759, 72], [ 15, 22, 10, 36, 21, 19, 19, 8, 30, 820]]) . conf_matrix = confusion_matrix(y_test, predictions) * (np.ones((10,10)) - np.eye(10)) import seaborn as sns plt.figure(figsize=(12,8)) sns.heatmap(conf_matrix, annot=True); . Predicing a specific image . new_image = X_test[8] plt.imshow(new_image); . model.predict_classes(new_image.reshape(1,32,32,3)) . array([3]) .",
            "url": "https://sams101.github.io/DataScience/2020/10/04/Keras_CNN_CIFAR10-multiclassification.html",
            "relUrl": "/2020/10/04/Keras_CNN_CIFAR10-multiclassification.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Keras CNN - MNIST number classification",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt . from tensorflow.keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() . Visualizing the Image Data . x_train.shape . (60000, 28, 28) . single_image = x_train[0] . single_image.shape . (28, 28) . plt.imshow(single_image) . &lt;matplotlib.image.AxesImage at 0x7fca5dd7a790&gt; . PreProcess . Label Data . y_train . array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) . y_test . array([7, 2, 1, ..., 4, 5, 6], dtype=uint8) . from tensorflow.keras.utils import to_categorical . y_train.shape . (60000,) . y_example = to_categorical(y_train) . y_example.shape . (60000, 10) . y_example[0] . array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32) . y_cat_test = to_categorical(y_test, 10) y_cat_train = to_categorical(y_train,10) . Feature Data . # normalize X data single_image.max(), . (255,) . single_image.min() . 0 . x_train = x_train/255 x_test = x_test/255 . scaled_single = x_train[0] . scaled_single.max() . 1.0 . plt.imshow(scaled_single) . &lt;matplotlib.image.AxesImage at 0x7fca5de238d0&gt; . Reshape Data . x_train.shape . (60000, 28, 28) . x_test.shape . (10000, 28, 28) . # Reshape to include channel dimension x_train = x_train.reshape(60000, 28, 28, 1) x_train.shape . (60000, 28, 28, 1) . x_test = x_test.reshape(10000, 28, 28, 1) x_test.shape . (10000, 28, 28, 1) . Training the Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten import tensorflow as tf #import os #os.environ[&#39;KMP_DUPLICATE_LIB_OK&#39;]=&#39;True&#39; . model = Sequential() model.add(Conv2D(filters=16, kernel_size=(4,4), activation=&#39;relu&#39;,input_shape=(28,28,1))) model.add(MaxPool2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(20, activation=&#39;relu&#39;)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 25, 25, 16) 272 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 12, 12, 16) 0 _________________________________________________________________ flatten (Flatten) (None, 2304) 0 _________________________________________________________________ dense (Dense) (None, 20) 46100 _________________________________________________________________ dense_1 (Dense) (None, 10) 210 ================================================================= Total params: 46,582 Trainable params: 46,582 Non-trainable params: 0 _________________________________________________________________ . from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor = &#39;val_loss&#39;, patience=2) . Save and Load data and model . from numpy import save from numpy import load from tensorflow.keras.models import load_model save(&#39;x_train.npy&#39;, x_train) save(&#39;y_cat_train.npy&#39;, y_cat_train) model.save(&#39;my_model.h5&#39;) . load(&#39;x_train.npy&#39;) load(&#39;y_cat_train.npy&#39;); model = load_model(&#39;my_model.h5&#39;) . Train Model . model.fit(x_train, y_cat_train, epochs=30, batch_size=256, validation_data=(x_test, y_cat_test), verbose = 1) . Train on 60000 samples, validate on 10000 samples Epoch 1/30 60000/60000 [==============================] - 10s 168us/sample - loss: 0.5176 - accuracy: 0.8474 - val_loss: 0.1848 - val_accuracy: 0.9462 Epoch 2/30 60000/60000 [==============================] - 9s 142us/sample - loss: 0.1605 - accuracy: 0.9535 - val_loss: 0.1215 - val_accuracy: 0.9641 Epoch 3/30 60000/60000 [==============================] - 9s 148us/sample - loss: 0.1113 - accuracy: 0.9681 - val_loss: 0.0949 - val_accuracy: 0.9711 Epoch 4/30 60000/60000 [==============================] - 9s 150us/sample - loss: 0.0909 - accuracy: 0.9740 - val_loss: 0.0809 - val_accuracy: 0.9746 Epoch 5/30 60000/60000 [==============================] - 9s 148us/sample - loss: 0.0748 - accuracy: 0.9782 - val_loss: 0.0695 - val_accuracy: 0.9786 Epoch 6/30 60000/60000 [==============================] - 9s 157us/sample - loss: 0.0643 - accuracy: 0.9814 - val_loss: 0.0632 - val_accuracy: 0.9790 Epoch 7/30 60000/60000 [==============================] - 9s 156us/sample - loss: 0.0575 - accuracy: 0.9832 - val_loss: 0.0569 - val_accuracy: 0.9814 Epoch 8/30 60000/60000 [==============================] - 10s 171us/sample - loss: 0.0524 - accuracy: 0.9846 - val_loss: 0.0590 - val_accuracy: 0.9815 Epoch 9/30 60000/60000 [==============================] - 11s 179us/sample - loss: 0.0486 - accuracy: 0.9859 - val_loss: 0.0548 - val_accuracy: 0.9824 Epoch 10/30 60000/60000 [==============================] - 11s 179us/sample - loss: 0.0425 - accuracy: 0.9873 - val_loss: 0.0523 - val_accuracy: 0.9832 Epoch 11/30 60000/60000 [==============================] - 14s 228us/sample - loss: 0.0406 - accuracy: 0.9884 - val_loss: 0.0519 - val_accuracy: 0.9821 Epoch 12/30 60000/60000 [==============================] - 11s 187us/sample - loss: 0.0366 - accuracy: 0.9893 - val_loss: 0.0490 - val_accuracy: 0.9827 Epoch 13/30 60000/60000 [==============================] - 12s 196us/sample - loss: 0.0347 - accuracy: 0.9903 - val_loss: 0.0472 - val_accuracy: 0.9836 Epoch 14/30 60000/60000 [==============================] - 13s 214us/sample - loss: 0.0319 - accuracy: 0.9905 - val_loss: 0.0526 - val_accuracy: 0.9828 Epoch 15/30 60000/60000 [==============================] - 13s 212us/sample - loss: 0.0299 - accuracy: 0.9911 - val_loss: 0.0467 - val_accuracy: 0.9844 Epoch 16/30 60000/60000 [==============================] - 10s 165us/sample - loss: 0.0278 - accuracy: 0.9918 - val_loss: 0.0487 - val_accuracy: 0.9841 Epoch 17/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0257 - accuracy: 0.9926 - val_loss: 0.0483 - val_accuracy: 0.9840 Epoch 18/30 60000/60000 [==============================] - 12s 193us/sample - loss: 0.0246 - accuracy: 0.9929 - val_loss: 0.0475 - val_accuracy: 0.9837 Epoch 19/30 60000/60000 [==============================] - 13s 211us/sample - loss: 0.0229 - accuracy: 0.9933 - val_loss: 0.0523 - val_accuracy: 0.9828 Epoch 20/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0210 - accuracy: 0.9941 - val_loss: 0.0470 - val_accuracy: 0.9843 Epoch 21/30 60000/60000 [==============================] - 10s 174us/sample - loss: 0.0208 - accuracy: 0.9937 - val_loss: 0.0488 - val_accuracy: 0.9829 Epoch 22/30 60000/60000 [==============================] - 12s 195us/sample - loss: 0.0183 - accuracy: 0.9948 - val_loss: 0.0524 - val_accuracy: 0.9840 Epoch 23/30 60000/60000 [==============================] - 11s 177us/sample - loss: 0.0178 - accuracy: 0.9949 - val_loss: 0.0538 - val_accuracy: 0.9830 Epoch 24/30 60000/60000 [==============================] - 11s 178us/sample - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.0482 - val_accuracy: 0.9840 Epoch 25/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0159 - accuracy: 0.9957 - val_loss: 0.0527 - val_accuracy: 0.9838 Epoch 26/30 60000/60000 [==============================] - 10s 173us/sample - loss: 0.0140 - accuracy: 0.9962 - val_loss: 0.0532 - val_accuracy: 0.9833 Epoch 27/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.0513 - val_accuracy: 0.9847 Epoch 28/30 60000/60000 [==============================] - 10s 170us/sample - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0584 - val_accuracy: 0.9822 Epoch 29/30 60000/60000 [==============================] - 11s 179us/sample - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.0480 - val_accuracy: 0.9856 Epoch 30/30 60000/60000 [==============================] - 11s 178us/sample - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.0512 - val_accuracy: 0.9851 . &lt;tensorflow.python.keras.callbacks.History at 0x7fca5ae89bd0&gt; . Evaluate the Model . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . losses = pd.DataFrame(model.history.history) . losses[[&#39;loss&#39;,&#39;val_loss&#39;]].plot() . &lt;AxesSubplot:&gt; . losses[[&#39;accuracy&#39;,&#39;val_accuracy&#39;]].plot() . &lt;AxesSubplot:&gt; . print(model.metrics_names) print(model.evaluate(x_test, y_cat_test, verbose=0)) . [&#39;loss&#39;, &#39;accuracy&#39;] [0.05124218984251929, 0.9851] . from sklearn.metrics import classification_report, confusion_matrix . predictions = model.predict_classes(x_test) . predictions[0] . 7 . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.98 0.99 0.99 980 1 0.99 1.00 0.99 1135 2 0.98 0.98 0.98 1032 3 0.98 0.99 0.99 1010 4 0.99 0.99 0.99 982 5 0.98 0.99 0.98 892 6 0.99 0.98 0.99 958 7 0.97 0.99 0.98 1028 8 0.99 0.98 0.98 974 9 0.99 0.96 0.98 1009 accuracy 0.99 10000 macro avg 0.99 0.98 0.98 10000 weighted avg 0.99 0.99 0.99 10000 . print(confusion_matrix(y_test, predictions)) . [[ 974 0 2 0 0 0 2 2 0 0] [ 0 1130 2 0 0 1 1 1 0 0] [ 2 2 1012 3 0 0 2 8 2 1] [ 0 0 1 1001 0 4 0 2 2 0] [ 0 1 0 0 972 0 2 1 0 6] [ 1 0 1 6 0 881 3 0 0 0] [ 5 2 1 0 3 5 941 0 1 0] [ 0 1 4 3 1 0 0 1017 1 1] [ 6 0 4 2 0 4 1 4 950 3] [ 2 2 1 5 10 6 0 9 1 973]] . import seaborn as sns . plt.figure(figsize=(12,8)) sns.heatmap(confusion_matrix(y_test, predictions), annot=True); . Predict a given image . new_num = x_test[4] . plt.imshow(new_num.reshape(28,28)); . model.predict(new_num.reshape(1,28,28,1)) . array([[1.5632841e-10, 5.4694190e-11, 8.7363444e-10, 4.7918874e-11, 9.9992108e-01, 3.0790169e-11, 7.6901131e-11, 2.0181250e-07, 3.2277640e-07, 7.8314668e-05]], dtype=float32) . model.predict(new_num.reshape(1,28,28,1)).argmax() . 4 . # or you can use this model.predict_classes(new_num.reshape(1,28,28,1)) . array([4]) .",
            "url": "https://sams101.github.io/DataScience/2020/10/04/Keras_CNN-MNIST.html",
            "relUrl": "/2020/10/04/Keras_CNN-MNIST.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Keras CNN - Fashion_MNIST - Multiclass",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt . from tensorflow.keras.datasets import fashion_mnist (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data() . X_train.shape . (60000, 28, 28) . plt.imshow(X_train[0]); . y_train[0] . 9 . PreProcess Data . X_train.max() . 255 . # Normalise Data X_train = X_train / 255 X_test = X_test / 255 . X_train.shape, X_test.shape . ((60000, 28, 28), (10000, 28, 28)) . # Reshape data X_train = X_train.reshape(60000, 28, 28, 1) X_test = X_test.reshape(10000, 28, 28, 1) . # One Hot encode Lables y_train . array([9, 0, 0, ..., 3, 0, 5], dtype=uint8) . from tensorflow.keras.utils import to_categorical y_train = to_categorical(y_train) y_test = to_categorical(y_test) . y_train . array([[0., 0., 0., ..., 0., 0., 1.], [1., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) . Build Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten . model = Sequential() model.add(Conv2D(filters=32, kernel_size=(4,4), activation=&#39;relu&#39;, input_shape=(28,28,1))) model.add(MaxPool2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer= &#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_6 (Conv2D) (None, 25, 25, 32) 544 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32) 0 _________________________________________________________________ flatten_4 (Flatten) (None, 4608) 0 _________________________________________________________________ dense_8 (Dense) (None, 128) 589952 _________________________________________________________________ dense_9 (Dense) (None, 10) 1290 ================================================================= Total params: 591,786 Trainable params: 591,786 Non-trainable params: 0 _________________________________________________________________ . # Early Stopping from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor=&#39;val_loss&#39;, patience=3) . model.fit(X_train, y_train, epochs=30, batch_size=128, validation_data=(X_test, y_test), callbacks = [early_stop], verbose=2) . Train on 60000 samples, validate on 10000 samples Epoch 1/30 60000/60000 - 25s - loss: 0.4568 - accuracy: 0.8421 - val_loss: 0.3588 - val_accuracy: 0.8753 Epoch 2/30 60000/60000 - 25s - loss: 0.3125 - accuracy: 0.8899 - val_loss: 0.3288 - val_accuracy: 0.8840 Epoch 3/30 60000/60000 - 22s - loss: 0.2672 - accuracy: 0.9041 - val_loss: 0.2898 - val_accuracy: 0.8953 Epoch 4/30 60000/60000 - 23s - loss: 0.2373 - accuracy: 0.9141 - val_loss: 0.2881 - val_accuracy: 0.8928 Epoch 5/30 60000/60000 - 25s - loss: 0.2156 - accuracy: 0.9209 - val_loss: 0.2590 - val_accuracy: 0.9052 Epoch 6/30 60000/60000 - 24s - loss: 0.1948 - accuracy: 0.9295 - val_loss: 0.2723 - val_accuracy: 0.9052 Epoch 7/30 60000/60000 - 23s - loss: 0.1764 - accuracy: 0.9352 - val_loss: 0.2630 - val_accuracy: 0.9075 Epoch 8/30 60000/60000 - 24s - loss: 0.1605 - accuracy: 0.9420 - val_loss: 0.2649 - val_accuracy: 0.9099 . &lt;tensorflow.python.keras.callbacks.History at 0x7fb658208cd0&gt; . losses = pd.DataFrame(model.history.history) . losses[[&#39;loss&#39;,&#39;val_loss&#39;]].plot() . &lt;AxesSubplot:&gt; . losses[[&#39;accuracy&#39;,&#39;val_accuracy&#39;]].plot() . &lt;AxesSubplot:&gt; . Evaluate Model . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . model.evaluate(X_test, y_test, verbose=0) . [0.2649084388911724, 0.9099] . from sklearn.metrics import classification_report, confusion_matrix . predictions = model.predict_classes(X_test) predictions . array([9, 2, 1, ..., 8, 1, 5]) . y_test . array([[0., 0., 0., ..., 0., 0., 1.], [0., 0., 1., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 1., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) . y_test.argmax(axis=1) . array([9, 2, 1, ..., 8, 1, 5]) . print(classification_report(y_test.argmax(axis=1), predictions)) . precision recall f1-score support 0 0.80 0.91 0.85 1000 1 0.97 0.99 0.98 1000 2 0.86 0.88 0.87 1000 3 0.91 0.93 0.92 1000 4 0.85 0.87 0.86 1000 5 0.99 0.96 0.97 1000 6 0.82 0.63 0.71 1000 7 0.94 0.98 0.96 1000 8 0.98 0.98 0.98 1000 9 0.98 0.96 0.97 1000 accuracy 0.91 10000 macro avg 0.91 0.91 0.91 10000 weighted avg 0.91 0.91 0.91 10000 . print(confusion_matrix(y_test.argmax(axis=1), predictions)) . [[912 1 7 13 4 1 56 1 5 0] [ 2 989 1 5 1 0 1 0 1 0] [ 23 2 875 10 54 1 34 0 1 0] [ 14 17 5 932 18 0 11 0 3 0] [ 1 3 56 33 871 1 34 0 1 0] [ 0 0 0 0 0 962 0 29 0 9] [178 3 72 30 79 0 631 0 7 0] [ 0 0 0 0 0 2 0 984 0 14] [ 5 0 3 4 0 1 0 3 984 0] [ 0 0 0 0 0 6 0 34 1 959]] . Predict specific object . plt.imshow(X_test[0]); . X_test[0].shape . (28, 28, 1) . model.predict(X_test[0].reshape(1,28,28,1)).argmax() # 9 is an Ankle Boot . 9 .",
            "url": "https://sams101.github.io/DataScience/2020/10/04/Keras-CNN-Fashion-MNIST-multiclass.html",
            "relUrl": "/2020/10/04/Keras-CNN-Fashion-MNIST-multiclass.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Keras ANNs Basics - Regression - Diamond Prices",
            "content": "import pandas as pd import seaborn as sns import matplotlib.pyplot as plt . df = pd.read_csv(&#39;DATA/fake_reg.csv&#39;) . df.head() . price feature1 feature2 . 0 461.527929 | 999.787558 | 999.766096 | . 1 548.130011 | 998.861615 | 1001.042403 | . 2 410.297162 | 1000.070267 | 998.844015 | . 3 540.382220 | 999.952251 | 1000.440940 | . 4 546.024553 | 1000.446011 | 1000.338531 | . sns.pairplot(df); . df.head(2) . price feature1 feature2 . 0 461.527929 | 999.787558 | 999.766096 | . 1 548.130011 | 998.861615 | 1001.042403 | . # Convert Pandas to Numpy for Keras from sklearn.model_selection import train_test_split # Features X = df[[&#39;feature1&#39;, &#39;feature2&#39;]].values # Label y = df[&#39;price&#39;].values # Slit X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) . X_train.shape . (700, 2) . y_test.shape . (300,) . # Normalise and Scale data from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() # Fit to Training data scaler.fit(X_train) # Transform Train and test data based on Fit X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) . import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation . # Create Model model = Sequential() model.add(Dense(4, activation=&#39;relu&#39;)) model.add(Dense(4, activation=&#39;relu&#39;)) model.add(Dense(4, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;mse&#39;) . model.fit(X_train, y_train, epochs=100); . Train on 700 samples Epoch 1/100 700/700 [==============================] - 1s 2ms/sample - loss: 256734.2171 Epoch 2/100 700/700 [==============================] - 0s 100us/sample - loss: 256585.3193 Epoch 3/100 700/700 [==============================] - 0s 99us/sample - loss: 256419.2663 Epoch 4/100 700/700 [==============================] - 0s 99us/sample - loss: 256235.6121s - loss: 256104.81 Epoch 5/100 700/700 [==============================] - 0s 90us/sample - loss: 256019.6986 Epoch 6/100 700/700 [==============================] - 0s 137us/sample - loss: 255769.5409 Epoch 7/100 700/700 [==============================] - 0s 120us/sample - loss: 255486.2454 Epoch 8/100 700/700 [==============================] - 0s 100us/sample - loss: 255165.6379 Epoch 9/100 700/700 [==============================] - 0s 135us/sample - loss: 254808.9467 Epoch 10/100 700/700 [==============================] - 0s 120us/sample - loss: 254418.8767 Epoch 11/100 700/700 [==============================] - 0s 117us/sample - loss: 253993.2737 Epoch 12/100 700/700 [==============================] - 0s 108us/sample - loss: 253532.9037 Epoch 13/100 700/700 [==============================] - 0s 127us/sample - loss: 253039.5647 Epoch 14/100 700/700 [==============================] - 0s 120us/sample - loss: 252513.3096 Epoch 15/100 700/700 [==============================] - 0s 116us/sample - loss: 251953.1364 Epoch 16/100 700/700 [==============================] - 0s 99us/sample - loss: 251354.9470 Epoch 17/100 700/700 [==============================] - 0s 123us/sample - loss: 250701.4620 Epoch 18/100 700/700 [==============================] - 0s 112us/sample - loss: 249991.8088 Epoch 19/100 700/700 [==============================] - 0s 120us/sample - loss: 249224.1071 Epoch 20/100 700/700 [==============================] - 0s 118us/sample - loss: 248392.8379 Epoch 21/100 700/700 [==============================] - 0s 131us/sample - loss: 247503.0635 Epoch 22/100 700/700 [==============================] - 0s 103us/sample - loss: 246550.7026 Epoch 23/100 700/700 [==============================] - 0s 93us/sample - loss: 245525.2023 Epoch 24/100 700/700 [==============================] - 0s 114us/sample - loss: 244424.7712 Epoch 25/100 700/700 [==============================] - 0s 120us/sample - loss: 243256.8364 Epoch 26/100 700/700 [==============================] - 0s 114us/sample - loss: 242005.4338 Epoch 27/100 700/700 [==============================] - 0s 124us/sample - loss: 240694.4196 Epoch 28/100 700/700 [==============================] - 0s 104us/sample - loss: 239295.4205 Epoch 29/100 700/700 [==============================] - 0s 119us/sample - loss: 237815.9690 Epoch 30/100 700/700 [==============================] - 0s 117us/sample - loss: 236250.1349 Epoch 31/100 700/700 [==============================] - 0s 123us/sample - loss: 234589.2504 Epoch 32/100 700/700 [==============================] - 0s 91us/sample - loss: 232837.4500 Epoch 33/100 700/700 [==============================] - 0s 123us/sample - loss: 230987.5181 Epoch 34/100 700/700 [==============================] - 0s 112us/sample - loss: 229022.9837 Epoch 35/100 700/700 [==============================] - 0s 117us/sample - loss: 226955.6740 Epoch 36/100 700/700 [==============================] - 0s 118us/sample - loss: 224790.2327 Epoch 37/100 700/700 [==============================] - 0s 127us/sample - loss: 222513.4288 Epoch 38/100 700/700 [==============================] - 0s 120us/sample - loss: 220123.7206 Epoch 39/100 700/700 [==============================] - 0s 111us/sample - loss: 217604.6604 Epoch 40/100 700/700 [==============================] - 0s 104us/sample - loss: 214968.2704 Epoch 41/100 700/700 [==============================] - 0s 120us/sample - loss: 212223.9364 Epoch 42/100 700/700 [==============================] - 0s 116us/sample - loss: 209351.5223 Epoch 43/100 700/700 [==============================] - 0s 119us/sample - loss: 206372.1771 Epoch 44/100 700/700 [==============================] - 0s 117us/sample - loss: 203244.3596 Epoch 45/100 700/700 [==============================] - 0s 124us/sample - loss: 200001.6553 Epoch 46/100 700/700 [==============================] - 0s 133us/sample - loss: 196621.6040 Epoch 47/100 700/700 [==============================] - 0s 119us/sample - loss: 193118.5028 Epoch 48/100 700/700 [==============================] - 0s 135us/sample - loss: 189493.1879 Epoch 49/100 700/700 [==============================] - 0s 146us/sample - loss: 185738.1114 Epoch 50/100 700/700 [==============================] - 0s 127us/sample - loss: 181815.4626 Epoch 51/100 700/700 [==============================] - 0s 139us/sample - loss: 177811.2594 Epoch 52/100 700/700 [==============================] - 0s 124us/sample - loss: 173674.9202 Epoch 53/100 700/700 [==============================] - 0s 132us/sample - loss: 169387.5123 Epoch 54/100 700/700 [==============================] - 0s 149us/sample - loss: 164988.9645 Epoch 55/100 700/700 [==============================] - 0s 134us/sample - loss: 160489.6636 Epoch 56/100 700/700 [==============================] - 0s 123us/sample - loss: 155834.9291 Epoch 57/100 700/700 [==============================] - 0s 113us/sample - loss: 151046.0179 Epoch 58/100 700/700 [==============================] - 0s 118us/sample - loss: 146175.0710 Epoch 59/100 700/700 [==============================] - 0s 125us/sample - loss: 141198.4020 Epoch 60/100 700/700 [==============================] - 0s 124us/sample - loss: 136109.4471 Epoch 61/100 700/700 [==============================] - 0s 124us/sample - loss: 130940.7521 Epoch 62/100 700/700 [==============================] - 0s 135us/sample - loss: 125713.8463 Epoch 63/100 700/700 [==============================] - 0s 108us/sample - loss: 120369.5925 Epoch 64/100 700/700 [==============================] - 0s 116us/sample - loss: 114954.3468 Epoch 65/100 700/700 [==============================] - 0s 120us/sample - loss: 109446.0565 Epoch 66/100 700/700 [==============================] - 0s 127us/sample - loss: 103925.0006 Epoch 67/100 700/700 [==============================] - 0s 131us/sample - loss: 98349.1309 Epoch 68/100 700/700 [==============================] - 0s 124us/sample - loss: 92735.8169 Epoch 69/100 700/700 [==============================] - 0s 123us/sample - loss: 87106.1582 Epoch 70/100 700/700 [==============================] - 0s 115us/sample - loss: 81484.0130 Epoch 71/100 700/700 [==============================] - 0s 120us/sample - loss: 75883.2828 Epoch 72/100 700/700 [==============================] - 0s 110us/sample - loss: 70311.0703 Epoch 73/100 700/700 [==============================] - 0s 124us/sample - loss: 64779.8148 Epoch 74/100 700/700 [==============================] - 0s 121us/sample - loss: 59317.3782 Epoch 75/100 700/700 [==============================] - 0s 114us/sample - loss: 53911.8225 Epoch 76/100 700/700 [==============================] - 0s 129us/sample - loss: 48680.9900 Epoch 77/100 700/700 [==============================] - 0s 125us/sample - loss: 43579.7131 Epoch 78/100 700/700 [==============================] - 0s 126us/sample - loss: 38609.0669 Epoch 79/100 700/700 [==============================] - 0s 127us/sample - loss: 33854.9086 Epoch 80/100 700/700 [==============================] - 0s 126us/sample - loss: 29302.7500 Epoch 81/100 700/700 [==============================] - 0s 115us/sample - loss: 24985.3489 Epoch 82/100 700/700 [==============================] - 0s 120us/sample - loss: 20966.3412 Epoch 83/100 700/700 [==============================] - 0s 120us/sample - loss: 17273.1389 Epoch 84/100 700/700 [==============================] - 0s 125us/sample - loss: 13918.6046 Epoch 85/100 700/700 [==============================] - 0s 120us/sample - loss: 10918.2614 Epoch 86/100 700/700 [==============================] - 0s 122us/sample - loss: 8299.2842 Epoch 87/100 700/700 [==============================] - 0s 123us/sample - loss: 6104.3133 Epoch 88/100 700/700 [==============================] - 0s 118us/sample - loss: 4426.2176 Epoch 89/100 700/700 [==============================] - 0s 113us/sample - loss: 3218.5811 Epoch 90/100 700/700 [==============================] - 0s 129us/sample - loss: 2481.1472 Epoch 91/100 700/700 [==============================] - 0s 130us/sample - loss: 2149.1669 Epoch 92/100 700/700 [==============================] - 0s 113us/sample - loss: 2050.6529 Epoch 93/100 700/700 [==============================] - 0s 129us/sample - loss: 2018.1331 Epoch 94/100 700/700 [==============================] - 0s 123us/sample - loss: 1985.4679 Epoch 95/100 700/700 [==============================] - 0s 122us/sample - loss: 1957.8603 Epoch 96/100 700/700 [==============================] - 0s 128us/sample - loss: 1924.4281 Epoch 97/100 700/700 [==============================] - 0s 129us/sample - loss: 1892.2963 Epoch 98/100 700/700 [==============================] - 0s 134us/sample - loss: 1862.4811 Epoch 99/100 700/700 [==============================] - 0s 131us/sample - loss: 1831.3929 Epoch 100/100 700/700 [==============================] - 0s 130us/sample - loss: 1805.8847 . Evaluation . model.history.history . {&#39;loss&#39;: [256734.21714285715, 256585.31928571427, 256419.26625, 256235.61214285714, 256019.69857142857, 255769.54089285716, 255486.24535714285, 255165.63785714286, 254808.94669642858, 254418.87669642858, 253993.27366071427, 253532.90375, 253039.56473214287, 252513.30955357142, 251953.13642857142, 251354.94696428571, 250701.46196428573, 249991.80875, 249224.10705357144, 248392.83785714285, 247503.06348214287, 246550.7025892857, 245525.20232142857, 244424.7711607143, 243256.83642857143, 242005.43375, 240694.41955357144, 239295.42053571428, 237815.96901785713, 236250.1349107143, 234589.25044642857, 232837.45, 230987.518125, 229022.9836607143, 226955.67401785715, 224790.23267857142, 222513.4288392857, 220123.720625, 217604.66044642858, 214968.27044642856, 212223.93642857144, 209351.52232142858, 206372.17705357142, 203244.35955357144, 200001.65526785713, 196621.60401785714, 193118.50276785714, 189493.18794642857, 185738.11142857143, 181815.46258928571, 177811.259375, 173674.92017857142, 169387.51232142857, 164988.9644642857, 160489.66357142857, 155834.92910714285, 151046.01794642856, 146175.07098214285, 141198.4019642857, 136109.44705357144, 130940.75205357143, 125713.84629464286, 120369.59245535714, 114954.34678571428, 109446.05647321428, 103925.00058035714, 98349.1309375, 92735.816875, 87106.15816964286, 81484.01299107143, 75883.28279017858, 70311.07026785714, 64779.814776785715, 59317.37823660715, 53911.82245535714, 48680.989955357145, 43579.71308035714, 38609.066852678574, 33854.90863839286, 29302.749988839285, 24985.348939732143, 20966.341183035714, 17273.138872767857, 13918.604561941964, 10918.261395089286, 8299.284235491072, 6104.3133203125, 4426.217624162947, 3218.5810560825894, 2481.1472181919644, 2149.166927315848, 2050.652879464286, 2018.1330650111606, 1985.4679157366072, 1957.8602859933035, 1924.4280615234375, 1892.296298828125, 1862.4810923549107, 1831.3928627232142, 1805.884675641741]} . loss = model.history.history[&#39;loss&#39;] . sns.lineplot(x= range(len(loss)), y=loss) plt.title(&#39;Training Loss per Epoch&#39;); . model.metrics_names . [&#39;loss&#39;] . training_score = model.evaluate(X_train, y_train, verbose=0) test_score = model.evaluate(X_test, y_test, verbose=0) . test_predictions = model.predict(X_test) test_predictions[0:10] . array([[447.78525], [575.96216], [564.137 ], [530.7126 ], [396.1584 ], [545.59674], [484.13968], [490.73978], [517.7349 ], [486.22565]], dtype=float32) . pred_df = pd.DataFrame(y_test, columns=[&#39;Test Y&#39;]) pred_df . Test Y . 0 402.296319 | . 1 624.156198 | . 2 582.455066 | . 3 578.588606 | . 4 371.224104 | . ... ... | . 295 525.704657 | . 296 502.909473 | . 297 612.727910 | . 298 417.569725 | . 299 410.538250 | . 300 rows × 1 columns . test_predictions = pd.Series(test_predictions.reshape(300,)) test_predictions . 0 447.785248 1 575.962158 2 564.137024 3 530.712585 4 396.158386 ... 295 507.563782 296 465.209320 297 571.083984 298 468.226532 299 439.993561 Length: 300, dtype: float32 . test_predictions . 0 447.785248 1 575.962158 2 564.137024 3 530.712585 4 396.158386 ... 295 507.563782 296 465.209320 297 571.083984 298 468.226532 299 439.993561 Length: 300, dtype: float32 . pred_df = pd.concat([pred_df, test_predictions], axis=1) . pred_df.columns = [&#39;Test Y&#39;, &#39;Model Predictions&#39;] . pred_df . Test Y Model Predictions . 0 402.296319 | 447.785248 | . 1 624.156198 | 575.962158 | . 2 582.455066 | 564.137024 | . 3 578.588606 | 530.712585 | . 4 371.224104 | 396.158386 | . ... ... | ... | . 295 525.704657 | 507.563782 | . 296 502.909473 | 465.209320 | . 297 612.727910 | 571.083984 | . 298 417.569725 | 468.226532 | . 299 410.538250 | 439.993561 | . 300 rows × 2 columns . sns.scatterplot(x = &#39;Test Y&#39;, y=&#39;Model Predictions&#39;, data=pred_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbdac0a6250&gt; . pred_df[&#39;Error&#39;] = pred_df[&#39;Test Y&#39;] - pred_df[&#39;Model Predictions&#39;] . sns.distplot(pred_df[&#39;Error&#39;], bins = 50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbdac0c1750&gt; . from sklearn.metrics import mean_absolute_error, mean_squared_error . mean_absolute_error(pred_df[&#39;Test Y&#39;], pred_df[&#39;Model Predictions&#39;]) . 33.24022047283535 . mean_squared_error(pred_df[&#39;Test Y&#39;], pred_df[&#39;Model Predictions&#39;]) . 1767.7946086328484 . test_score . 1767.7945979817707 . test_score ** 0.5 . 42.045149517890536 . Predict on new data . new_gem = [[998, 1000]] . scaler.transform(new_gem) . array([[0.14117652, 0.53968792]]) . new_gem = scaler.transform(new_gem) . model.predict(new_gem) . array([[429.36853]], dtype=float32) . Save Model . from tensorflow.keras.models import load_model . model.save(&#39;my_model.h5&#39;) . later_model = load_model(&#39;my_model.h5&#39;) . WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer. . Load Model . later_model.predict(new_gem) . array([[429.36853]], dtype=float32) .",
            "url": "https://sams101.github.io/DataScience/2020/10/03/Keras-Regression-Simple-Diamond-Prices.html",
            "relUrl": "/2020/10/03/Keras-Regression-Simple-Diamond-Prices.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Keras ANNs Intermediate - Regression - House Prices",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . df = pd.read_csv(&#39;DATA/kc_house_data.csv&#39;) . df.head() . id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view ... grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 0 7129300520 | 10/13/2014 | 221900.0 | 3 | 1.00 | 1180 | 5650 | 1.0 | 0 | 0 | ... | 7 | 1180 | 0 | 1955 | 0 | 98178 | 47.5112 | -122.257 | 1340 | 5650 | . 1 6414100192 | 12/9/2014 | 538000.0 | 3 | 2.25 | 2570 | 7242 | 2.0 | 0 | 0 | ... | 7 | 2170 | 400 | 1951 | 1991 | 98125 | 47.7210 | -122.319 | 1690 | 7639 | . 2 5631500400 | 2/25/2015 | 180000.0 | 2 | 1.00 | 770 | 10000 | 1.0 | 0 | 0 | ... | 6 | 770 | 0 | 1933 | 0 | 98028 | 47.7379 | -122.233 | 2720 | 8062 | . 3 2487200875 | 12/9/2014 | 604000.0 | 4 | 3.00 | 1960 | 5000 | 1.0 | 0 | 0 | ... | 7 | 1050 | 910 | 1965 | 0 | 98136 | 47.5208 | -122.393 | 1360 | 5000 | . 4 1954400510 | 2/18/2015 | 510000.0 | 3 | 2.00 | 1680 | 8080 | 1.0 | 0 | 0 | ... | 8 | 1680 | 0 | 1987 | 0 | 98074 | 47.6168 | -122.045 | 1800 | 7503 | . 5 rows × 21 columns . df.isnull().sum() . id 0 date 0 price 0 bedrooms 0 bathrooms 0 sqft_living 0 sqft_lot 0 floors 0 waterfront 0 view 0 condition 0 grade 0 sqft_above 0 sqft_basement 0 yr_built 0 yr_renovated 0 zipcode 0 lat 0 long 0 sqft_living15 0 sqft_lot15 0 dtype: int64 . df.describe().transpose() . count mean std min 25% 50% 75% max . id 21597.0 | 4.580474e+09 | 2.876736e+09 | 1.000102e+06 | 2.123049e+09 | 3.904930e+09 | 7.308900e+09 | 9.900000e+09 | . price 21597.0 | 5.402966e+05 | 3.673681e+05 | 7.800000e+04 | 3.220000e+05 | 4.500000e+05 | 6.450000e+05 | 7.700000e+06 | . bedrooms 21597.0 | 3.373200e+00 | 9.262989e-01 | 1.000000e+00 | 3.000000e+00 | 3.000000e+00 | 4.000000e+00 | 3.300000e+01 | . bathrooms 21597.0 | 2.115826e+00 | 7.689843e-01 | 5.000000e-01 | 1.750000e+00 | 2.250000e+00 | 2.500000e+00 | 8.000000e+00 | . sqft_living 21597.0 | 2.080322e+03 | 9.181061e+02 | 3.700000e+02 | 1.430000e+03 | 1.910000e+03 | 2.550000e+03 | 1.354000e+04 | . sqft_lot 21597.0 | 1.509941e+04 | 4.141264e+04 | 5.200000e+02 | 5.040000e+03 | 7.618000e+03 | 1.068500e+04 | 1.651359e+06 | . floors 21597.0 | 1.494096e+00 | 5.396828e-01 | 1.000000e+00 | 1.000000e+00 | 1.500000e+00 | 2.000000e+00 | 3.500000e+00 | . waterfront 21597.0 | 7.547345e-03 | 8.654900e-02 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 1.000000e+00 | . view 21597.0 | 2.342918e-01 | 7.663898e-01 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 4.000000e+00 | . condition 21597.0 | 3.409825e+00 | 6.505456e-01 | 1.000000e+00 | 3.000000e+00 | 3.000000e+00 | 4.000000e+00 | 5.000000e+00 | . grade 21597.0 | 7.657915e+00 | 1.173200e+00 | 3.000000e+00 | 7.000000e+00 | 7.000000e+00 | 8.000000e+00 | 1.300000e+01 | . sqft_above 21597.0 | 1.788597e+03 | 8.277598e+02 | 3.700000e+02 | 1.190000e+03 | 1.560000e+03 | 2.210000e+03 | 9.410000e+03 | . sqft_basement 21597.0 | 2.917250e+02 | 4.426678e+02 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 5.600000e+02 | 4.820000e+03 | . yr_built 21597.0 | 1.971000e+03 | 2.937523e+01 | 1.900000e+03 | 1.951000e+03 | 1.975000e+03 | 1.997000e+03 | 2.015000e+03 | . yr_renovated 21597.0 | 8.446479e+01 | 4.018214e+02 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 2.015000e+03 | . zipcode 21597.0 | 9.807795e+04 | 5.351307e+01 | 9.800100e+04 | 9.803300e+04 | 9.806500e+04 | 9.811800e+04 | 9.819900e+04 | . lat 21597.0 | 4.756009e+01 | 1.385518e-01 | 4.715590e+01 | 4.747110e+01 | 4.757180e+01 | 4.767800e+01 | 4.777760e+01 | . long 21597.0 | -1.222140e+02 | 1.407235e-01 | -1.225190e+02 | -1.223280e+02 | -1.222310e+02 | -1.221250e+02 | -1.213150e+02 | . sqft_living15 21597.0 | 1.986620e+03 | 6.852305e+02 | 3.990000e+02 | 1.490000e+03 | 1.840000e+03 | 2.360000e+03 | 6.210000e+03 | . sqft_lot15 21597.0 | 1.275828e+04 | 2.727444e+04 | 6.510000e+02 | 5.100000e+03 | 7.620000e+03 | 1.008300e+04 | 8.712000e+05 | . sns.distplot(df[&#39;price&#39;]); . sns.countplot(df[&#39;bedrooms&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad44c38b90&gt; . plt.figure(figsize = (12,8)) sns.scatterplot(x = &#39;price&#39;, y=&#39;sqft_living&#39;, data=df); . sns.boxplot(x = &#39;bedrooms&#39;, y=&#39;price&#39;, data=df); . Geographical Properties . sns.scatterplot(x=&#39;price&#39;, y=&#39;long&#39;, data=df); . sns.scatterplot(x=&#39;price&#39;, y=&#39;lat&#39;, data=df); . plt.figure(figsize=(12,8)) sns.scatterplot(x= &#39;long&#39;, y=&#39;lat&#39;, data=df, hue=&#39;price&#39;); . df.sort_values(&#39;price&#39;, ascending=False).head(3) . id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view ... grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 7245 6762700020 | 10/13/2014 | 7700000.0 | 6 | 8.00 | 12050 | 27600 | 2.5 | 0 | 3 | ... | 13 | 8570 | 3480 | 1910 | 1987 | 98102 | 47.6298 | -122.323 | 3940 | 8800 | . 3910 9808700762 | 6/11/2014 | 7060000.0 | 5 | 4.50 | 10040 | 37325 | 2.0 | 1 | 2 | ... | 11 | 7680 | 2360 | 1940 | 2001 | 98004 | 47.6500 | -122.214 | 3930 | 25449 | . 9245 9208900037 | 9/19/2014 | 6890000.0 | 6 | 7.75 | 9890 | 31374 | 2.0 | 0 | 4 | ... | 13 | 8860 | 1030 | 2001 | 0 | 98039 | 47.6305 | -122.240 | 4540 | 42730 | . 3 rows × 21 columns . len(df) * 0.01 . 215.97 . non_top_1_perc = df.sort_values(&#39;price&#39;, ascending=False).iloc[216:] . plt.figure(figsize=(12,8)) sns.scatterplot(x=&#39;long&#39;, y=&#39;lat&#39;, data=non_top_1_perc, hue=&#39;price&#39;, palette=&#39;RdYlGn&#39;, edgecolor=None, alpha=0.2); . sns.boxplot(x=&#39;waterfront&#39;, y=&#39;price&#39;, data=df); . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 21597 entries, 0 to 21596 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 id 21597 non-null int64 1 date 21597 non-null object 2 price 21597 non-null float64 3 bedrooms 21597 non-null int64 4 bathrooms 21597 non-null float64 5 sqft_living 21597 non-null int64 6 sqft_lot 21597 non-null int64 7 floors 21597 non-null float64 8 waterfront 21597 non-null int64 9 view 21597 non-null int64 10 condition 21597 non-null int64 11 grade 21597 non-null int64 12 sqft_above 21597 non-null int64 13 sqft_basement 21597 non-null int64 14 yr_built 21597 non-null int64 15 yr_renovated 21597 non-null int64 16 zipcode 21597 non-null int64 17 lat 21597 non-null float64 18 long 21597 non-null float64 19 sqft_living15 21597 non-null int64 20 sqft_lot15 21597 non-null int64 dtypes: float64(5), int64(15), object(1) memory usage: 3.5+ MB . df = df.drop(&#39;id&#39;, axis=1) df.head(3) . date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 0 10/13/2014 | 221900.0 | 3 | 1.00 | 1180 | 5650 | 1.0 | 0 | 0 | 3 | 7 | 1180 | 0 | 1955 | 0 | 98178 | 47.5112 | -122.257 | 1340 | 5650 | . 1 12/9/2014 | 538000.0 | 3 | 2.25 | 2570 | 7242 | 2.0 | 0 | 0 | 3 | 7 | 2170 | 400 | 1951 | 1991 | 98125 | 47.7210 | -122.319 | 1690 | 7639 | . 2 2/25/2015 | 180000.0 | 2 | 1.00 | 770 | 10000 | 1.0 | 0 | 0 | 3 | 6 | 770 | 0 | 1933 | 0 | 98028 | 47.7379 | -122.233 | 2720 | 8062 | . Feature Engineering . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;]) . df[&#39;month&#39;] = df[&#39;date&#39;].apply(lambda date:date.month) df[&#39;year&#39;] = df[&#39;date&#39;].apply(lambda date:date.year) . sns.boxplot(x=&#39;year&#39;, y=&#39;price&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad4445b250&gt; . sns.boxplot(x=&#39;month&#39;, y=&#39;price&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad44d52c50&gt; . df.groupby(&#39;month&#39;).mean()[&#39;price&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad45833510&gt; . df.groupby(&#39;year&#39;).mean()[&#39;price&#39;].plot(); . df = df.drop(&#39;date&#39;, axis=1) . df.columns . Index([&#39;price&#39;, &#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft_living&#39;, &#39;sqft_lot&#39;, &#39;floors&#39;, &#39;waterfront&#39;, &#39;view&#39;, &#39;condition&#39;, &#39;grade&#39;, &#39;sqft_above&#39;, &#39;sqft_basement&#39;, &#39;yr_built&#39;, &#39;yr_renovated&#39;, &#39;zipcode&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;sqft_living15&#39;, &#39;sqft_lot15&#39;, &#39;month&#39;, &#39;year&#39;], dtype=&#39;object&#39;) . df[&#39;zipcode&#39;].value_counts() . 98103 602 98038 589 98115 583 98052 574 98117 553 ... 98102 104 98010 100 98024 80 98148 57 98039 50 Name: zipcode, Length: 70, dtype: int64 . df = df.drop(&#39;zipcode&#39;, axis=1) . Create Train and Test splits . # Features X = df.drop(&#39;price&#39;, axis=1).values y = df[&#39;price&#39;].values . from sklearn.model_selection import train_test_split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) . Scaling . from sklearn.preprocessing import MinMaxScaler . scaler = MinMaxScaler() . X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) . X_train.shape . (15117, 19) . X_test.shape . (6480, 19) . Create Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation from tensorflow.keras.optimizers import Adam . model = Sequential() model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimiser=&#39;adam&#39;, loss=&#39;mse&#39;) . Train Model . model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=128, epochs=600, verbose=0) . &lt;tensorflow.python.keras.callbacks.History at 0x7fad34105d50&gt; . losses = pd.DataFrame(model.history.history) . losses.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad3414f950&gt; . Evaluation on Test Data . from sklearn.metrics import mean_squared_error, mean_absolute_error from sklearn.metrics import explained_variance_score . predictions = model.predict(X_test) . mean_absolute_error(y_test, predictions) . 76821.42352189429 . np.sqrt(mean_squared_error(y_test, predictions)) . 126278.81521004501 . explained_variance_score(y_test, predictions) . 0.8824346233371788 . df[&#39;price&#39;].mean() . 540296.5735055795 . df[&#39;price&#39;].median() . 450000.0 . # Our predictions plt.scatter(y_test, predictions); # Perfect predictions plt.plot(y_test, y_test, &#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fad33458890&gt;] . errors = y_test.reshape(6480, 1) - predictions . sns.distplot(errors); . Predicting on new house . single_house = df.drop(&#39;price&#39;, axis=1).iloc[0] single_house . bedrooms 3.0000 bathrooms 1.0000 sqft_living 1180.0000 sqft_lot 5650.0000 floors 1.0000 waterfront 0.0000 view 0.0000 condition 3.0000 grade 7.0000 sqft_above 1180.0000 sqft_basement 0.0000 yr_built 1955.0000 yr_renovated 0.0000 lat 47.5112 long -122.2570 sqft_living15 1340.0000 sqft_lot15 5650.0000 month 10.0000 year 2014.0000 Name: 0, dtype: float64 . single_house = scaler.transform(single_house.values.reshape(1,19)) . model.predict(single_house) . array([[267865.03]], dtype=float32) . df.iloc[0] . price 221900.0000 bedrooms 3.0000 bathrooms 1.0000 sqft_living 1180.0000 sqft_lot 5650.0000 floors 1.0000 waterfront 0.0000 view 0.0000 condition 3.0000 grade 7.0000 sqft_above 1180.0000 sqft_basement 0.0000 yr_built 1955.0000 yr_renovated 0.0000 lat 47.5112 long -122.2570 sqft_living15 1340.0000 sqft_lot15 5650.0000 month 10.0000 year 2014.0000 Name: 0, dtype: float64 .",
            "url": "https://sams101.github.io/DataScience/2020/10/03/Keras-Regression-Complex-House-Prices.html",
            "relUrl": "/2020/10/03/Keras-Regression-Complex-House-Prices.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Keras ANNs - Breast Cancer Classification",
            "content": "import pandas as pd import numpy as np . df = pd.read_csv(&#39;DATA/cancer_classification.csv&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 mean radius 569 non-null float64 1 mean texture 569 non-null float64 2 mean perimeter 569 non-null float64 3 mean area 569 non-null float64 4 mean smoothness 569 non-null float64 5 mean compactness 569 non-null float64 6 mean concavity 569 non-null float64 7 mean concave points 569 non-null float64 8 mean symmetry 569 non-null float64 9 mean fractal dimension 569 non-null float64 10 radius error 569 non-null float64 11 texture error 569 non-null float64 12 perimeter error 569 non-null float64 13 area error 569 non-null float64 14 smoothness error 569 non-null float64 15 compactness error 569 non-null float64 16 concavity error 569 non-null float64 17 concave points error 569 non-null float64 18 symmetry error 569 non-null float64 19 fractal dimension error 569 non-null float64 20 worst radius 569 non-null float64 21 worst texture 569 non-null float64 22 worst perimeter 569 non-null float64 23 worst area 569 non-null float64 24 worst smoothness 569 non-null float64 25 worst compactness 569 non-null float64 26 worst concavity 569 non-null float64 27 worst concave points 569 non-null float64 28 worst symmetry 569 non-null float64 29 worst fractal dimension 569 non-null float64 30 benign_0__mal_1 569 non-null int64 dtypes: float64(30), int64(1) memory usage: 137.9 KB . df.describe().transpose() . count mean std min 25% 50% 75% max . mean radius 569.0 | 14.127292 | 3.524049 | 6.981000 | 11.700000 | 13.370000 | 15.780000 | 28.11000 | . mean texture 569.0 | 19.289649 | 4.301036 | 9.710000 | 16.170000 | 18.840000 | 21.800000 | 39.28000 | . mean perimeter 569.0 | 91.969033 | 24.298981 | 43.790000 | 75.170000 | 86.240000 | 104.100000 | 188.50000 | . mean area 569.0 | 654.889104 | 351.914129 | 143.500000 | 420.300000 | 551.100000 | 782.700000 | 2501.00000 | . mean smoothness 569.0 | 0.096360 | 0.014064 | 0.052630 | 0.086370 | 0.095870 | 0.105300 | 0.16340 | . mean compactness 569.0 | 0.104341 | 0.052813 | 0.019380 | 0.064920 | 0.092630 | 0.130400 | 0.34540 | . mean concavity 569.0 | 0.088799 | 0.079720 | 0.000000 | 0.029560 | 0.061540 | 0.130700 | 0.42680 | . mean concave points 569.0 | 0.048919 | 0.038803 | 0.000000 | 0.020310 | 0.033500 | 0.074000 | 0.20120 | . mean symmetry 569.0 | 0.181162 | 0.027414 | 0.106000 | 0.161900 | 0.179200 | 0.195700 | 0.30400 | . mean fractal dimension 569.0 | 0.062798 | 0.007060 | 0.049960 | 0.057700 | 0.061540 | 0.066120 | 0.09744 | . radius error 569.0 | 0.405172 | 0.277313 | 0.111500 | 0.232400 | 0.324200 | 0.478900 | 2.87300 | . texture error 569.0 | 1.216853 | 0.551648 | 0.360200 | 0.833900 | 1.108000 | 1.474000 | 4.88500 | . perimeter error 569.0 | 2.866059 | 2.021855 | 0.757000 | 1.606000 | 2.287000 | 3.357000 | 21.98000 | . area error 569.0 | 40.337079 | 45.491006 | 6.802000 | 17.850000 | 24.530000 | 45.190000 | 542.20000 | . smoothness error 569.0 | 0.007041 | 0.003003 | 0.001713 | 0.005169 | 0.006380 | 0.008146 | 0.03113 | . compactness error 569.0 | 0.025478 | 0.017908 | 0.002252 | 0.013080 | 0.020450 | 0.032450 | 0.13540 | . concavity error 569.0 | 0.031894 | 0.030186 | 0.000000 | 0.015090 | 0.025890 | 0.042050 | 0.39600 | . concave points error 569.0 | 0.011796 | 0.006170 | 0.000000 | 0.007638 | 0.010930 | 0.014710 | 0.05279 | . symmetry error 569.0 | 0.020542 | 0.008266 | 0.007882 | 0.015160 | 0.018730 | 0.023480 | 0.07895 | . fractal dimension error 569.0 | 0.003795 | 0.002646 | 0.000895 | 0.002248 | 0.003187 | 0.004558 | 0.02984 | . worst radius 569.0 | 16.269190 | 4.833242 | 7.930000 | 13.010000 | 14.970000 | 18.790000 | 36.04000 | . worst texture 569.0 | 25.677223 | 6.146258 | 12.020000 | 21.080000 | 25.410000 | 29.720000 | 49.54000 | . worst perimeter 569.0 | 107.261213 | 33.602542 | 50.410000 | 84.110000 | 97.660000 | 125.400000 | 251.20000 | . worst area 569.0 | 880.583128 | 569.356993 | 185.200000 | 515.300000 | 686.500000 | 1084.000000 | 4254.00000 | . worst smoothness 569.0 | 0.132369 | 0.022832 | 0.071170 | 0.116600 | 0.131300 | 0.146000 | 0.22260 | . worst compactness 569.0 | 0.254265 | 0.157336 | 0.027290 | 0.147200 | 0.211900 | 0.339100 | 1.05800 | . worst concavity 569.0 | 0.272188 | 0.208624 | 0.000000 | 0.114500 | 0.226700 | 0.382900 | 1.25200 | . worst concave points 569.0 | 0.114606 | 0.065732 | 0.000000 | 0.064930 | 0.099930 | 0.161400 | 0.29100 | . worst symmetry 569.0 | 0.290076 | 0.061867 | 0.156500 | 0.250400 | 0.282200 | 0.317900 | 0.66380 | . worst fractal dimension 569.0 | 0.083946 | 0.018061 | 0.055040 | 0.071460 | 0.080040 | 0.092080 | 0.20750 | . benign_0__mal_1 569.0 | 0.627417 | 0.483918 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.00000 | . import seaborn as sns import matplotlib.pyplot as plt . sns.countplot(x=&#39;benign_0__mal_1&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7edfc6250&gt; . plt.figure(figsize=(12,8)) sns.heatmap(df.corr()); . df.corr()[&#39;benign_0__mal_1&#39;].sort_values() . worst concave points -0.793566 worst perimeter -0.782914 mean concave points -0.776614 worst radius -0.776454 mean perimeter -0.742636 worst area -0.733825 mean radius -0.730029 mean area -0.708984 mean concavity -0.696360 worst concavity -0.659610 mean compactness -0.596534 worst compactness -0.590998 radius error -0.567134 perimeter error -0.556141 area error -0.548236 worst texture -0.456903 worst smoothness -0.421465 worst symmetry -0.416294 mean texture -0.415185 concave points error -0.408042 mean smoothness -0.358560 mean symmetry -0.330499 worst fractal dimension -0.323872 compactness error -0.292999 concavity error -0.253730 fractal dimension error -0.077972 symmetry error 0.006522 texture error 0.008303 mean fractal dimension 0.012838 smoothness error 0.067016 benign_0__mal_1 1.000000 Name: benign_0__mal_1, dtype: float64 . df.corr()[&#39;benign_0__mal_1&#39;].drop(&#39;benign_0__mal_1&#39;).sort_values().plot(kind=&#39;bar&#39;); . Train Test Split . from sklearn.model_selection import train_test_split . X = df.drop(&#39;benign_0__mal_1&#39;, axis=1).values y = df[&#39;benign_0__mal_1&#39;].values . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101) . Scaling Data . from sklearn.preprocessing import MinMaxScaler . scaler = MinMaxScaler() . X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) . Creating the Model . # For a binary classification problem use model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation, Dropout . X_train.shape . (426, 30) . https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-net . model = Sequential() model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . Training the Model . https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network . https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch . model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), verbose=0); . model_loss = pd.DataFrame(model.history.history) . model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7d5acba10&gt; . Add Early Stopping . model = Sequential() model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor = &#39;val_loss&#39;, mode = &#39;min&#39;, patience = 25, verbose = 1) . model.fit(x = X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test), callbacks = [early_stop], verbose = 0) . Epoch 00066: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x7fd7ba372050&gt; . model_loss = pd.DataFrame(model.history.history) model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7ba1513d0&gt; . Adding DroupOut Layers . from tensorflow.keras.layers import Dropout . model = Sequential() model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . model.fit(x = X_train, y = y_train, epochs=600, validation_data=(X_test, y_test), verbose = 0, callbacks = [early_stop]) . Epoch 00151: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x7fd7c0ca5510&gt; . model_loss = pd.DataFrame(model.history.history) model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7c1977250&gt; . Model Evaluation . predictions = model.predict_classes(X_test) . from sklearn.metrics import classification_report, confusion_matrix . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.93 0.98 0.96 55 1 0.99 0.95 0.97 88 accuracy 0.97 143 macro avg 0.96 0.97 0.96 143 weighted avg 0.97 0.97 0.97 143 . print(confusion_matrix(y_test, predictions)) . [[54 1] [ 4 84]] . Tensorboard . from tensorflow.keras.callbacks import TensorBoard from datetime import datetime . datetime.now().strftime(&#39;%Y-%m-%d--%H%M&#39;) . &#39;2020-10-03--1554&#39; . pwd . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study&#39; . log_directory = &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study&#39; + &#39;/&#39; + &#39;logs/fit&#39; . log_directory . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study/logs/fit&#39; . # Create Tensorboard Callback # OPTIONAL: ADD A TIMESTAMP FOR UNIQUE FOLDER # timestamp = datetime.now().strftime(&quot;%Y-%m-%d--%H%M&quot;) # log_directory = log_directory + &#39;/&#39; + timestamp board = TensorBoard(log_dir = log_directory, histogram_freq = 1, write_graph=True, write_images=True, update_freq=&#39;epoch&#39;, profile_batch=2, embeddings_freq=1) . model = Sequential() model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . model.fit(x = X_train, y = y_train, epochs =600, validation_data= (X_test, y_test), callbacks = [early_stop, board], verbose = 0) . Epoch 00096: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x7fd7ad557dd0&gt; . model_loss = pd.DataFrame(model.history.history) model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7aec6dc10&gt; . print(log_directory) . /Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study/logs/fit . Run this code in Terminal to view Tensorboard . Use cd at your command line to change directory to the file path reported back by pwd or your current .py file location. . Then run this code at your command line or terminal . tensorboard --logdir logs/fit .",
            "url": "https://sams101.github.io/DataScience/2020/10/03/Keras-Breast-Cancer-Classifification.html",
            "relUrl": "/2020/10/03/Keras-Breast-Cancer-Classifification.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Pandas Basics",
            "content": "import pandas as pd . df = pd.read_csv(&#39;african_econ_crises.csv&#39;) . df.head() . case cc3 country year systemic_crisis exch_usd domestic_debt_in_default sovereign_external_debt_default gdp_weighted_default inflation_annual_cpi independence currency_crises inflation_crises banking_crisis . 0 1 | DZA | Algeria | 1870 | 1 | 0.052264 | 0 | 0 | 0.0 | 3.441456 | 0 | 0 | 0 | crisis | . 1 1 | DZA | Algeria | 1871 | 0 | 0.052798 | 0 | 0 | 0.0 | 14.149140 | 0 | 0 | 0 | no_crisis | . 2 1 | DZA | Algeria | 1872 | 0 | 0.052274 | 0 | 0 | 0.0 | -3.718593 | 0 | 0 | 0 | no_crisis | . 3 1 | DZA | Algeria | 1873 | 0 | 0.051680 | 0 | 0 | 0.0 | 11.203897 | 0 | 0 | 0 | no_crisis | . 4 1 | DZA | Algeria | 1874 | 0 | 0.051308 | 0 | 0 | 0.0 | -3.848561 | 0 | 0 | 0 | no_crisis | . df[&#39;country&#39;].nunique() . 13 . df[&#39;country&#39;].unique() . array([&#39;Algeria&#39;, &#39;Angola&#39;, &#39;Central African Republic&#39;, &#39;Ivory Coast&#39;, &#39;Egypt&#39;, &#39;Kenya&#39;, &#39;Mauritius&#39;, &#39;Morocco&#39;, &#39;Nigeria&#39;, &#39;South Africa&#39;, &#39;Tunisia&#39;, &#39;Zambia&#39;, &#39;Zimbabwe&#39;], dtype=object) . df.sort_values(&#39;inflation_annual_cpi&#39;, ascending=False).head(1) . case cc3 country year systemic_crisis exch_usd domestic_debt_in_default sovereign_external_debt_default gdp_weighted_default inflation_annual_cpi independence currency_crises inflation_crises banking_crisis . 1053 70 | ZWE | Zimbabwe | 2008 | 1 | 0.002 | 1 | 1 | 0.0 | 21989695.22 | 1 | 1 | 1 | crisis | . df[(df[&#39;country&#39;] == &#39;Kenya&#39;) &amp; (df[&#39;systemic_crisis&#39;] == 1)].sort_values(&#39;year&#39;) . case cc3 country year systemic_crisis exch_usd domestic_debt_in_default sovereign_external_debt_default gdp_weighted_default inflation_annual_cpi independence currency_crises inflation_crises banking_crisis . 475 35 | KEN | Kenya | 1985 | 1 | 16.2843 | 0 | 0 | 0.0 | 11.398 | 1 | 0 | 0 | crisis | . 476 35 | KEN | Kenya | 1986 | 1 | 16.0422 | 0 | 0 | 0.0 | 10.284 | 1 | 0 | 0 | crisis | . 477 35 | KEN | Kenya | 1987 | 1 | 16.5149 | 0 | 0 | 0.0 | 13.007 | 1 | 0 | 0 | crisis | . 478 35 | KEN | Kenya | 1988 | 1 | 18.5994 | 0 | 0 | 0.0 | 4.804 | 1 | 0 | 0 | crisis | . 479 35 | KEN | Kenya | 1989 | 1 | 21.6010 | 0 | 0 | 0.0 | 7.617 | 1 | 1 | 0 | no_crisis | . 482 35 | KEN | Kenya | 1992 | 1 | 36.2163 | 0 | 0 | 0.0 | 27.332 | 1 | 1 | 1 | crisis | . 483 35 | KEN | Kenya | 1993 | 1 | 68.1631 | 0 | 0 | 0.0 | 45.979 | 1 | 1 | 1 | crisis | . 484 35 | KEN | Kenya | 1994 | 1 | 44.8389 | 0 | 1 | 0.0 | 28.814 | 1 | 0 | 1 | crisis | . 485 35 | KEN | Kenya | 1995 | 1 | 55.9389 | 0 | 1 | 0.0 | 1.554 | 1 | 0 | 0 | crisis | . 486 35 | KEN | Kenya | 1996 | 1 | 55.0211 | 0 | 1 | 0.0 | 8.862 | 1 | 0 | 0 | no_crisis | . 487 35 | KEN | Kenya | 1997 | 1 | 62.6778 | 0 | 1 | 0.0 | 11.924 | 1 | 0 | 0 | no_crisis | . 488 35 | KEN | Kenya | 1998 | 1 | 61.9056 | 0 | 1 | 0.0 | 6.716 | 1 | 0 | 0 | no_crisis | . 489 35 | KEN | Kenya | 1999 | 1 | 72.9306 | 0 | 0 | 0.0 | 5.753 | 1 | 1 | 0 | no_crisis | . crisis = df[df[&#39;systemic_crisis&#39;]==1] crisis.groupby(&#39;country&#39;).count()[&#39;systemic_crisis&#39;] . country Algeria 4 Central African Republic 19 Egypt 6 Ivory Coast 4 Kenya 13 Morocco 2 Nigeria 10 Tunisia 5 Zambia 4 Zimbabwe 15 Name: systemic_crisis, dtype: int64 . len(df[ (df[&#39;country&#39;] == &#39;Zimbabwe&#39;) &amp; (df[&#39;sovereign_external_debt_default&#39;]==1)]) . 30 . df[df[&#39;country&#39;]==&#39;Algeria&#39;].sort_values(&#39;exch_usd&#39;, ascending=False) . case cc3 country year systemic_crisis exch_usd domestic_debt_in_default sovereign_external_debt_default gdp_weighted_default inflation_annual_cpi independence currency_crises inflation_crises banking_crisis . 84 1 | DZA | Algeria | 2014 | 0 | 87.970698 | 0 | 0 | 0.0 | 2.917000 | 1 | 0 | 0 | no_crisis | . 72 1 | DZA | Algeria | 2002 | 0 | 79.723400 | 0 | 0 | 0.0 | 1.430000 | 1 | 0 | 0 | no_crisis | . 83 1 | DZA | Algeria | 2013 | 0 | 78.148701 | 0 | 0 | 0.0 | 3.255000 | 1 | 0 | 0 | no_crisis | . 82 1 | DZA | Algeria | 2012 | 0 | 78.102500 | 0 | 0 | 0.0 | 8.916000 | 1 | 0 | 0 | no_crisis | . 71 1 | DZA | Algeria | 2001 | 0 | 77.819600 | 0 | 0 | 0.0 | 4.200000 | 1 | 0 | 0 | no_crisis | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6 1 | DZA | Algeria | 1876 | 0 | 0.051867 | 0 | 0 | 0.0 | -1.769547 | 0 | 0 | 0 | no_crisis | . 3 1 | DZA | Algeria | 1873 | 0 | 0.051680 | 0 | 0 | 0.0 | 11.203897 | 0 | 0 | 0 | no_crisis | . 5 1 | DZA | Algeria | 1875 | 0 | 0.051546 | 0 | 0 | 0.0 | -20.924178 | 0 | 0 | 0 | no_crisis | . 4 1 | DZA | Algeria | 1874 | 0 | 0.051308 | 0 | 0 | 0.0 | -3.848561 | 0 | 0 | 0 | no_crisis | . 12 1 | DZA | Algeria | 1882 | 0 | 0.050761 | 0 | 0 | 0.0 | -12.356127 | 0 | 0 | 0 | no_crisis | . 85 rows × 14 columns . DataFrames . import pandas as pd import numpy as np . columns = [&#39;W&#39;, &#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;] index = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;d&#39;,&#39;E&#39;] . np.random.seed(42) data = np.random.randint(-100, 100, (5,4)) data . array([[ 2, 79, -8, -86], [ 6, -29, 88, -80], [ 2, 21, -26, -13], [ 16, -1, 3, 51], [ 30, 49, -48, -99]]) . df = pd.DataFrame(data, index= index, columns=columns ) df . W X Y Z . A 2 | 79 | -8 | -86 | . B 6 | -29 | 88 | -80 | . C 2 | 21 | -26 | -13 | . d 16 | -1 | 3 | 51 | . E 30 | 49 | -48 | -99 | . df[&#39;W&#39;] . A 2 B 6 C 2 d 16 E 30 Name: W, dtype: int64 . df[[&#39;W&#39;, &#39;Z&#39;]] . W Z . A 2 | -86 | . B 6 | -80 | . C 2 | -13 | . d 16 | 51 | . E 30 | -99 | . type(df[&#39;W&#39;]) . pandas.core.series.Series . df[&#39;new&#39;] = df[&#39;W&#39;] + df[&#39;Y&#39;] df . W X Y Z new . A 2 | 79 | -8 | -86 | -6 | . B 6 | -29 | 88 | -80 | 94 | . C 2 | 21 | -26 | -13 | -24 | . d 16 | -1 | 3 | 51 | 19 | . E 30 | 49 | -48 | -99 | -18 | . df.drop(&#39;new&#39;, axis=1) df . W X Y Z new . A 2 | 79 | -8 | -86 | -6 | . B 6 | -29 | 88 | -80 | 94 | . C 2 | 21 | -26 | -13 | -24 | . d 16 | -1 | 3 | 51 | 19 | . E 30 | 49 | -48 | -99 | -18 | . df = df.drop(&#39;new&#39;, axis=1) df . W X Y Z . A 2 | 79 | -8 | -86 | . B 6 | -29 | 88 | -80 | . C 2 | 21 | -26 | -13 | . d 16 | -1 | 3 | 51 | . E 30 | 49 | -48 | -99 | . df.loc[&#39;A&#39;] . W 2 X 79 Y -8 Z -86 Name: A, dtype: int64 . df.loc[[&#39;A&#39;, &#39;C&#39;]] . W X Y Z . A 2 | 79 | -8 | -86 | . C 2 | 21 | -26 | -13 | . df.iloc[0] . W 2 X 79 Y -8 Z -86 Name: A, dtype: int64 . df.iloc[0:2] . W X Y Z . A 2 | 79 | -8 | -86 | . B 6 | -29 | 88 | -80 | . df.drop(&#39;C&#39;, axis=0) . W X Y Z . A 2 | 79 | -8 | -86 | . B 6 | -29 | 88 | -80 | . d 16 | -1 | 3 | 51 | . E 30 | 49 | -48 | -99 | . df.loc[[&#39;A&#39;, &#39;C&#39;], [&#39;W&#39;, &#39;Y&#39;]] . W Y . A 2 | -8 | . C 2 | -26 | . df . W X Y Z . A 2 | 79 | -8 | -86 | . B 6 | -29 | 88 | -80 | . C 2 | 21 | -26 | -13 | . d 16 | -1 | 3 | 51 | . E 30 | 49 | -48 | -99 | . df &gt; 0 . W X Y Z . A True | True | False | False | . B True | False | True | False | . C True | True | False | False | . d True | False | True | True | . E True | True | False | False | . df[df &gt; 0 ] . W X Y Z x . A 2 | 79.0 | NaN | NaN | NaN | . B 6 | NaN | 88.0 | NaN | NaN | . C 2 | 21.0 | NaN | NaN | NaN | . d 16 | NaN | 3.0 | 51.0 | NaN | . E 30 | 49.0 | NaN | NaN | NaN | . df[&#39;X&#39;] &gt; 0 . A True B False C True d False E True Name: X, dtype: bool . df[df[&#39;X&#39;] &gt; 0] . df[df[&#39;X&#39;] &gt; 0][&#39;Y&#39;] . A -8 C -26 E -48 Name: Y, dtype: int64 . df[df[&#39;X&#39;] &gt; 0 ][[&#39;X&#39;, &#39;Z&#39;]] . X Z . A 79 | -86 | . C 21 | -13 | . E 49 | -99 | . df[(df[&#39;W&#39;]&gt;0) &amp; (df[&#39;Y&#39;] &gt; 1)] . W X Y Z x . B 6 | -29 | 88 | -80 | 0 | . d 16 | -1 | 3 | 51 | 0 | . df . W X Y Z x . A 2 | 79 | -8 | -86 | 0 | . B 6 | -29 | 88 | -80 | 0 | . C 2 | 21 | -26 | -13 | 0 | . d 16 | -1 | 3 | 51 | 0 | . E 30 | 49 | -48 | -99 | 0 | . df.reset_index() . index W X Y Z x . 0 A | 2 | 79 | -8 | -86 | 0 | . 1 B | 6 | -29 | 88 | -80 | 0 | . 2 C | 2 | 21 | -26 | -13 | 0 | . 3 d | 16 | -1 | 3 | 51 | 0 | . 4 E | 30 | 49 | -48 | -99 | 0 | . newind = &#39;CA NY WY OR CO&#39;.split() newind . [&#39;CA&#39;, &#39;NY&#39;, &#39;WY&#39;, &#39;OR&#39;, &#39;CO&#39;] . df[&#39;States&#39;] = newind . df . W X Y Z x States . States . CA 2 | 79 | -8 | -86 | 0 | CA | . NY 6 | -29 | 88 | -80 | 0 | NY | . WY 2 | 21 | -26 | -13 | 0 | WY | . OR 16 | -1 | 3 | 51 | 0 | OR | . CO 30 | 49 | -48 | -99 | 0 | CO | . df.set_index(&#39;States&#39;) . W X Y Z x . States . CA 2 | 79 | -8 | -86 | 0 | . NY 6 | -29 | 88 | -80 | 0 | . WY 2 | 21 | -26 | -13 | 0 | . OR 16 | -1 | 3 | 51 | 0 | . CO 30 | 49 | -48 | -99 | 0 | . df . W X Y Z x States . States . CA 2 | 79 | -8 | -86 | 0 | CA | . NY 6 | -29 | 88 | -80 | 0 | NY | . WY 2 | 21 | -26 | -13 | 0 | WY | . OR 16 | -1 | 3 | 51 | 0 | OR | . CO 30 | 49 | -48 | -99 | 0 | CO | . df = df.set_index(&#39;States&#39;) df . W X Y Z x . States . CA 2 | 79 | -8 | -86 | 0 | . NY 6 | -29 | 88 | -80 | 0 | . WY 2 | 21 | -26 | -13 | 0 | . OR 16 | -1 | 3 | 51 | 0 | . CO 30 | 49 | -48 | -99 | 0 | . df.describe() . W X Y Z x . count 5.00000 | 5.000000 | 5.000000 | 5.000000 | 5.0 | . mean 11.20000 | 23.800000 | 1.800000 | -45.400000 | 0.0 | . std 11.96662 | 42.109381 | 51.915316 | 63.366395 | 0.0 | . min 2.00000 | -29.000000 | -48.000000 | -99.000000 | 0.0 | . 25% 2.00000 | -1.000000 | -26.000000 | -86.000000 | 0.0 | . 50% 6.00000 | 21.000000 | -8.000000 | -80.000000 | 0.0 | . 75% 16.00000 | 49.000000 | 3.000000 | -13.000000 | 0.0 | . max 30.00000 | 79.000000 | 88.000000 | 51.000000 | 0.0 | . df.dtypes . W int64 X int64 Y int64 Z int64 x int64 dtype: object . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 5 entries, CA to CO Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 W 5 non-null int64 1 X 5 non-null int64 2 Y 5 non-null int64 3 Z 5 non-null int64 4 x 5 non-null int64 dtypes: int64(5) memory usage: 240.0+ bytes . Missing Data . import pandas as pd df = pd.DataFrame({&#39;A&#39;: [1, 2, np.nan, 4], &#39;B&#39;: [5, np.nan, np.nan, 8], &#39;C&#39;: [10, 20, 30, 40 ]}) df . A B C . 0 1.0 | 5.0 | 10 | . 1 2.0 | NaN | 20 | . 2 NaN | NaN | 30 | . 3 4.0 | 8.0 | 40 | . df.dropna() . A B C . 0 1.0 | 5.0 | 10 | . 3 4.0 | 8.0 | 40 | . df.dropna(axis=1) . C . 0 10 | . 1 20 | . 2 30 | . 3 40 | . df.dropna(thresh = 2) . A B C . 0 1.0 | 5.0 | 10 | . 1 2.0 | NaN | 20 | . 3 4.0 | 8.0 | 40 | . df.fillna(value = &#39;Fill Value&#39;) . A B C . 0 1 | 5 | 10 | . 1 2 | Fill Value | 20 | . 2 Fill Value | Fill Value | 30 | . 3 4 | 8 | 40 | . df[&#39;A&#39;].fillna(value = 0) . 0 1.0 1 2.0 2 0.0 3 4.0 Name: A, dtype: float64 . df[&#39;A&#39;].fillna(df[&#39;A&#39;].mean()) . 0 1.000000 1 2.000000 2 2.333333 3 4.000000 Name: A, dtype: float64 . df.fillna(df.mean()) . A B C . 0 1.000000 | 5.0 | 10 | . 1 2.000000 | 6.5 | 20 | . 2 2.333333 | 6.5 | 30 | . 3 4.000000 | 8.0 | 40 | . Groupby . import pandas as pd . df = pd.read_csv(&#39;Universities.csv&#39;) . df.head() . Sector University Year Completions Geography . 0 Private for-profit, 2-year | Pima Medical Institute-Las Vegas | 2016 | 591 | Nevada | . 1 Private for-profit, less-than 2-year | Healthcare Preparatory Institute | 2016 | 28 | Nevada | . 2 Private for-profit, less-than 2-year | Milan Institute-Las Vegas | 2016 | 408 | Nevada | . 3 Private for-profit, less-than 2-year | Utah College of Massage Therapy-Vegas | 2016 | 240 | Nevada | . 4 Public, 4-year or above | Western Nevada College | 2016 | 960 | Nevada | . # Step 1 simply returns a special groupby object waiting to have an aggregate method called on it! df.groupby(&#39;Year&#39;) . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fa2eddab750&gt; . df.groupby(&#39;Year&#39;).mean() . Completions . Year . 2012 535.078947 | . 2013 526.150000 | . 2014 588.809524 | . 2015 597.250000 | . 2016 609.860465 | . type(df.groupby(&#39;Year&#39;).mean()) . pandas.core.frame.DataFrame . df.groupby(&#39;Year&#39;).mean().sort_index(ascending=False) . Completions . Year . 2016 609.860465 | . 2015 597.250000 | . 2014 588.809524 | . 2013 526.150000 | . 2012 535.078947 | . df.groupby(&#39;Year&#39;).mad() . Completions . Year . 2012 542.655125 | . 2013 560.455000 | . 2014 652.596372 | . 2015 661.863636 | . 2016 699.015684 | . df.groupby(&#39;Year&#39;).median() . Completions . Year . 2012 229.5 | . 2013 189.0 | . 2014 203.5 | . 2015 191.0 | . 2016 208.0 | . df.groupby(&#39;Year&#39;).std() . Completions . Year . 2012 1036.433239 | . 2013 1040.474782 | . 2014 1150.355857 | . 2015 1183.371791 | . 2016 1235.952796 | . df.groupby(&#39;Year&#39;).skew() . Completions . Year . 2012 3.682215 | . 2013 3.483705 | . 2014 3.101389 | . 2015 3.142177 | . 2016 3.074736 | . df.groupby(&#39;Year&#39;).var() . Completions . Year . 2012 1.074194e+06 | . 2013 1.082588e+06 | . 2014 1.323319e+06 | . 2015 1.400369e+06 | . 2016 1.527579e+06 | . df.head() . Sector University Year Completions Geography . 0 Private for-profit, 2-year | Pima Medical Institute-Las Vegas | 2016 | 591 | Nevada | . 1 Private for-profit, less-than 2-year | Healthcare Preparatory Institute | 2016 | 28 | Nevada | . 2 Private for-profit, less-than 2-year | Milan Institute-Las Vegas | 2016 | 408 | Nevada | . 3 Private for-profit, less-than 2-year | Utah College of Massage Therapy-Vegas | 2016 | 240 | Nevada | . 4 Public, 4-year or above | Western Nevada College | 2016 | 960 | Nevada | . df.groupby([&#39;Year&#39;, &#39;Sector&#39;]).mean() . Completions . Year Sector . 2012 Private for-profit, 2-year 204.800000 | . Private for-profit, 4-year or above 158.000000 | . Private for-profit, less-than 2-year 189.571429 | . Private not-for-profit, 2-year 332.500000 | . Private not-for-profit, 4-year or above 353.000000 | . Public, 2-year 1170.000000 | . Public, 4-year or above 2068.000000 | . 2013 Private for-profit, 2-year 190.812500 | . Private for-profit, 4-year or above 155.000000 | . Private for-profit, less-than 2-year 183.000000 | . Private not-for-profit, 2-year 235.500000 | . Private not-for-profit, 4-year or above 338.666667 | . Public, 2-year 1633.000000 | . Public, 4-year or above 2136.166667 | . 2014 Private for-profit, 2-year 184.812500 | . Private for-profit, 4-year or above 251.000000 | . Private for-profit, less-than 2-year 166.000000 | . Private not-for-profit, 2-year 224.500000 | . Private not-for-profit, 4-year or above 347.333333 | . Public, 2-year 2286.000000 | . Public, 4-year or above 2527.000000 | . 2015 Private for-profit, 2-year 205.000000 | . Private for-profit, 4-year or above 163.250000 | . Private for-profit, less-than 2-year 203.625000 | . Private not-for-profit, 2-year 212.500000 | . Private not-for-profit, 4-year or above 409.333333 | . Public, 2-year 2355.000000 | . Public, 4-year or above 2676.000000 | . 2016 Private for-profit, 2-year 205.375000 | . Private for-profit, 4-year or above 124.666667 | . Private for-profit, less-than 2-year 194.000000 | . Private not-for-profit, 2-year 161.000000 | . Private not-for-profit, 4-year or above 302.000000 | . Public, 2-year 2431.000000 | . Public, 4-year or above 2779.500000 | . df.groupby(&#39;Year&#39;).describe() . Completions . count mean std min 25% 50% 75% max . Year . 2012 38.0 | 535.078947 | 1036.433239 | 13.0 | 114.25 | 229.5 | 420.50 | 5388.0 | . 2013 40.0 | 526.150000 | 1040.474782 | 0.0 | 98.50 | 189.0 | 413.00 | 5278.0 | . 2014 42.0 | 588.809524 | 1150.355857 | 0.0 | 104.50 | 203.5 | 371.75 | 5093.0 | . 2015 44.0 | 597.250000 | 1183.371791 | 0.0 | 87.75 | 191.0 | 405.75 | 5335.0 | . 2016 43.0 | 609.860465 | 1235.952796 | 0.0 | 90.00 | 208.0 | 414.00 | 5367.0 | . df.groupby(&#39;Year&#39;).describe().transpose() . Year 2012 2013 2014 2015 2016 . Completions count 38.000000 | 40.000000 | 42.000000 | 44.000000 | 43.000000 | . mean 535.078947 | 526.150000 | 588.809524 | 597.250000 | 609.860465 | . std 1036.433239 | 1040.474782 | 1150.355857 | 1183.371791 | 1235.952796 | . min 13.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 114.250000 | 98.500000 | 104.500000 | 87.750000 | 90.000000 | . 50% 229.500000 | 189.000000 | 203.500000 | 191.000000 | 208.000000 | . 75% 420.500000 | 413.000000 | 371.750000 | 405.750000 | 414.000000 | . max 5388.000000 | 5278.000000 | 5093.000000 | 5335.000000 | 5367.000000 | . Data Input/Output . pwd . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study&#39; . ls . 01 Numpy.ipynb Universities.csv example.csv 02 Pandas.ipynb african_econ_crises.csv output.csv Excel_Sample.xlsx bank.csv . df = pd.read_csv(&#39;example.csv&#39;) df . a b c d . 0 0 | 1 | 2 | 3 | . 1 4 | 5 | 6 | 7 | . 2 8 | 9 | 10 | 11 | . 3 12 | 13 | 14 | 15 | . df.to_csv(&#39;example.csv&#39;, index=False) . wiki = pd.read_html(&#39;https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)&#39;) . wiki[7] . Rank Country/Territory GDP(US$million) . 0 NaN | World[19] | 87265226 | . 1 1 | United States | 21439453 | . 2 — | European Union[22][n 1] | 18705132 | . 3 2 | China[n 2] | 14140163 | . 4 3 | Japan | 5154475 | . ... ... | ... | ... | . 189 182 | Palau | 291 | . 190 183 | Marshall Islands | 220 | . 191 184 | Kiribati | 184 | . 192 185 | Nauru | 108 | . 193 186 | Tuvalu | 42 | . 194 rows × 3 columns . tables = pd.read_html(&#39;http://www.fdic.gov/bank/individual/failed/banklist.html&#39;) . tables[0].head() . Bank Name City ST CERT Acquiring Institution Closing Date . 0 The First State Bank | Barboursville | WV | 14361 | MVB Bank, Inc. | April 3, 2020 | . 1 Ericson State Bank | Ericson | NE | 18265 | Farmers and Merchants Bank | February 14, 2020 | . 2 City National Bank of New Jersey | Newark | NJ | 21111 | Industrial Bank | November 1, 2019 | . 3 Resolute Bank | Maumee | OH | 58317 | Buckeye State Bank | October 25, 2019 | . 4 Louisa Community Bank | Louisa | KY | 58112 | Kentucky Farmers Bank Corporation | October 25, 2019 | . Operations . import pandas as pd df_one = pd.DataFrame({&#39;k1&#39;:[&#39;A&#39;,&#39;A&#39;,&#39;B&#39;,&#39;B&#39;,&#39;C&#39;,&#39;C&#39;], &#39;col1&#39;:[100,200,300,300,400,500], &#39;col2&#39;:[&#39;NY&#39;,&#39;CA&#39;,&#39;WA&#39;,&#39;WA&#39;,&#39;AK&#39;,&#39;NV&#39;]}) . df_one . k1 col1 col2 . 0 A | 100 | NY | . 1 A | 200 | CA | . 2 B | 300 | WA | . 3 B | 300 | WA | . 4 C | 400 | AK | . 5 C | 500 | NV | . df_one[&#39;col2&#39;].unique() . array([&#39;NY&#39;, &#39;CA&#39;, &#39;WA&#39;, &#39;AK&#39;, &#39;NV&#39;], dtype=object) . df_one[&#39;col2&#39;].nunique() . 5 . df_one[&#39;col2&#39;].value_counts() . WA 2 CA 1 AK 1 NY 1 NV 1 Name: col2, dtype: int64 . df_one.drop_duplicates() . k1 col1 col2 . 0 A | 100 | NY | . 1 A | 200 | CA | . 2 B | 300 | WA | . 4 C | 400 | AK | . 5 C | 500 | NV | . df_one[&#39;New Col&#39;] = df_one[&#39;col1&#39;] * 10 df_one . k1 col1 col2 New Col . 0 A | 100 | NY | 1000 | . 1 A | 200 | CA | 2000 | . 2 B | 300 | WA | 3000 | . 3 B | 300 | WA | 3000 | . 4 C | 400 | AK | 4000 | . 5 C | 500 | NV | 5000 | . def grab_first_letter(state): return state[0] . grab_first_letter(&#39;NY&#39;) . &#39;N&#39; . df_one[&#39;col2&#39;].apply(grab_first_letter) . 0 N 1 C 2 W 3 W 4 A 5 N Name: col2, dtype: object . df_one[&#39;first letter&#39;] = df_one[&#39;col2&#39;].apply(grab_first_letter) . df_one . k1 col1 col2 New Col first letter . 0 A | 100 | NY | 1000 | N | . 1 A | 200 | CA | 2000 | C | . 2 B | 300 | WA | 3000 | W | . 3 B | 300 | WA | 3000 | W | . 4 C | 400 | AK | 4000 | A | . 5 C | 500 | NV | 5000 | N | . import pandas as pd df = pd.read_csv(&#39;african_econ_crises.csv&#39;) . df[&#39;country&#39;].nunique() . 13 . df[(df[&#39;country&#39;]==&#39;Kenya&#39;) &amp; (df[&#39;systemic_crisis&#39;]==1)].sort_values(&#39;year&#39;) . case cc3 country year systemic_crisis exch_usd domestic_debt_in_default sovereign_external_debt_default gdp_weighted_default inflation_annual_cpi independence currency_crises inflation_crises banking_crisis . 475 35 | KEN | Kenya | 1985 | 1 | 16.2843 | 0 | 0 | 0.0 | 11.398 | 1 | 0 | 0 | crisis | . 476 35 | KEN | Kenya | 1986 | 1 | 16.0422 | 0 | 0 | 0.0 | 10.284 | 1 | 0 | 0 | crisis | . 477 35 | KEN | Kenya | 1987 | 1 | 16.5149 | 0 | 0 | 0.0 | 13.007 | 1 | 0 | 0 | crisis | . 478 35 | KEN | Kenya | 1988 | 1 | 18.5994 | 0 | 0 | 0.0 | 4.804 | 1 | 0 | 0 | crisis | . 479 35 | KEN | Kenya | 1989 | 1 | 21.6010 | 0 | 0 | 0.0 | 7.617 | 1 | 1 | 0 | no_crisis | . 482 35 | KEN | Kenya | 1992 | 1 | 36.2163 | 0 | 0 | 0.0 | 27.332 | 1 | 1 | 1 | crisis | . 483 35 | KEN | Kenya | 1993 | 1 | 68.1631 | 0 | 0 | 0.0 | 45.979 | 1 | 1 | 1 | crisis | . 484 35 | KEN | Kenya | 1994 | 1 | 44.8389 | 0 | 1 | 0.0 | 28.814 | 1 | 0 | 1 | crisis | . 485 35 | KEN | Kenya | 1995 | 1 | 55.9389 | 0 | 1 | 0.0 | 1.554 | 1 | 0 | 0 | crisis | . 486 35 | KEN | Kenya | 1996 | 1 | 55.0211 | 0 | 1 | 0.0 | 8.862 | 1 | 0 | 0 | no_crisis | . 487 35 | KEN | Kenya | 1997 | 1 | 62.6778 | 0 | 1 | 0.0 | 11.924 | 1 | 0 | 0 | no_crisis | . 488 35 | KEN | Kenya | 1998 | 1 | 61.9056 | 0 | 1 | 0.0 | 6.716 | 1 | 0 | 0 | no_crisis | . 489 35 | KEN | Kenya | 1999 | 1 | 72.9306 | 0 | 0 | 0.0 | 5.753 | 1 | 1 | 0 | no_crisis | .",
            "url": "https://sams101.github.io/DataScience/2020/10/02/Pandas.html",
            "relUrl": "/2020/10/02/Pandas.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Numpy Basics",
            "content": "import numpy as np . my_list = [1,2,3] my_list . [1, 2, 3] . np.array(my_list) . array([1, 2, 3]) . my_matrix = [[1,2,3],[4,5,6],[7,8,9]] my_matrix . [[1, 2, 3], [4, 5, 6], [7, 8, 9]] . np.array(my_matrix) . array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . np.arange(0,10) . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.arange(0,11,2) . array([ 0, 2, 4, 6, 8, 10]) . np.zeros((5,5)) . array([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) . np.ones(3) . array([1., 1., 1.]) . np.ones((3,4)) . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . np.linspace(0,20,5) . array([ 0., 5., 10., 15., 20.]) . np.linspace(0,5,20) . array([0. , 0.26315789, 0.52631579, 0.78947368, 1.05263158, 1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105, 2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053, 3.94736842, 4.21052632, 4.47368421, 4.73684211, 5. ]) . np.linspace(0,5,21) . array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. , 2.25, 2.5 , 2.75, 3. , 3.25, 3.5 , 3.75, 4. , 4.25, 4.5 , 4.75, 5. ]) . np.eye(4) . array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) . np.random.rand(2) . array([0.21178548, 0.18008915]) . np.random.rand(3,4) . array([[0.55366324, 0.67128084, 0.98579352, 0.02325072], [0.07701532, 0.79497617, 0.57776202, 0.26708334], [0.88177627, 0.0227815 , 0.25584316, 0.32664116]]) . np.random.randn(3) . array([ 1.34425056, -1.88190178, -0.04103648]) . np.random.randint(1,100) . 6 . np.random.randint(1,100,10) . array([34, 55, 55, 43, 35, 97, 4, 52, 20, 58]) . np.random.seed(42) . np.random.rand(4) . array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) . arr = np.arange(25) ranarr = np.random.randint(0,50,10) . arr . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]) . ranarr . array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) . arr.reshape(5,5) . array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]) . ranarr . array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) . ranarr.max() . 39 . ranarr.min() . 2 . ranarr.argmax() . 7 . ranarr.argmin() . 9 . arr.shape . (25,) . arr.reshape(1,25) . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) . arr.reshape(1,25).shape . (1, 25) . arr.reshape(25,1) . array([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]) . arr.reshape(25,1).shape . (25, 1) . arr.dtype . dtype(&#39;int64&#39;) . arr2 = np.array([1.2, 3.4, 5.6]) arr2.dtype . dtype(&#39;float64&#39;) . import numpy as np . arr = np.arange(0,11) arr . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) . arr[8] . 8 . arr[1:5] . array([1, 2, 3, 4]) . arr[0:4] . array([0, 1, 2, 3]) . arr[0:5] = 100 arr . array([100, 100, 100, 100, 100, 5, 6, 7, 8, 9, 10]) . arr = np.arange(0,11) arr . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) . # important note on slices slice_of_arr = arr[0:6] slice_of_arr . array([0, 1, 2, 3, 4, 5]) . slice_of_arr[:] = 99 slice_of_arr . array([99, 99, 99, 99, 99, 99]) . arr . array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) . # To get a copy, need to be explicit arr_copy = arr.copy() arr_copy . array([99, 99, 99, 99, 99, 99, 6, 7, 8, 9, 10]) . Indexing a 2D array (matrices) . The general format is arr_2d[row][col] or arr_2d[row,col]. I recommend using the comma notation for clarity. . arr_2d = np.array(([5,10,15], [20,25,30], [35, 40, 45])) arr_2d . array([[ 5, 10, 15], [20, 25, 30], [35, 40, 45]]) . arr_2d[1] . array([20, 25, 30]) . arr_2d[1][0] . 20 . arr_2d[1,0] . 20 . arr_2d[:2,1:] . array([[10, 15], [25, 30]]) . arr_2d[2] . array([35, 40, 45]) . arr_2d[2,1] . 40 . Conditional Selection . arr = np.arange(1,11) arr . array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) . arr &gt; 4 . array([False, False, False, False, True, True, True, True, True, True]) . bool_arr = arr &gt; 4 bool_arr . array([False, False, False, False, True, True, True, True, True, True]) . arr[bool_arr] . array([ 5, 6, 7, 8, 9, 10]) . arr[arr&gt;2] . array([ 3, 4, 5, 6, 7, 8, 9, 10]) . x = 2 arr[arr&gt;x] . array([ 3, 4, 5, 6, 7, 8, 9, 10]) . NumPy Operations . import numpy as np . arr = np.arange(0,10) arr . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . arr + arr . array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) . arr * arr . array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) . arr/arr . /Users/samtreacy/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide &#34;&#34;&#34;Entry point for launching an IPython kernel. . array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) . 1/arr . /Users/samtreacy/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in true_divide &#34;&#34;&#34;Entry point for launching an IPython kernel. . array([ inf, 1. , 0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) . arr ** 4 . array([ 0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561]) . Universal Array Functions . np.sqrt(arr) . array([0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) . np.exp(arr) . array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03]) . np.sin(arr) . array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) . np.log(arr) . /Users/samtreacy/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log &#34;&#34;&#34;Entry point for launching an IPython kernel. . array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458]) . Summary Statistics on Arrays . arr = np.arange(0,10) . arr.sum() . 45 . arr.mean() . 4.5 . arr.max() . 9 . arr.var() . 8.25 . arr.std() . 2.8722813232690143 . Axis Logic . arr_2d = np.array(([1,2,3,4],[5,6,7,8],[9,10,11,12])) arr_2d . array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) . arr_2d.sum(axis=0) . array([15, 18, 21, 24]) . arr_2d.shape . (3, 4) . arr_2d.sum(axis=1) . array([10, 26, 42]) . NumPy Exercises and Solutions . import numpy as np . np.zeros(10) . array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . np.ones(10) * 5 . array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]) . np.arange(10,51,2) . array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]) . np.arange(9).reshape(3,3) . array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) . np.eye(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . np.random.rand(1) . array([0.03663051]) . np.random.randn(25) . array([ 0.97453543, 0.2926023 , -0.92797909, 0.48026505, 1.74186039, -1.69246091, -1.37212144, 0.81768023, -0.13545292, -1.63617008, -1.57488459, -1.43701345, 0.10597854, -0.56858574, -0.31176684, -0.85042466, 0.00568464, -1.58572796, -0.11983444, -0.39465075, -0.89174802, 0.42194063, -2.06712032, -1.94194237, 1.05986301]) . np.arange(1,101).reshape(10,10)/100 . array([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ], [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ], [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ], [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ], [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ], [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ], [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ], [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ], [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ], [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1. ]]) . np.linspace(0,1,20) . array([0. , 0.05263158, 0.10526316, 0.15789474, 0.21052632, 0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421, 0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211, 0.78947368, 0.84210526, 0.89473684, 0.94736842, 1. ]) . mat = np.arange(1,26).reshape(5,5) mat . array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) . mat[2:,1:] . array([[12, 13, 14, 15], [17, 18, 19, 20], [22, 23, 24, 25]]) . mat[3,4] . 20 . mat[:3,1:2] . array([[ 2], [ 7], [12]]) . mat[4,:] . array([21, 22, 23, 24, 25]) . mat[3:5,:] . array([[16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]) . mat.sum() . 325 . mat.std() . 7.211102550927978 . mat.sum(axis=0) . array([55, 60, 65, 70, 75]) .",
            "url": "https://sams101.github.io/DataScience/2020/10/02/Numpy.html",
            "relUrl": "/2020/10/02/Numpy.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Matplotlib and Seaborn Basics",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt . x = [0,1,2] y = [100,200,300] . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7ffd3f86b4d0&gt;] . # add ; to hide matplotlib output text plt.plot(x,y); . Basic Tools . housing = pd.DataFrame({&#39;rooms&#39;:[1,1,2,2,2,3,3,3,], &#39;price&#39;:[100,120, 190, 200,230,310,330,305]}) housing . rooms price . 0 1 | 100 | . 1 1 | 120 | . 2 2 | 190 | . 3 2 | 200 | . 4 2 | 230 | . 5 3 | 310 | . 6 3 | 330 | . 7 3 | 305 | . plt.scatter(housing[&#39;rooms&#39;], housing[&#39;price&#39;]) . &lt;matplotlib.collections.PathCollection at 0x7ffd3ed8a1d0&gt; . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7ffd4031b450&gt;] . plt.plot(x,y, color=&#39;red&#39;) plt.title(&#39;Title&#39;) plt.xlabel(&#39;X Label&#39;) plt.ylabel(&#39;Y Label&#39;) plt.xlim(0,2) plt.ylim(100,300) . (100.0, 300.0) . plt.plot(x,y, color=&#39;red&#39;, marker=&#39;o&#39;, markersize=20, linestyle=&#39;--&#39;) plt.xlim(0,2) plt.ylim(100,300) plt.title(&#39;Title&#39;) plt.xlabel(&#39;X Label&#39;) plt.ylabel(&#39;Y Label&#39;) . Text(0, 0.5, &#39;Y Label&#39;) . Seaborn . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . ls . 01 Numpy.ipynb Excel_Sample.xlsx example.csv 02 Pandas.ipynb Universities.csv output.csv 03 Matplotlib.ipynb african_econ_crises.csv DATA/ bank.csv . df = pd.read_csv(&#39;DATA/heart.csv&#39;) df.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . sns.distplot(df[&#39;age&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd422ff310&gt; . plt.figure(figsize=(12,8)) sns.distplot(df[&#39;age&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd423e3650&gt; . sns.distplot(df[&#39;age&#39;], kde=False); . sns.distplot(df[&#39;age&#39;], kde=False, bins=40, color=&#39;red&#39;) plt.xlim(40,70); . df.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . sns.countplot(x=&#39;sex&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd4324c6d0&gt; . sns.countplot(df[&#39;sex&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd43428dd0&gt; . sns.countplot(x=&#39;target&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd4350cf50&gt; . sns.countplot(x=&#39;cp&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd435e7910&gt; . sns.countplot(x=&#39;cp&#39;, hue=&#39;sex&#39;, data=df); . sns.countplot(x=&#39;cp&#39;, palette=&#39;terrain&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42cf9910&gt; . sns.boxplot(x=&#39;sex&#39;, y=&#39;age&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42a93a10&gt; . sns.boxplot(x=&#39;sex&#39;, y=&#39;age&#39;, hue=&#39;sex&#39;, data=df); . Scatter Plots . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42e99e10&gt; . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, hue=&#39;sex&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd43950c10&gt; . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, hue=&#39;sex&#39;, palette=&#39;Dark2&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42e3bd10&gt; . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, size=&#39;age&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42e549d0&gt; . iris = pd.read_csv(&#39;DATA/iris.csv&#39;) . iris.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . sns.pairplot(iris); . sns.pairplot(iris, hue=&#39;species&#39;); . cd OneDrive - TietoEVRY/ . /Users/samtreacy/OneDrive - TietoEVRY . cd 00 Analysis/ . /Users/samtreacy/OneDrive - TietoEVRY/00 Analysis . diamonds = pd.read_csv(&#39;DATA/diamonds.csv&#39;) . diamonds[&#39;cut&#39;].unique() . array([&#39;Ideal&#39;, &#39;Premium&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Fair&#39;], dtype=object) . cut_order = list(diamonds[&#39;cut&#39;].unique()) . cut_order . [&#39;Ideal&#39;, &#39;Premium&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Fair&#39;] . plt.figure(figsize=(12,8)) sns.boxplot(x=&#39;cut&#39;, y=&#39;price&#39;, data=diamonds, order=cut_order,palette=&#39;cool&#39; ); .",
            "url": "https://sams101.github.io/DataScience/2020/10/02/Matplotlib_Seaborn.html",
            "relUrl": "/2020/10/02/Matplotlib_Seaborn.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Pandas Intermediate",
            "content": "import pandas as pd . df = pd.read_csv(&#39;IMDb movies.csv&#39;) . df.head(2) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 rows × 22 columns . Mass renaming of columns . df.rename(columns=lambda x: x + &#39;_new&#39;).head(2) . imdb_title_id_new title_new original_title_new year_new date_published_new genre_new duration_new country_new language_new director_new ... actors_new description_new avg_vote_new votes_new budget_new usa_gross_income_new worlwide_gross_income_new metascore_new reviews_from_users_new reviews_from_critics_new . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 rows × 22 columns . df.rename(columns=lambda x: x.upper()).head(2) . IMDB_TITLE_ID TITLE ORIGINAL_TITLE YEAR DATE_PUBLISHED GENRE DURATION COUNTRY LANGUAGE DIRECTOR ... ACTORS DESCRIPTION AVG_VOTE VOTES BUDGET USA_GROSS_INCOME WORLWIDE_GROSS_INCOME METASCORE REVIEWS_FROM_USERS REVIEWS_FROM_CRITICS . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 rows × 22 columns . df.rename(columns=lambda x: x.lower()).head(2) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 rows × 22 columns . | Mass renaming of index . df.rename(index=lambda x: x * 10).head(2) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 10 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 rows × 22 columns . df.rename(index=lambda x: str(x) + &#39;_new&#39; ).head(2) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0_new tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1_new tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 rows × 22 columns . new = df.set_index(&#39;year&#39;).head(3) . new.sort_index(ascending = False) . imdb_title_id title original_title date_published genre duration country language director writer ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . year . 1912 tt0002101 | Cleopatra | Cleopatra | 1912-11-13 | Drama, History | 100 | USA | English | Charles L. Gaskill | Victorien Sardou | ... | Helen Gardner, Pearl Sindelar, Miss Fielding, ... | The fabled queen of Egypt&#39;s affair with Roman ... | 5.2 | 420 | $ 45000 | NaN | NaN | NaN | 24.0 | 3.0 | . 1911 tt0001892 | Den sorte drøm | Den sorte drøm | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | Urban Gad, Gebhard Schätzler-Perasini | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 1906 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 3 rows × 21 columns . new can even combine multiple methods. . df.set_index(&#39;year&#39;).sort_index(ascending=False).head(3) . imdb_title_id title original_title date_published genre duration country language director writer ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . year . 2019 tt9914286 | Sokagin Çocuklari | Sokagin Çocuklari | 2019-03-15 | Drama, Family | 98 | Turkey | Turkish | Ahmet Faik Akinci | Ahmet Faik Akinci, Kasim Uçkan | ... | Ahmet Faik Akinci, Belma Mamati, Metin Keçeci,... | NaN | 7.2 | 190 | NaN | NaN | $ 2833 | NaN | NaN | NaN | . 2019 tt7287656 | Çat Kapi Ask | Çat Kapi Ask | 2019-03-22 | Comedy, Romance | 92 | Turkey | Turkish | Erhan Baytimur | Deniz Güney Isintek, Nesrin Zamur | ... | Müge Boz, Jess Molho, Nergis Kumbasar, Elmaddi... | Su is a dreamer who believes that the love of ... | 3.5 | 208 | TRL 1500000 | NaN | $ 34417 | NaN | NaN | NaN | . 2019 tt7201744 | Philophobia | Philophobia | 2019-10-13 | Drama | 125 | UK | English | Guy Davies | Matthew Brawley, Guy Davies | ... | Harry Lloyd, James Faulkner, Joshua Glenister,... | Set in the English countryside, Philophobia de... | 8.4 | 487 | NaN | NaN | NaN | NaN | 1.0 | 7.0 | . 3 rows × 21 columns . new = df.set_index(&#39;year&#39;) new.head(3) . imdb_title_id title original_title date_published genre duration country language director writer ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . year . 1906 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1911 tt0001892 | Den sorte drøm | Den sorte drøm | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | Urban Gad, Gebhard Schätzler-Perasini | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 1912 tt0002101 | Cleopatra | Cleopatra | 1912-11-13 | Drama, History | 100 | USA | English | Charles L. Gaskill | Victorien Sardou | ... | Helen Gardner, Pearl Sindelar, Miss Fielding, ... | The fabled queen of Egypt&#39;s affair with Roman ... | 5.2 | 420 | $ 45000 | NaN | NaN | NaN | 24.0 | 3.0 | . 3 rows × 21 columns . new.reset_index().head(3) . year imdb_title_id title original_title date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 1906 | tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 1911 | tt0001892 | Den sorte drøm | Den sorte drøm | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 1912 | tt0002101 | Cleopatra | Cleopatra | 1912-11-13 | Drama, History | 100 | USA | English | Charles L. Gaskill | ... | Helen Gardner, Pearl Sindelar, Miss Fielding, ... | The fabled queen of Egypt&#39;s affair with Roman ... | 5.2 | 420 | $ 45000 | NaN | NaN | NaN | 24.0 | 3.0 | . 3 rows × 22 columns . y . 0 True 1 False 2 False 3 False 4 False ... 81268 False 81269 False 81270 False 81271 False 81272 False Name: title, Length: 81273, dtype: bool . s.str.contains() s.str.startswith() s.str.endswith() s.dt.year month, day, dayofweek, pd.to_datetime(series goes here) # Filter, Sort, and Groupby pd.isnull() | Checks for null Values, Returns Boolean Arrray pd.notnull() | Opposite of pd.isnull() df[df[col] &gt; 0.5] | Rows where the column col is greater than 0.5 df[(df[col] &gt; 0.5) &amp; (df[col] &lt; 0.7)] | Rows where 0.7 &gt; col &gt; 0.5 df.groupby(col) | Returns a groupby object for values from one column df.groupby([col1,col2]) | Returns groupby object for values from multiple columns df.groupby(col1)[col2] | Returns the mean of the values in col2, grouped by the values in col1 (mean can be replaced with almost any function from the statistics module) df.pivot_table(index=col1,values=[col2,col3],aggfunc=mean) | Create a pivot table that groups by col1 and calculates the mean of col2 and col3 df.groupby(col1).agg(np.mean) | Find the average across all columns for every unique col1 group df.apply(np.mean) | Apply the function np.mean() across each column nf.apply(np.max,axis=1) | Apply the function np.max() across each row Join/Combine df1.append(df2) | Add the rows in df1 to the end of df2 (columns should be identical) pd.concat([df1, df2],axis=1) | Add the columns in df1 to the end of df2 (rows should be identical) df1.join(df2,on=col1,how=&#39;inner&#39;) | SQL-style join the columns in df1 with the columns on df2 where the rows for col have identical values. &#39;how&#39; can be one of &#39;left&#39;, &#39;right&#39;, &#39;outer&#39;, &#39;inner&#39; Conditional renaming values based on filter df[df[year] == 1945 [&#39;one&#39;, &#39;two&#39;]] = [&#39;new&#39;, &#39;new2&#39;] get_dummies unstack stack group.size group.sum group.agg({}) multiindex pd.concat([get_dummies]) . File &#34;&lt;ipython-input-7-5ea84c6938fb&gt;&#34;, line 1 Data Cleaning ^ SyntaxError: invalid syntax .",
            "url": "https://sams101.github.io/DataScience/2020/06/09/Pandas-Intermediate.html",
            "relUrl": "/2020/06/09/Pandas-Intermediate.html",
            "date": " • Jun 9, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Pandas Basics - new",
            "content": "This notebook will get you to a point where you can quickly explore and manipulate a dataset. . If you would like to replicate the exercises below, just download the following IMDb movies.csv from Kaggle here. If you don&#39;t have a Kaggle account you will need to create one first to download the file. . Importing modules . Let&#39;s start by loading the Pandas module using import pandas. To save time when calling the module we can load it using the alias &quot;pd&quot; by using the as command. This means you only need to use the following pattern when calling a pandas function pd.function(..). . import pandas as pd . The dataset is formatted as CSV so we need to call the .read_csv(..) function. If it was saved as an Excel file we could use the .read_excel(..) function instead. . df = pd.read_csv(&#39;IMDb movies.csv&#39;) . Viewing Data . Let&#39;s view the data. Use the .head(..) method to view the first rows of data. If you pass a value in, in this case 2, then only the first 2 rows will be selected. . df.head(2) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 rows × 22 columns . Use the .tail(..) method to view the last rows of data. If you pass a value in, in this case 2, then only the last 2 rows will be selected. . df.tail(2) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 81271 tt9911774 | Padmavyuhathile Abhimanyu | Padmavyuhathile Abhimanyu | 2019 | 2019-03-08 | Drama | 130 | India | Malayalam | Vineesh Aaradya | ... | Anoop Chandran, Indrans, Sona Nair, Simon Brit... | NaN | 8.4 | 369 | NaN | NaN | NaN | NaN | NaN | NaN | . 81272 tt9914286 | Sokagin Çocuklari | Sokagin Çocuklari | 2019 | 2019-03-15 | Drama, Family | 98 | Turkey | Turkish | Ahmet Faik Akinci | ... | Ahmet Faik Akinci, Belma Mamati, Metin Keçeci,... | NaN | 7.2 | 190 | NaN | NaN | $ 2833 | NaN | NaN | NaN | . 2 rows × 22 columns . Get Dataframe information . You can use the .shape method to check the dimension of the dataframe. In the example below we see there are 81273 rows and 22 columns. . Note this and the .size method below do not have a trailing parentheses. . df.shape . (81273, 22) . The .size method returns the number of cells in the dataframe i.e. number of rows by columns. . df.size . 1788006 . The .info(..) method is very useful as you can simultaneously check the names of each column, how many values are not null per column, and their datatype. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 81273 entries, 0 to 81272 Data columns (total 22 columns): # Column Non-Null Count Dtype -- -- 0 imdb_title_id 81273 non-null object 1 title 81273 non-null object 2 original_title 81273 non-null object 3 year 81273 non-null int64 4 date_published 81273 non-null object 5 genre 81273 non-null object 6 duration 81273 non-null int64 7 country 81234 non-null object 8 language 80518 non-null object 9 director 81200 non-null object 10 writer 79780 non-null object 11 production_company 76948 non-null object 12 actors 81207 non-null object 13 description 78843 non-null object 14 avg_vote 81273 non-null float64 15 votes 81273 non-null int64 16 budget 22804 non-null object 17 usa_gross_income 15094 non-null object 18 worlwide_gross_income 29892 non-null object 19 metascore 12722 non-null float64 20 reviews_from_users 74196 non-null float64 21 reviews_from_critics 70286 non-null float64 dtypes: float64(4), int64(3), object(15) memory usage: 13.6+ MB . Use the following command to create a list of all column names. . df.columns.to_list() . [&#39;imdb_title_id&#39;, &#39;title&#39;, &#39;original_title&#39;, &#39;year&#39;, &#39;date_published&#39;, &#39;genre&#39;, &#39;duration&#39;, &#39;country&#39;, &#39;language&#39;, &#39;director&#39;, &#39;writer&#39;, &#39;production_company&#39;, &#39;actors&#39;, &#39;description&#39;, &#39;avg_vote&#39;, &#39;votes&#39;, &#39;budget&#39;, &#39;usa_gross_income&#39;, &#39;worlwide_gross_income&#39;, &#39;metascore&#39;, &#39;reviews_from_users&#39;, &#39;reviews_from_critics&#39;] . Use .value_counts(..) to quickly check the number of occurrences of each value in a column. . Note: The values themselves (e.g. 2017, 2016, ...) are returned in the index of a new data object. . df[&#39;year&#39;].value_counts() . 2017 3106 2016 3033 2015 2903 2018 2880 2014 2851 ... 1915 21 1913 13 1912 5 1911 4 1906 1 Name: year, Length: 110, dtype: int64 . Sorting Values . You can sort column values either alphabetically or numerically by using the .sort_values(..) method. . Note: By default values will be sorted in ascending order. . df[&#39;year&#39;].sort_values() . 0 1906 34547 1911 3 1911 38007 1911 1 1911 ... 78770 2019 78735 2019 78707 2019 78856 2019 81272 2019 Name: year, Length: 81273, dtype: int64 . Pass ascending=False into the .sort_values(..) method and values will be sorted in descending order. . df[&#39;year&#39;].sort_values(ascending=False) . 81272 2019 78856 2019 78707 2019 78735 2019 78770 2019 ... 1 1911 38007 1911 3 1911 34547 1911 0 1906 Name: year, Length: 81273, dtype: int64 . You can sort the dataframe based on more than one column. Just pass the names of the columns into the .sort_values(..) method as a list. You can independently sort the values by also passing a list into the ascending argument, with boolean True or False. . df.sort_values([&#39;year&#39;,&#39;duration&#39;],ascending=[True,False]).head(4) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 34547 tt0191323 | Oborona Sevastopolya | Oborona Sevastopolya | 1911 | 1911-12-09 | History, War | 100 | Russia | NaN | Vasili Goncharov, Aleksandr Khanzhonkov | ... | Andrey Gromov, N. Semyonov, Olga Petrova-Zvant... | First film ever that was shot by two cameras. ... | 6.0 | 130 | NaN | NaN | NaN | NaN | NaN | NaN | . 38007 tt0266688 | Karadjordje | Karadjordje | 1911 | 1911 | Drama, War | 80 | Serbia | Serbian | Ilija Stanojevic-Cica | ... | Jovan Antonijevic-Djedo, Teodora Arsenovic, Vi... | This is the oldest found dramatic film from Se... | 6.3 | 145 | NaN | NaN | NaN | NaN | 4.0 | NaN | . 3 tt0002130 | L&#39;Inferno | L&#39;Inferno | 1911 | 1911-03-06 | Adventure, Drama, Fantasy | 68 | Italy | Italian | Francesco Bertolini, Adolfo Padovan | ... | Salvatore Papa, Arturo Pirovano, Giuseppe de L... | Loosely adapted from Dante&#39;s Divine Comedy and... | 7.0 | 2019 | NaN | NaN | NaN | NaN | 28.0 | 14.0 | . 4 rows × 22 columns . Basic Statistics . Get the number of values using the .count(..) method. . df.year.count() . 81273 . Get the maximum value in a column using the .max(..). . df.year.max() . 2019 . Get the minimum value in a column using the .min(..). . df.year.min() . 1906 . Get the mean of all values in a column using .mean(..). . df.year.mean() . 1993.0072102666322 . Get the median of all values in a column using .median(..). . df.year.median() . 2002.0 . Get the standard deviation of all values in a column using .std(..). . df.year.std() . 23.992283867737942 . Get the variance of all values in a column using .var(..). . df.year.var() . 575.6296851901183 . Get a statistical summary of the dataframe by using the .describe(..) method below. Only columns with numerical values will be analysed by default. . df.describe() . year duration avg_vote votes metascore reviews_from_users reviews_from_critics . count 81273.000000 | 81273.000000 | 81273.000000 | 8.127300e+04 | 12722.000000 | 74196.000000 | 70286.000000 | . mean 1993.007210 | 100.565981 | 5.926587 | 9.421771e+03 | 55.762695 | 43.753194 | 27.992758 | . std 23.992284 | 25.320189 | 1.243315 | 5.220245e+04 | 17.757453 | 159.903568 | 58.708764 | . min 1906.000000 | 40.000000 | 1.000000 | 9.900000e+01 | 1.000000 | 1.000000 | 1.000000 | . 25% 1979.000000 | 88.000000 | 5.200000 | 2.060000e+02 | 43.000000 | 4.000000 | 3.000000 | . 50% 2002.000000 | 96.000000 | 6.100000 | 4.950000e+02 | 56.000000 | 9.000000 | 8.000000 | . 75% 2012.000000 | 108.000000 | 6.800000 | 1.865000e+03 | 69.000000 | 26.000000 | 24.000000 | . max 2019.000000 | 3360.000000 | 10.000000 | 2.159628e+06 | 100.000000 | 8302.000000 | 987.000000 | . If you pass in the include = &#39;all&#39; argument to .describe(..) categorical and numerical statistics are returned. . df.describe(include=&#39;all&#39;) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . count 81273 | 81273 | 81273 | 81273.000000 | 81273 | 81273 | 81273.000000 | 81234 | 80518 | 81200 | ... | 81207 | 78843 | 81273.000000 | 8.127300e+04 | 22804 | 15094 | 29892 | 12722.000000 | 74196.000000 | 70286.000000 | . unique 81273 | 76618 | 76631 | NaN | 21087 | 1264 | NaN | 4632 | 4251 | 32544 | ... | 81159 | 78727 | NaN | NaN | 4425 | 14648 | 29378 | NaN | NaN | NaN | . top tt0116635 | Anna | Anna | NaN | 2010 | Drama | NaN | USA | English | Michael Curtiz | ... | Nobuyo Ôyama, Noriko Ohara, Michiko Nomura, Ka... | The story of | NaN | NaN | $ 1000000 | $ 1000000 | $ 8144 | NaN | NaN | NaN | . freq 1 | 9 | 9 | NaN | 111 | 11809 | NaN | 27490 | 34519 | 86 | ... | 13 | 11 | NaN | NaN | 731 | 19 | 16 | NaN | NaN | NaN | . mean NaN | NaN | NaN | 1993.007210 | NaN | NaN | 100.565981 | NaN | NaN | NaN | ... | NaN | NaN | 5.926587 | 9.421771e+03 | NaN | NaN | NaN | 55.762695 | 43.753194 | 27.992758 | . std NaN | NaN | NaN | 23.992284 | NaN | NaN | 25.320189 | NaN | NaN | NaN | ... | NaN | NaN | 1.243315 | 5.220245e+04 | NaN | NaN | NaN | 17.757453 | 159.903568 | 58.708764 | . min NaN | NaN | NaN | 1906.000000 | NaN | NaN | 40.000000 | NaN | NaN | NaN | ... | NaN | NaN | 1.000000 | 9.900000e+01 | NaN | NaN | NaN | 1.000000 | 1.000000 | 1.000000 | . 25% NaN | NaN | NaN | 1979.000000 | NaN | NaN | 88.000000 | NaN | NaN | NaN | ... | NaN | NaN | 5.200000 | 2.060000e+02 | NaN | NaN | NaN | 43.000000 | 4.000000 | 3.000000 | . 50% NaN | NaN | NaN | 2002.000000 | NaN | NaN | 96.000000 | NaN | NaN | NaN | ... | NaN | NaN | 6.100000 | 4.950000e+02 | NaN | NaN | NaN | 56.000000 | 9.000000 | 8.000000 | . 75% NaN | NaN | NaN | 2012.000000 | NaN | NaN | 108.000000 | NaN | NaN | NaN | ... | NaN | NaN | 6.800000 | 1.865000e+03 | NaN | NaN | NaN | 69.000000 | 26.000000 | 24.000000 | . max NaN | NaN | NaN | 2019.000000 | NaN | NaN | 3360.000000 | NaN | NaN | NaN | ... | NaN | NaN | 10.000000 | 2.159628e+06 | NaN | NaN | NaN | 100.000000 | 8302.000000 | 987.000000 | . 11 rows × 22 columns . Subsetting a Dataframe . You can select a single column by simply typing the name of the column after the dataframe name, separated by a fullstop. A single column is returned as a Series datatype. . Note: This only works if the column name does not contain spaces. . df.year . 0 1906 1 1911 2 1912 3 1911 4 1912 ... 81268 2019 81269 2019 81270 2019 81271 2019 81272 2019 Name: year, Length: 81273, dtype: int64 . Second method for selecting a single column. This method works even if the column name contains spaces. . df[&#39;year&#39;] . 0 1906 1 1911 2 1912 3 1911 4 1912 ... 81268 2019 81269 2019 81270 2019 81271 2019 81272 2019 Name: year, Length: 81273, dtype: int64 . Select multiple columns by passing the column names as a list [..] into the dataframe brackets. This is why there are two square brackets. . df[[&#39;title&#39;,&#39;year&#39;,&#39;votes&#39;]] . title year votes . 0 The Story of the Kelly Gang | 1906 | 537 | . 1 Den sorte drøm | 1911 | 171 | . 2 Cleopatra | 1912 | 420 | . 3 L&#39;Inferno | 1911 | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 1912 | 438 | . ... ... | ... | ... | . 81268 Jessie | 2019 | 219 | . 81269 Ottam | 2019 | 510 | . 81270 Pengalila | 2019 | 604 | . 81271 Padmavyuhathile Abhimanyu | 2019 | 369 | . 81272 Sokagin Çocuklari | 2019 | 190 | . 81273 rows × 3 columns . You can use an equal sign to assign a subset of a dataframe to a new dataframe. This is often used if you want to manipulate a dataframe line by line, which can make it more readable for yourself and others. . df = df[[&#39;title&#39;,&#39;year&#39;,&#39;votes&#39;]] df . title year votes . 0 The Story of the Kelly Gang | 1906 | 537 | . 1 Den sorte drøm | 1911 | 171 | . 2 Cleopatra | 1912 | 420 | . 3 L&#39;Inferno | 1911 | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 1912 | 438 | . ... ... | ... | ... | . 81268 Jessie | 2019 | 219 | . 81269 Ottam | 2019 | 510 | . 81270 Pengalila | 2019 | 604 | . 81271 Padmavyuhathile Abhimanyu | 2019 | 369 | . 81272 Sokagin Çocuklari | 2019 | 190 | . 81273 rows × 3 columns . You can rename column names by passing in a list of new names using the .columns method. . df.columns = [&#39;title_new&#39;,&#39;year_new&#39;,&#39;votes_new&#39;] df . title_new year_new votes_new . 0 The Story of the Kelly Gang | 1906 | 537 | . 1 Den sorte drøm | 1911 | 171 | . 2 Cleopatra | 1912 | 420 | . 3 L&#39;Inferno | 1911 | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 1912 | 438 | . ... ... | ... | ... | . 81268 Jessie | 2019 | 219 | . 81269 Ottam | 2019 | 510 | . 81270 Pengalila | 2019 | 604 | . 81271 Padmavyuhathile Abhimanyu | 2019 | 369 | . 81272 Sokagin Çocuklari | 2019 | 190 | . 81273 rows × 3 columns . Rename column names back to their oringinal names. . df.columns = [&#39;title&#39;,&#39;year&#39;,&#39;votes&#39;] df . title year votes . 0 The Story of the Kelly Gang | 1906 | 537 | . 1 Den sorte drøm | 1911 | 171 | . 2 Cleopatra | 1912 | 420 | . 3 L&#39;Inferno | 1911 | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 1912 | 438 | . ... ... | ... | ... | . 81268 Jessie | 2019 | 219 | . 81269 Ottam | 2019 | 510 | . 81270 Pengalila | 2019 | 604 | . 81271 Padmavyuhathile Abhimanyu | 2019 | 369 | . 81272 Sokagin Çocuklari | 2019 | 190 | . 81273 rows × 3 columns . You can also change column or index names using the .rename(..) method. Since a key/value pair is used, we pass in a python dictionary, which takes the forma { &#39;key&#39; : &#39;value&#39; }. . Note: A python dictionary uses curly brackets, while a list uses square brackets. . df = df.rename(columns={&#39;title&#39;: &#39;movie_name&#39;}) df.head() . movie_name year votes . 0 The Story of the Kelly Gang | 1906 | 537 | . 1 Den sorte drøm | 1911 | 171 | . 2 Cleopatra | 1912 | 420 | . 3 L&#39;Inferno | 1911 | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 1912 | 438 | . Now let&#39;s change the column name back to &#39;title&#39; and proceed. . df = df.rename(columns={&#39;movie_name&#39;: &#39;title&#39;}) df.head() . title year votes . 0 The Story of the Kelly Gang | 1906 | 537 | . 1 Den sorte drøm | 1911 | 171 | . 2 Cleopatra | 1912 | 420 | . 3 L&#39;Inferno | 1911 | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 1912 | 438 | . Subsetting a Dataframe using .iloc[...] . You can select a value from a column using the .iloc[..] method. . df.title.iloc[2] . &#39;Cleopatra&#39; . You can select an complete row from a dataframe using the following. . df.iloc[2] . title Cleopatra year 1912 votes 420 Name: 2, dtype: object . Select multiple values from a column using the following. . df.title.iloc[0:5] . 0 The Story of the Kelly Gang 1 Den sorte drøm 2 Cleopatra 3 L&#39;Inferno 4 From the Manger to the Cross; or, Jesus of Naz... Name: title, dtype: object . Use negitive indexing to select multiple values at the end of a column. . df.title.iloc[-3:] . 81270 Pengalila 81271 Padmavyuhathile Abhimanyu 81272 Sokagin Çocuklari Name: title, dtype: object . Use negitive indexing to select multiple rows at the end of a dataframe. . df.iloc[-3:] . title year votes . 81270 Pengalila | 2019 | 604 | . 81271 Padmavyuhathile Abhimanyu | 2019 | 369 | . 81272 Sokagin Çocuklari | 2019 | 190 | . Select a single column from a dataframe using its index. In this case by selecting the first column which is &#39;year&#39;. . df.iloc[:,1] . 0 1906 1 1911 2 1912 3 1911 4 1912 ... 81268 2019 81269 2019 81270 2019 81271 2019 81272 2019 Name: year, Length: 81273, dtype: int64 . Select multiple columns from a dataframe using column indexing. . Note: As with python subsetting the first number is included, but the last number is excluded. So only columns 0 and 1 are selected. . df.iloc[:,[0,2]] . title votes . 0 The Story of the Kelly Gang | 537 | . 1 Den sorte drøm | 171 | . 2 Cleopatra | 420 | . 3 L&#39;Inferno | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 438 | . ... ... | ... | . 81268 Jessie | 219 | . 81269 Ottam | 510 | . 81270 Pengalila | 604 | . 81271 Padmavyuhathile Abhimanyu | 369 | . 81272 Sokagin Çocuklari | 190 | . 81273 rows × 2 columns . Select multiple rows from a dataframe using row indexes. . df.iloc[[0,2,3,4],:] . title year votes . 0 The Story of the Kelly Gang | 1906 | 537 | . 2 Cleopatra | 1912 | 420 | . 3 L&#39;Inferno | 1911 | 2019 | . 4 From the Manger to the Cross; or, Jesus of Naz... | 1912 | 438 | . Select a single cell using row and column indexes. . df.iloc[4,1] . 1912 . Subsetting using .loc[..] . You can subset a dataframe using column and index names by using the .loc[..] method. In the example, one column is subset using the column name. . df.loc[:,&#39;title&#39;] . 0 The Story of the Kelly Gang 1 Den sorte drøm 2 Cleopatra 3 L&#39;Inferno 4 From the Manger to the Cross; or, Jesus of Naz... ... 81268 Jessie 81269 Ottam 81270 Pengalila 81271 Padmavyuhathile Abhimanyu 81272 Sokagin Çocuklari Name: title, Length: 81273, dtype: object . # Reimporting the full dataframe from the CSV file. df = pd.read_csv(&#39;IMDb movies.csv&#39;) df.head(3) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 tt0002101 | Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | 100 | USA | English | Charles L. Gaskill | ... | Helen Gardner, Pearl Sindelar, Miss Fielding, ... | The fabled queen of Egypt&#39;s affair with Roman ... | 5.2 | 420 | $ 45000 | NaN | NaN | NaN | 24.0 | 3.0 | . 3 rows × 22 columns . Below all columns between &#39;title&#39; and &#39;genre&#39; are selected. . df.loc[:,&#39;title&#39;:&#39;genre&#39;] . title original_title year date_published genre . 0 The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | . 1 Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | . 2 Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | . 3 L&#39;Inferno | L&#39;Inferno | 1911 | 1911-03-06 | Adventure, Drama, Fantasy | . 4 From the Manger to the Cross; or, Jesus of Naz... | From the Manger to the Cross; or, Jesus of Naz... | 1912 | 1913 | Biography, Drama | . ... ... | ... | ... | ... | ... | . 81268 Jessie | Jessie | 2019 | 2019-03-15 | Horror, Thriller | . 81269 Ottam | Ottam | 2019 | 2019-03-08 | Drama | . 81270 Pengalila | Pengalila | 2019 | 2019-03-08 | Drama | . 81271 Padmavyuhathile Abhimanyu | Padmavyuhathile Abhimanyu | 2019 | 2019-03-08 | Drama | . 81272 Sokagin Çocuklari | Sokagin Çocuklari | 2019 | 2019-03-15 | Drama, Family | . 81273 rows × 5 columns . df.loc[0:3,&#39;title&#39;:&#39;genre&#39;] . title original_title year date_published genre . 0 The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | . 1 Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | . 2 Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | . 3 L&#39;Inferno | L&#39;Inferno | 1911 | 1911-03-06 | Adventure, Drama, Fantasy | . Manipulating dataframe values . # Reimporting the full dataframe from the CSV file. df = pd.read_csv(&#39;IMDb movies.csv&#39;) df.head(3) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 tt0002101 | Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | 100 | USA | English | Charles L. Gaskill | ... | Helen Gardner, Pearl Sindelar, Miss Fielding, ... | The fabled queen of Egypt&#39;s affair with Roman ... | 5.2 | 420 | $ 45000 | NaN | NaN | NaN | 24.0 | 3.0 | . 3 rows × 22 columns . Removing rows and columns with null values . Remove all rows with null (NAN) values using the .dropna(..) method. . df.dropna().head(3) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 488 tt0017136 | Metropolis | Metropolis | 1927 | 1927-02-06 | Drama, Sci-Fi | 153 | Germany | German | Fritz Lang | ... | Alfred Abel, Gustav Fröhlich, Rudolf Klein-Rog... | In a futuristic city sharply divided between t... | 8.3 | 148396 | DEM 6000000 | $ 1236166 | $ 1349711 | 98.0 | 471.0 | 194.0 | . 1005 tt0021749 | City Lights | City Lights | 1931 | 1931-08-21 | Comedy, Drama, Romance | 87 | USA | English | Charles Chaplin | ... | Virginia Cherrill, Florence Lee, Harry Myers, ... | With the aid of a wealthy erratic tippler, a d... | 8.5 | 152716 | $ 1500000 | $ 19181 | $ 32609 | 99.0 | 270.0 | 120.0 | . 2336 tt0027977 | Modern Times | Modern Times | 1936 | 1936-10-16 | Comedy, Drama, Family | 87 | USA | English | Charles Chaplin | ... | Charles Chaplin, Paulette Goddard, Henry Bergm... | The Tramp struggles to live in modern industri... | 8.5 | 197969 | $ 1500000 | $ 163577 | $ 445226 | 96.0 | 262.0 | 146.0 | . 3 rows × 22 columns . Drop all columns with missing values by passing in the axis = &#39;columns&#39; argument. . df.dropna(axis = &#39;columns&#39;) . imdb_title_id title original_title year date_published genre duration avg_vote votes . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | 6.1 | 537 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | 5.9 | 171 | . 2 tt0002101 | Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | 100 | 5.2 | 420 | . 3 tt0002130 | L&#39;Inferno | L&#39;Inferno | 1911 | 1911-03-06 | Adventure, Drama, Fantasy | 68 | 7.0 | 2019 | . 4 tt0002199 | From the Manger to the Cross; or, Jesus of Naz... | From the Manger to the Cross; or, Jesus of Naz... | 1912 | 1913 | Biography, Drama | 60 | 5.7 | 438 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 81268 tt9903716 | Jessie | Jessie | 2019 | 2019-03-15 | Horror, Thriller | 106 | 7.2 | 219 | . 81269 tt9905412 | Ottam | Ottam | 2019 | 2019-03-08 | Drama | 120 | 7.8 | 510 | . 81270 tt9905462 | Pengalila | Pengalila | 2019 | 2019-03-08 | Drama | 111 | 8.4 | 604 | . 81271 tt9911774 | Padmavyuhathile Abhimanyu | Padmavyuhathile Abhimanyu | 2019 | 2019-03-08 | Drama | 130 | 8.4 | 369 | . 81272 tt9914286 | Sokagin Çocuklari | Sokagin Çocuklari | 2019 | 2019-03-15 | Drama, Family | 98 | 7.2 | 190 | . 81273 rows × 9 columns . Drop missing values from specified columns using the subset argument. . df.dropna(subset=[&#39;metascore&#39;,&#39;budget&#39;]).head(2) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 73 tt0006864 | Intolerance: Love&#39;s Struggle Throughout the Ages | Intolerance: Love&#39;s Struggle Throughout the Ages | 1916 | 1918-02-24 | Drama, History | 163 | USA | NaN | D.W. Griffith | ... | Lillian Gish, Mae Marsh, Robert Harron, F.A. T... | The story of a poor young woman, separated by ... | 7.8 | 13116 | $ 385907 | NaN | NaN | 93.0 | 105.0 | 77.0 | . 488 tt0017136 | Metropolis | Metropolis | 1927 | 1927-02-06 | Drama, Sci-Fi | 153 | Germany | German | Fritz Lang | ... | Alfred Abel, Gustav Fröhlich, Rudolf Klein-Rog... | In a futuristic city sharply divided between t... | 8.3 | 148396 | DEM 6000000 | $ 1236166 | $ 1349711 | 98.0 | 471.0 | 194.0 | . 2 rows × 22 columns . You can pass the inplace argument into many methods. This will change the dataframe without it needing to be reassigned using the equal sign. . df.dropna(axis=&#39;columns&#39;, inplace = True) df . imdb_title_id title original_title year date_published genre duration avg_vote votes . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | 6.1 | 537 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | 5.9 | 171 | . 2 tt0002101 | Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | 100 | 5.2 | 420 | . 3 tt0002130 | L&#39;Inferno | L&#39;Inferno | 1911 | 1911-03-06 | Adventure, Drama, Fantasy | 68 | 7.0 | 2019 | . 4 tt0002199 | From the Manger to the Cross; or, Jesus of Naz... | From the Manger to the Cross; or, Jesus of Naz... | 1912 | 1913 | Biography, Drama | 60 | 5.7 | 438 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 81268 tt9903716 | Jessie | Jessie | 2019 | 2019-03-15 | Horror, Thriller | 106 | 7.2 | 219 | . 81269 tt9905412 | Ottam | Ottam | 2019 | 2019-03-08 | Drama | 120 | 7.8 | 510 | . 81270 tt9905462 | Pengalila | Pengalila | 2019 | 2019-03-08 | Drama | 111 | 8.4 | 604 | . 81271 tt9911774 | Padmavyuhathile Abhimanyu | Padmavyuhathile Abhimanyu | 2019 | 2019-03-08 | Drama | 130 | 8.4 | 369 | . 81272 tt9914286 | Sokagin Çocuklari | Sokagin Çocuklari | 2019 | 2019-03-15 | Drama, Family | 98 | 7.2 | 190 | . 81273 rows × 9 columns . # Reimporting the full dataframe from the CSV file. df = pd.read_csv(&#39;IMDb movies.csv&#39;) df.head(3) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | NaN | NaN | NaN | NaN | 4.0 | 2.0 | . 2 tt0002101 | Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | 100 | USA | English | Charles L. Gaskill | ... | Helen Gardner, Pearl Sindelar, Miss Fielding, ... | The fabled queen of Egypt&#39;s affair with Roman ... | 5.2 | 420 | $ 45000 | NaN | NaN | NaN | 24.0 | 3.0 | . 3 rows × 22 columns . Replacing null values . The .fillna(..) method allows you to fill a null value with a value of your choice. In the example below we fill null values with 0. . We also use the inplace = True argument to change the dataframe without having to reassign it to df. . df[&#39;budget&#39;].fillna(0, inplace=True) df.head(3) . imdb_title_id title original_title year date_published genre duration country language director ... actors description avg_vote votes budget usa_gross_income worlwide_gross_income metascore reviews_from_users reviews_from_critics . 0 tt0000574 | The Story of the Kelly Gang | The Story of the Kelly Gang | 1906 | 1906-12-26 | Biography, Crime, Drama | 70 | Australia | NaN | Charles Tait | ... | Elizabeth Tait, John Tait, Norman Campbell, Be... | True story of notorious Australian outlaw Ned ... | 6.1 | 537 | $ 2250 | NaN | NaN | NaN | 7.0 | 7.0 | . 1 tt0001892 | Den sorte drøm | Den sorte drøm | 1911 | 1911-08-19 | Drama | 53 | Germany, Denmark | NaN | Urban Gad | ... | Asta Nielsen, Valdemar Psilander, Gunnar Helse... | Two men of high rank are both wooing the beaut... | 5.9 | 171 | 0 | NaN | NaN | NaN | 4.0 | 2.0 | . 2 tt0002101 | Cleopatra | Cleopatra | 1912 | 1912-11-13 | Drama, History | 100 | USA | English | Charles L. Gaskill | ... | Helen Gardner, Pearl Sindelar, Miss Fielding, ... | The fabled queen of Egypt&#39;s affair with Roman ... | 5.2 | 420 | $ 45000 | NaN | NaN | NaN | 24.0 | 3.0 | . 3 rows × 22 columns . You will now see some more useful methods for quickly changing values in a dataframe. But first we will create a new column based on divided the avg_vote by the number of votes per moview. We&#39;ll call the column avg_votes_div_votes. We will also multiply by 100 to make the numbers more interesting to play around with. . Note: the new column is appending to the most right position in the dataframe. . df[&#39;avg_votes_div_votes&#39;] = (df[&#39;avg_vote&#39;] / df[&#39;votes&#39;]) * 100 df[&#39;avg_votes_div_votes&#39;] . 0 1.135940 1 3.450292 2 1.238095 3 0.346706 4 1.301370 ... 81268 3.287671 81269 1.529412 81270 1.390728 81271 2.276423 81272 3.789474 Name: avg_votes_div_votes, Length: 81273, dtype: float64 . Rounding values . Now you will learn how to use the .round(..) method to round the new column values to a specified decimal place. In this case we will round it to 2 decimal places by passing 2 in as an argument. . df[&#39;avg_votes_div_votes&#39;].round(3) . 0 1.136 1 3.450 2 1.238 3 0.347 4 1.301 ... 81268 3.288 81269 1.529 81270 1.391 81271 2.276 81272 3.789 Name: avg_votes_div_votes, Length: 81273, dtype: float64 . Changing data types . We can also change the type of data to decimals (float), integers (int), text strings (str), and boolean True/False (bool) by passing these valus into the .astype(..) method. In the example below we will covert the float value to an integer as follows. Integers do not support decimal places so all decimals will be removed. . Note: This does not act the same way as round. For example, if you round 1.8, it will be rounded up to 2. However, if you convert 1.8 to an integer the .8 will be removed ao it will become simply 1 instead. . df[&#39;avg_votes_div_votes&#39;].astype(int) . 0 1 1 3 2 1 3 0 4 1 .. 81268 3 81269 1 81270 1 81271 2 81272 3 Name: avg_votes_div_votes, Length: 81273, dtype: int64 . Now let&#39;s convert it to a text string. Once a text string it will not be possible to perform mathematical operations on the values. . df[&#39;avg_votes_div_votes&#39;] = df[&#39;avg_votes_div_votes&#39;].astype(str) df[&#39;avg_votes_div_votes&#39;] . 0 1.1359404096834265 1 3.4502923976608186 2 1.2380952380952381 3 0.3467062902426944 4 1.3013698630136987 ... 81268 3.2876712328767126 81269 1.5294117647058822 81270 1.3907284768211923 81271 2.2764227642276422 81272 3.7894736842105265 Name: avg_votes_div_votes, Length: 81273, dtype: object . we can check that this values are actually strings and not numbers by using the type(..) function. We will check the first value by indexing it as follows. . df[&#39;avg_votes_div_votes&#39;][0] . &#39;1.1359404096834265&#39; . Once indexed we apply the type(..) function. . type(df[&#39;avg_votes_div_votes&#39;][0]) . str . Finally, we will wrap up the lesson by using the .replace(..) method to replace the values in the text with other values. Specifically, we will replace the number values to their english equivalents e.g. &#39;1&#39; with &#39;one&#39;. . df[&#39;avg_votes_div_votes&#39;].str.replace(&#39;1&#39;,&#39;o&#39;) . 0 o.o359404096834265 1 3.4502923976608o86 2 o.238095238095238o 3 0.3467062902426944 4 o.30o3698630o36987 ... 81268 3.28767o2328767o26 81269 o.5294oo7647058822 81270 o.39072847682oo923 81271 2.2764227642276422 81272 3.7894736842o05265 Name: avg_votes_div_votes, Length: 81273, dtype: object . Now let&#39;s replace multiple values. This time each number with the first letter of its english equivalent. e.g. &#39;2&#39; with &#39;t&#39;, and &#39;5&#39; with &#39;f&#39;. . df[&#39;avg_votes_div_votes&#39;].replace({&#39;1&#39;:&#39;o&#39;,&#39;2&#39;:&#39;t&#39;,&#39;3&#39;:&#39;t&#39;,&#39;4&#39;:&#39;f&#39;,&#39;5&#39;:&#39;f&#39;,&#39;6&#39;:&#39;s&#39;,&#39;7&#39;:&#39;s&#39;,&#39;8&#39;:&#39;e&#39;,&#39;9&#39;:&#39;n&#39;,&#39;0&#39;:&#39;z&#39;}, regex=True) . 0 o.otfnfzfznsetftsf 1 t.ffztnttnssszeoes 2 o.tteznftteznftteo 3 z.tfsszstnztftsnff 4 o.tzotsnestzotsnes ... 81268 t.tesssotttesssots 81269 o.ftnfoossfszfeett 81270 o.tnzstefssetoontt 81271 t.tssfttssfttssftt 81272 t.senfstseftozftsf Name: avg_votes_div_votes, Length: 81273, dtype: object . Acknowledgements: . I&#39;d like to acknowledge the following sources provided inspiration for the examples in this blog post. . Pandas Tutorial, Pycon 2015, Brandon Rhodes | 10 minutes to Pandas, Pandas development team | Pandas cheat Sheet, Dataquest | .",
            "url": "https://sams101.github.io/DataScience/pandas/python/2020/06/07/Pandas-Basics.html",
            "relUrl": "/pandas/python/2020/06/07/Pandas-Basics.html",
            "date": " • Jun 7, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sams101.github.io/DataScience/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your page/pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sams101.github.io/DataScience/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sams101.github.io/DataScience/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
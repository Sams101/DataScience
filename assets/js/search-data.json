{
  
    
        "post0": {
            "title": "Keras CNN - Malaria custom data",
            "content": "import os import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from matplotlib.image import imread . pwd . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study&#39; . my_data_dir = &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study&#39; + &#39;/DATA/Malaria_cells&#39; . my_data_dir . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study/DATA/Malaria_cells&#39; . os.listdir(my_data_dir) . [&#39;test&#39;, &#39;train&#39;] . train_path = my_data_dir + &#39;/train&#39; test_path = my_data_dir + &#39;/test&#39; . test_path . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study/DATA/Malaria_cells/test&#39; . os.listdir(train_path) . [&#39;parasitized&#39;, &#39;uninfected&#39;] . os.listdir(train_path + &#39;/parasitized&#39;)[0] . &#39;C189P150ThinF_IMG_20151203_142224_cell_84.png&#39; . para_cell = train_path + &#39;/parasitized/&#39; + &#39;C189P150ThinF_IMG_20151203_142224_cell_84.png&#39; . para_cell = imread(para_cell) . plt.imshow(para_cell) . &lt;matplotlib.image.AxesImage at 0x7f89039e5810&gt; . para_cell.shape . (121, 118, 3) . Image count . len( os.listdir(train_path + &#39;/parasitized/&#39;)) . 12479 . len( os.listdir(train_path + &#39;/uninfected/&#39;)) . 12480 . Average image dimension . dim1 = [] dim2 = [] for image_filename in os.listdir(train_path + &#39;/uninfected/&#39;): img = imread(train_path + &#39;/uninfected/&#39; + image_filename) d1, d2, colours = img.shape dim1.append(d1) dim2.append(d2) . sns.jointplot(x=dim1, y=dim2); . np.mean(dim1) . 131.64820899110507 . np.mean(dim2) . 131.4041990544114 . Set default image shape . image_shape = (130, 130, 3) . Prepare Data . from tensorflow.keras.preprocessing.image import ImageDataGenerator . image_gen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, rescale =1/255, shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode=&#39;nearest&#39; ) . plt.imshow(para_cell); . plt.imshow(image_gen.random_transform(para_cell)); . Flow images from Directory . image_gen.flow_from_directory(train_path) . Found 24958 images belonging to 2 classes. . &lt;keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x7f88e46b9450&gt; . image_gen.flow_from_directory(test_path) . Found 2600 images belonging to 2 classes. . &lt;keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x7f88e3c4dd50&gt; . Create Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Dropout, MaxPool2D . model = Sequential() model.add(Conv2D(filters=32, kernel_size=(3,3), activation=&#39;relu&#39;, input_shape=image_shape)) model.add(MaxPool2D(pool_size=(2,2))) model.add(Conv2D(filters=32, kernel_size=(3,3), activation=&#39;relu&#39;, input_shape=image_shape)) model.add(MaxPool2D(pool_size=(2,2))) model.add(Conv2D(filters=32, kernel_size=(3,3), activation=&#39;relu&#39;, input_shape=image_shape)) model.add(MaxPool2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss = &#39;binary_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_18 (Conv2D) (None, 128, 128, 32) 896 _________________________________________________________________ max_pooling2d_6 (MaxPooling2 (None, 64, 64, 32) 0 _________________________________________________________________ conv2d_19 (Conv2D) (None, 62, 62, 32) 9248 _________________________________________________________________ max_pooling2d_7 (MaxPooling2 (None, 31, 31, 32) 0 _________________________________________________________________ conv2d_20 (Conv2D) (None, 29, 29, 32) 9248 _________________________________________________________________ max_pooling2d_8 (MaxPooling2 (None, 14, 14, 32) 0 _________________________________________________________________ flatten_5 (Flatten) (None, 6272) 0 _________________________________________________________________ dense_10 (Dense) (None, 128) 802944 _________________________________________________________________ dropout_5 (Dropout) (None, 128) 0 _________________________________________________________________ dense_11 (Dense) (None, 1) 129 ================================================================= Total params: 822,465 Trainable params: 822,465 Non-trainable params: 0 _________________________________________________________________ . image_shape . (130, 130, 3) . Early Stopping . from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor=&#39;val_loss&#39;, patience=2) . Train Model . batch_size = 256 . train_image_gen = image_gen.flow_from_directory(train_path, target_size=image_shape[:2], color_mode=&#39;rgb&#39;, batch_size=batch_size, shuffle = False, class_mode=&#39;binary&#39;) . Found 24958 images belonging to 2 classes. . test_image_gen = image_gen.flow_from_directory(test_path, target_size=image_shape[:2], color_mode=&#39;rgb&#39;, batch_size=batch_size, shuffle = False, class_mode=&#39;binary&#39;) . Found 2600 images belonging to 2 classes. . train_image_gen.class_indices . {&#39;parasitized&#39;: 0, &#39;uninfected&#39;: 1} . model.fit_generator(train_image_gen, epochs=3, validation_data=test_image_gen, callbacks = [early_stop]) . Epoch 1/3 98/98 [==============================] - 1195s 12s/step - loss: 0.3403 - accuracy: 0.8714 - val_loss: 0.1936 - val_accuracy: 0.9385 Epoch 2/3 98/98 [==============================] - 1857s 19s/step - loss: 0.2553 - accuracy: 0.9203 - val_loss: 0.2006 - val_accuracy: 0.9350 Epoch 3/3 98/98 [==============================] - 1208s 12s/step - loss: 0.1927 - accuracy: 0.9381 - val_loss: 0.2146 - val_accuracy: 0.9277 . &lt;tensorflow.python.keras.callbacks.History at 0x7f84e5815f90&gt; . Save Model . from tensorflow.keras.models import load_model model.save(&#39;marlaria_detector.h5&#39;) . Evaluate Model . losses = pd.DataFrame(model.history.history) . losses[[&#39;accuracy&#39;, &#39;val_accuracy&#39;]].plot(); . losses[[&#39;loss&#39;, &#39;val_loss&#39;]].plot(); . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . model.evaluate_generator(test_image_gen) . [0.21619475903836163, 0.9288462] . # https://datascience.stackexchange.com/questions/13894/how-to-get-predictions-with-predict-generator-on-streaming-test-data-in-keras pred_probabilities = model.predict_generator(test_image_gen, workers = 0) . pred_probabilities[1:10] . array([[1.7757118e-03], [1.7362297e-02], [1.8446982e-02], [2.9427925e-01], [3.6358833e-06], [1.4031053e-02], [8.5830688e-06], [1.0720789e-03], [5.3215027e-04]], dtype=float32) . trueClass = test_image_gen.classes . predictions = pred_probabilities &gt; 0.5 . predictions . array([[False], [False], [False], ..., [ True], [ True], [ True]]) . from sklearn.metrics import classification_report, confusion_matrix . print(classification_report(trueClass, predictions )) . precision recall f1-score support 0 0.98 0.87 0.92 1300 1 0.88 0.98 0.93 1300 accuracy 0.92 2600 macro avg 0.93 0.92 0.92 2600 weighted avg 0.93 0.92 0.92 2600 . confusion_matrix( test_image_gen.classes, predictions ) . array([[1127, 173], [ 23, 1277]]) . Predict Image . from tensorflow.keras.preprocessing import image . para_cell = train_path + &#39;/parasitized/&#39; + &#39;C189P150ThinF_IMG_20151203_142224_cell_84.png&#39; . para_cell . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study/DATA/Malaria_cells/train/parasitized/C189P150ThinF_IMG_20151203_142224_cell_84.png&#39; . my_image = image.load_img(para_cell, target_size=image_shape) my_image . my_image = image.img_to_array(my_image) . my_image.shape . (130, 130, 3) . my_image.reshape(1, 130, 130, 3).shape . (1, 130, 130, 3) . model.predict(my_image.reshape(1, 130, 130, 3)) . array([[0.]], dtype=float32) . train_image_gen.class_indices . {&#39;parasitized&#39;: 0, &#39;uninfected&#39;: 1} . test_image_gen.class_indices . {&#39;parasitized&#39;: 0, &#39;uninfected&#39;: 1} .",
            "url": "https://sams101.github.io/DataScience/keras/cnn/tensorflow/classification/python/2020/10/07/Keras_CNN_Malaria_custom_data.html",
            "relUrl": "/keras/cnn/tensorflow/classification/python/2020/10/07/Keras_CNN_Malaria_custom_data.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tensorflow Cert - Study Plan",
            "content": "Books . ✅ Deep learning with Python | ✅ Hands on Machine Learning | . Courses . ✅ Coursera: TensorFlow in Practice Specialization | ✅ MIT: MIT 6.S191 Introduction to Deep Learning | ✅ Udemy: Tensorflow 2 | ✅ Udemy: Machine Learning | ✅ Udemy: learning python for data analysis and visualisation | . Example problems . https://www.tensorflow.org/tutorials/images/classification | https://www.tensorflow.org/tutorials/text/word_embeddings | . Topics . ✅ Tensorflow Datasets | ✅ ANN binary, multi-class, regression, multi-regression | CNN Transfer learning | CNN ImageDataGenerator | CNN Conv2D, MaxPool, Optimisers | CNN binary, multi-class | CNN view Conv2D filters in row | CNN single and multi object detection | RNN LSTM | RNN bag of words | RNN Word embeddings | RNN music/text/audio generations | ✅ Tensorflow | Autoencoders (dimensionality reduciton, anomoly detection, image denoising | . Data Sets . Cats and Dogs | Marlaria Cells | MNIST fashion | MNIST digits | Cifar10 animals, cars, planes | Cifar100 | Titanic | Iris | Breast Cancer | Boston Houses | . Memorise Exercises . all Jose portilla | all coursera | all MIT | all deep learning with python | all hands-on machine learning NNs | 2 above Tf examples | . (1) Build and train neural network models using TensorFlow 2.x . ✅ UseTensorFlow2.x. | ✅ Build, compile and train machine learning (ML) models using TensorFlow. | ✅ Preprocess data to get it ready for use in a model. | ✅ Use models to predict results. | ✅ Build sequential models with multiplelayers. | ✅ Build and train models for binary classification. | ✅ Build and train models for multi-classcategorization. | ✅ Plot loss and accuracy of a trained model. | ✅ Identify strategies to prevent over fitting, including augmentation and dropout. | ❏ Use pretrained models (transfer learning). | ❏ Extract features from pre-trained models. | ✅ Ensure that inputs to a model are in the correct shape. | ✅ Ensure that you can match test data to the input shape of a neural network. | ❏ Ensure you can match output data of a neural network to specified input shape for test data. | ✅ Understand batch loading of data. | ✅ Use callbacks to trigger the end of training cycles. | ❏ Use datasets from different sources. | ❏ Use datasets in different formats, including json and csv. | ❏ Use datasets from tf.data.datasets. | . (2) Image classification . ✅ Define Convolutional neural networks with Conv2D and pooling layers. | ✅ Build and train models to process real-world image datasets. | ✅ Understand how to use convolutions to improve your neural network. | ❏ Use real-world images in different shapes and sizes.. | ✅ Use image augmentation to prevent overfitting. | ✅ Use ImageDataGenerator. | ✅ Understand how ImageDataGenerator labels images based on the directory structure. | . (3) Natural language processing (NLP) . ❏ Build natural language processing systems using TensorFlow. | ❏ Prepare text to use in TensorFlowmodels. | ❏ Build models that identify the category of a piece of text using binary categorization | ❏ Build models that identify the category of a piece of text using multi-classcategorization | ❏ Use word embeddings in your TensorFlow model. | ❏ Use LSTMs in your model to classify text for either binary or multi-class categorization. | ❏ Add RNN and GRU layers to your model. | ❏ Use RNNS, LSTMs, GRUs and CNNs in models that work with text. | ❏ Train LSTMs on existing text to generate text(such as songs and poetry) | . (4) Time series, sequences and predictions . ❏ Train, tune and use timeseries, sequence and prediction models. | ❏ Prepare data for timeseries learning. | ❏ Understand MeanAverageError (MAE) and how it can be used to evaluate accuracy of sequence models. | ❏ Use RNNs and CNNs for time series, sequence and forecasting models. | ❏ Identify when to use trailing versus centred windows. | ❏ Use TensorFlow for forecasting. | ❏ Prepare features and labels. | ❏ Identify and compensate for sequence bias. | ❏ Adjust the learning rate dynamically in timeseries, sequence and prediction models. | .",
            "url": "https://sams101.github.io/DataScience/keras/cnn/tensorflow/study/python/2020/10/04/Tensorflow_Certificate_Study_Plan.html",
            "relUrl": "/keras/cnn/tensorflow/study/python/2020/10/04/Tensorflow_Certificate_Study_Plan.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Keras CNN - CIFAR-10 Multiple Classification",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt . from tensorflow.keras.datasets import cifar10 (X_train, y_train), (X_test, y_test) = cifar10.load_data() . X_train.shape . (50000, 32, 32, 3) . plt.imshow(X_train[4]); . Normalise Data . X_train.max(), . (255,) . X_train = X_train/255 X_test = X_test/255 . Label Data . from tensorflow.keras.utils import to_categorical . y_train.max() . 9 . y_train = to_categorical(y_train, 10) y_test = to_categorical(y_test , 10) . Build Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten . model = Sequential() model.add(Conv2D(filters=32, kernel_size=(4,4), activation=&#39;relu&#39;, input_shape=(32,32,3))) model.add(MaxPool2D(2,2)) model.add(Conv2D(filters=32, kernel_size=(4,4), activation=&#39;relu&#39;, input_shape=(32,32,3))) model.add(MaxPool2D(2,2)) model.add(Flatten()) model.add(Dense(256, activation=&#39;relu&#39;)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_4 (Conv2D) (None, 29, 29, 32) 1568 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 11, 11, 32) 16416 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 800) 0 _________________________________________________________________ dense_4 (Dense) (None, 256) 205056 _________________________________________________________________ dense_5 (Dense) (None, 10) 2570 ================================================================= Total params: 225,610 Trainable params: 225,610 Non-trainable params: 0 _________________________________________________________________ . from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor=&#39;val_loss&#39;, patience=4) . model.fit(X_train, y_train, epochs=20, batch_size=516, validation_data=(X_test, y_test), callbacks = [early_stop], verbose= 1) . Train on 50000 samples, validate on 10000 samples Epoch 1/20 50000/50000 [==============================] - 26s 526us/sample - loss: 1.9519 - accuracy: 0.3024 - val_loss: 2.1164 - val_accuracy: 0.2693 Epoch 2/20 50000/50000 [==============================] - 24s 471us/sample - loss: 1.6544 - accuracy: 0.4150 - val_loss: 1.6044 - val_accuracy: 0.4418 Epoch 3/20 50000/50000 [==============================] - 22s 450us/sample - loss: 1.5079 - accuracy: 0.4694 - val_loss: 1.3855 - val_accuracy: 0.5176 Epoch 4/20 50000/50000 [==============================] - 24s 479us/sample - loss: 1.4067 - accuracy: 0.5047 - val_loss: 1.3563 - val_accuracy: 0.5168 Epoch 5/20 50000/50000 [==============================] - 24s 474us/sample - loss: 1.3298 - accuracy: 0.5361 - val_loss: 1.2736 - val_accuracy: 0.5526 Epoch 6/20 50000/50000 [==============================] - 24s 475us/sample - loss: 1.2627 - accuracy: 0.5588 - val_loss: 1.2547 - val_accuracy: 0.5565 Epoch 7/20 50000/50000 [==============================] - 23s 460us/sample - loss: 1.2135 - accuracy: 0.5789 - val_loss: 1.1838 - val_accuracy: 0.5852 Epoch 8/20 50000/50000 [==============================] - 23s 460us/sample - loss: 1.1594 - accuracy: 0.5952 - val_loss: 1.1685 - val_accuracy: 0.5939 Epoch 9/20 50000/50000 [==============================] - 24s 471us/sample - loss: 1.1193 - accuracy: 0.6114 - val_loss: 1.1167 - val_accuracy: 0.6154 Epoch 10/20 50000/50000 [==============================] - 30s 597us/sample - loss: 1.0799 - accuracy: 0.6241 - val_loss: 1.0816 - val_accuracy: 0.6262 Epoch 11/20 50000/50000 [==============================] - 27s 540us/sample - loss: 1.0402 - accuracy: 0.6414 - val_loss: 1.0576 - val_accuracy: 0.6281 Epoch 12/20 50000/50000 [==============================] - 23s 468us/sample - loss: 1.0041 - accuracy: 0.6527 - val_loss: 1.0503 - val_accuracy: 0.6382 Epoch 13/20 50000/50000 [==============================] - 24s 471us/sample - loss: 0.9675 - accuracy: 0.6637 - val_loss: 1.0547 - val_accuracy: 0.6341 Epoch 14/20 50000/50000 [==============================] - 23s 470us/sample - loss: 0.9371 - accuracy: 0.6747 - val_loss: 1.0385 - val_accuracy: 0.6376 Epoch 15/20 50000/50000 [==============================] - 23s 465us/sample - loss: 0.8996 - accuracy: 0.6883 - val_loss: 1.0758 - val_accuracy: 0.6306 Epoch 16/20 50000/50000 [==============================] - 23s 459us/sample - loss: 0.8726 - accuracy: 0.7005 - val_loss: 1.1459 - val_accuracy: 0.6130 Epoch 17/20 50000/50000 [==============================] - 23s 469us/sample - loss: 0.8357 - accuracy: 0.7116 - val_loss: 1.0751 - val_accuracy: 0.6276 Epoch 18/20 50000/50000 [==============================] - 22s 440us/sample - loss: 0.8131 - accuracy: 0.7188 - val_loss: 0.9483 - val_accuracy: 0.6722 Epoch 19/20 50000/50000 [==============================] - 23s 466us/sample - loss: 0.7866 - accuracy: 0.7296 - val_loss: 0.9500 - val_accuracy: 0.6725 Epoch 20/20 50000/50000 [==============================] - 24s 471us/sample - loss: 0.7566 - accuracy: 0.7388 - val_loss: 1.0179 - val_accuracy: 0.6590 . &lt;tensorflow.python.keras.callbacks.History at 0x7f981b631510&gt; . losses = pd.DataFrame(model.history.history) . losses[[&#39;loss&#39;, &#39;val_loss&#39;]].plot() . &lt;AxesSubplot:&gt; . losses[[&#39;accuracy&#39;, &#39;val_accuracy&#39;]].plot() . &lt;AxesSubplot:&gt; . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . print(model.metrics_names) print(model.evaluate(X_test, y_test, verbose=0)) . [&#39;loss&#39;, &#39;accuracy&#39;] [1.0178816331863403, 0.659] . from sklearn.metrics import classification_report, confusion_matrix . X_test.shape . (10000, 32, 32, 3) . predictions = model.predict_classes(X_test) . predictions . array([3, 8, 8, ..., 5, 4, 7]) . y_test . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 0., 0., ..., 0., 1., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 1., 0., 0.]], dtype=float32) . y_test = y_test.argmax(axis=1) . y_test . array([3, 8, 8, ..., 5, 1, 7]) . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.83 0.66 0.74 1000 1 0.90 0.61 0.73 1000 2 0.68 0.44 0.53 1000 3 0.45 0.57 0.51 1000 4 0.53 0.71 0.61 1000 5 0.54 0.58 0.56 1000 6 0.67 0.81 0.74 1000 7 0.81 0.63 0.71 1000 8 0.79 0.76 0.78 1000 9 0.65 0.82 0.73 1000 accuracy 0.66 10000 macro avg 0.69 0.66 0.66 10000 weighted avg 0.69 0.66 0.66 10000 . confusion_matrix(y_test, predictions) . array([[660, 14, 41, 40, 49, 18, 25, 11, 77, 65], [ 15, 607, 9, 30, 17, 17, 37, 3, 50, 215], [ 46, 2, 437, 112, 156, 95, 99, 29, 10, 14], [ 5, 4, 41, 572, 98, 159, 76, 16, 10, 19], [ 6, 2, 40, 84, 714, 35, 62, 40, 11, 6], [ 5, 2, 18, 216, 86, 580, 46, 35, 3, 9], [ 0, 1, 24, 72, 55, 22, 810, 8, 2, 6], [ 7, 0, 13, 65, 131, 107, 13, 631, 4, 29], [ 34, 18, 13, 33, 32, 20, 17, 2, 759, 72], [ 15, 22, 10, 36, 21, 19, 19, 8, 30, 820]]) . conf_matrix = confusion_matrix(y_test, predictions) * (np.ones((10,10)) - np.eye(10)) import seaborn as sns plt.figure(figsize=(12,8)) sns.heatmap(conf_matrix, annot=True); . Predicing a specific image . new_image = X_test[8] plt.imshow(new_image); . model.predict_classes(new_image.reshape(1,32,32,3)) . array([3]) .",
            "url": "https://sams101.github.io/DataScience/keras/cnn/tensorflow/classification/python/2020/10/04/Keras_CNN_CIFAR10-multiclassification.html",
            "relUrl": "/keras/cnn/tensorflow/classification/python/2020/10/04/Keras_CNN_CIFAR10-multiclassification.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Keras CNN - MNIST number classification",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt . from tensorflow.keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() . Visualizing the Image Data . x_train.shape . (60000, 28, 28) . single_image = x_train[0] . single_image.shape . (28, 28) . plt.imshow(single_image) . &lt;matplotlib.image.AxesImage at 0x7fca5dd7a790&gt; . PreProcess . Label Data . y_train . array([5, 0, 4, ..., 5, 6, 8], dtype=uint8) . y_test . array([7, 2, 1, ..., 4, 5, 6], dtype=uint8) . from tensorflow.keras.utils import to_categorical . y_train.shape . (60000,) . y_example = to_categorical(y_train) . y_example.shape . (60000, 10) . y_example[0] . array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32) . y_cat_test = to_categorical(y_test, 10) y_cat_train = to_categorical(y_train,10) . Feature Data . # normalize X data single_image.max(), . (255,) . single_image.min() . 0 . x_train = x_train/255 x_test = x_test/255 . scaled_single = x_train[0] . scaled_single.max() . 1.0 . plt.imshow(scaled_single) . &lt;matplotlib.image.AxesImage at 0x7fca5de238d0&gt; . Reshape Data . x_train.shape . (60000, 28, 28) . x_test.shape . (10000, 28, 28) . # Reshape to include channel dimension x_train = x_train.reshape(60000, 28, 28, 1) x_train.shape . (60000, 28, 28, 1) . x_test = x_test.reshape(10000, 28, 28, 1) x_test.shape . (10000, 28, 28, 1) . Training the Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten import tensorflow as tf #import os #os.environ[&#39;KMP_DUPLICATE_LIB_OK&#39;]=&#39;True&#39; . model = Sequential() model.add(Conv2D(filters=16, kernel_size=(4,4), activation=&#39;relu&#39;,input_shape=(28,28,1))) model.add(MaxPool2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(20, activation=&#39;relu&#39;)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 25, 25, 16) 272 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 12, 12, 16) 0 _________________________________________________________________ flatten (Flatten) (None, 2304) 0 _________________________________________________________________ dense (Dense) (None, 20) 46100 _________________________________________________________________ dense_1 (Dense) (None, 10) 210 ================================================================= Total params: 46,582 Trainable params: 46,582 Non-trainable params: 0 _________________________________________________________________ . from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor = &#39;val_loss&#39;, patience=2) . Save and Load data and model . from numpy import save from numpy import load from tensorflow.keras.models import load_model save(&#39;x_train.npy&#39;, x_train) save(&#39;y_cat_train.npy&#39;, y_cat_train) model.save(&#39;my_model.h5&#39;) . load(&#39;x_train.npy&#39;) load(&#39;y_cat_train.npy&#39;); model = load_model(&#39;my_model.h5&#39;) . Train Model . model.fit(x_train, y_cat_train, epochs=30, batch_size=256, validation_data=(x_test, y_cat_test), verbose = 1) . Train on 60000 samples, validate on 10000 samples Epoch 1/30 60000/60000 [==============================] - 10s 168us/sample - loss: 0.5176 - accuracy: 0.8474 - val_loss: 0.1848 - val_accuracy: 0.9462 Epoch 2/30 60000/60000 [==============================] - 9s 142us/sample - loss: 0.1605 - accuracy: 0.9535 - val_loss: 0.1215 - val_accuracy: 0.9641 Epoch 3/30 60000/60000 [==============================] - 9s 148us/sample - loss: 0.1113 - accuracy: 0.9681 - val_loss: 0.0949 - val_accuracy: 0.9711 Epoch 4/30 60000/60000 [==============================] - 9s 150us/sample - loss: 0.0909 - accuracy: 0.9740 - val_loss: 0.0809 - val_accuracy: 0.9746 Epoch 5/30 60000/60000 [==============================] - 9s 148us/sample - loss: 0.0748 - accuracy: 0.9782 - val_loss: 0.0695 - val_accuracy: 0.9786 Epoch 6/30 60000/60000 [==============================] - 9s 157us/sample - loss: 0.0643 - accuracy: 0.9814 - val_loss: 0.0632 - val_accuracy: 0.9790 Epoch 7/30 60000/60000 [==============================] - 9s 156us/sample - loss: 0.0575 - accuracy: 0.9832 - val_loss: 0.0569 - val_accuracy: 0.9814 Epoch 8/30 60000/60000 [==============================] - 10s 171us/sample - loss: 0.0524 - accuracy: 0.9846 - val_loss: 0.0590 - val_accuracy: 0.9815 Epoch 9/30 60000/60000 [==============================] - 11s 179us/sample - loss: 0.0486 - accuracy: 0.9859 - val_loss: 0.0548 - val_accuracy: 0.9824 Epoch 10/30 60000/60000 [==============================] - 11s 179us/sample - loss: 0.0425 - accuracy: 0.9873 - val_loss: 0.0523 - val_accuracy: 0.9832 Epoch 11/30 60000/60000 [==============================] - 14s 228us/sample - loss: 0.0406 - accuracy: 0.9884 - val_loss: 0.0519 - val_accuracy: 0.9821 Epoch 12/30 60000/60000 [==============================] - 11s 187us/sample - loss: 0.0366 - accuracy: 0.9893 - val_loss: 0.0490 - val_accuracy: 0.9827 Epoch 13/30 60000/60000 [==============================] - 12s 196us/sample - loss: 0.0347 - accuracy: 0.9903 - val_loss: 0.0472 - val_accuracy: 0.9836 Epoch 14/30 60000/60000 [==============================] - 13s 214us/sample - loss: 0.0319 - accuracy: 0.9905 - val_loss: 0.0526 - val_accuracy: 0.9828 Epoch 15/30 60000/60000 [==============================] - 13s 212us/sample - loss: 0.0299 - accuracy: 0.9911 - val_loss: 0.0467 - val_accuracy: 0.9844 Epoch 16/30 60000/60000 [==============================] - 10s 165us/sample - loss: 0.0278 - accuracy: 0.9918 - val_loss: 0.0487 - val_accuracy: 0.9841 Epoch 17/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0257 - accuracy: 0.9926 - val_loss: 0.0483 - val_accuracy: 0.9840 Epoch 18/30 60000/60000 [==============================] - 12s 193us/sample - loss: 0.0246 - accuracy: 0.9929 - val_loss: 0.0475 - val_accuracy: 0.9837 Epoch 19/30 60000/60000 [==============================] - 13s 211us/sample - loss: 0.0229 - accuracy: 0.9933 - val_loss: 0.0523 - val_accuracy: 0.9828 Epoch 20/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0210 - accuracy: 0.9941 - val_loss: 0.0470 - val_accuracy: 0.9843 Epoch 21/30 60000/60000 [==============================] - 10s 174us/sample - loss: 0.0208 - accuracy: 0.9937 - val_loss: 0.0488 - val_accuracy: 0.9829 Epoch 22/30 60000/60000 [==============================] - 12s 195us/sample - loss: 0.0183 - accuracy: 0.9948 - val_loss: 0.0524 - val_accuracy: 0.9840 Epoch 23/30 60000/60000 [==============================] - 11s 177us/sample - loss: 0.0178 - accuracy: 0.9949 - val_loss: 0.0538 - val_accuracy: 0.9830 Epoch 24/30 60000/60000 [==============================] - 11s 178us/sample - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.0482 - val_accuracy: 0.9840 Epoch 25/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0159 - accuracy: 0.9957 - val_loss: 0.0527 - val_accuracy: 0.9838 Epoch 26/30 60000/60000 [==============================] - 10s 173us/sample - loss: 0.0140 - accuracy: 0.9962 - val_loss: 0.0532 - val_accuracy: 0.9833 Epoch 27/30 60000/60000 [==============================] - 10s 169us/sample - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.0513 - val_accuracy: 0.9847 Epoch 28/30 60000/60000 [==============================] - 10s 170us/sample - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0584 - val_accuracy: 0.9822 Epoch 29/30 60000/60000 [==============================] - 11s 179us/sample - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.0480 - val_accuracy: 0.9856 Epoch 30/30 60000/60000 [==============================] - 11s 178us/sample - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.0512 - val_accuracy: 0.9851 . &lt;tensorflow.python.keras.callbacks.History at 0x7fca5ae89bd0&gt; . Evaluate the Model . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . losses = pd.DataFrame(model.history.history) . losses[[&#39;loss&#39;,&#39;val_loss&#39;]].plot() . &lt;AxesSubplot:&gt; . losses[[&#39;accuracy&#39;,&#39;val_accuracy&#39;]].plot() . &lt;AxesSubplot:&gt; . print(model.metrics_names) print(model.evaluate(x_test, y_cat_test, verbose=0)) . [&#39;loss&#39;, &#39;accuracy&#39;] [0.05124218984251929, 0.9851] . from sklearn.metrics import classification_report, confusion_matrix . predictions = model.predict_classes(x_test) . predictions[0] . 7 . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.98 0.99 0.99 980 1 0.99 1.00 0.99 1135 2 0.98 0.98 0.98 1032 3 0.98 0.99 0.99 1010 4 0.99 0.99 0.99 982 5 0.98 0.99 0.98 892 6 0.99 0.98 0.99 958 7 0.97 0.99 0.98 1028 8 0.99 0.98 0.98 974 9 0.99 0.96 0.98 1009 accuracy 0.99 10000 macro avg 0.99 0.98 0.98 10000 weighted avg 0.99 0.99 0.99 10000 . print(confusion_matrix(y_test, predictions)) . [[ 974 0 2 0 0 0 2 2 0 0] [ 0 1130 2 0 0 1 1 1 0 0] [ 2 2 1012 3 0 0 2 8 2 1] [ 0 0 1 1001 0 4 0 2 2 0] [ 0 1 0 0 972 0 2 1 0 6] [ 1 0 1 6 0 881 3 0 0 0] [ 5 2 1 0 3 5 941 0 1 0] [ 0 1 4 3 1 0 0 1017 1 1] [ 6 0 4 2 0 4 1 4 950 3] [ 2 2 1 5 10 6 0 9 1 973]] . import seaborn as sns . plt.figure(figsize=(12,8)) sns.heatmap(confusion_matrix(y_test, predictions), annot=True); . Predict a given image . new_num = x_test[4] . plt.imshow(new_num.reshape(28,28)); . model.predict(new_num.reshape(1,28,28,1)) . array([[1.5632841e-10, 5.4694190e-11, 8.7363444e-10, 4.7918874e-11, 9.9992108e-01, 3.0790169e-11, 7.6901131e-11, 2.0181250e-07, 3.2277640e-07, 7.8314668e-05]], dtype=float32) . model.predict(new_num.reshape(1,28,28,1)).argmax() . 4 . # or you can use this model.predict_classes(new_num.reshape(1,28,28,1)) . array([4]) .",
            "url": "https://sams101.github.io/DataScience/keras/cnn/tensorflow/classification/python/2020/10/04/Keras_CNN-MNIST.html",
            "relUrl": "/keras/cnn/tensorflow/classification/python/2020/10/04/Keras_CNN-MNIST.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Keras CNN - Fashion_MNIST - Multiclass",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt . from tensorflow.keras.datasets import fashion_mnist (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data() . X_train.shape . (60000, 28, 28) . plt.imshow(X_train[0]); . y_train[0] . 9 . PreProcess Data . X_train.max() . 255 . # Normalise Data X_train = X_train / 255 X_test = X_test / 255 . X_train.shape, X_test.shape . ((60000, 28, 28), (10000, 28, 28)) . # Reshape data X_train = X_train.reshape(60000, 28, 28, 1) X_test = X_test.reshape(10000, 28, 28, 1) . # One Hot encode Lables y_train . array([9, 0, 0, ..., 3, 0, 5], dtype=uint8) . from tensorflow.keras.utils import to_categorical y_train = to_categorical(y_train) y_test = to_categorical(y_test) . y_train . array([[0., 0., 0., ..., 0., 0., 1.], [1., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) . Build Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten . model = Sequential() model.add(Conv2D(filters=32, kernel_size=(4,4), activation=&#39;relu&#39;, input_shape=(28,28,1))) model.add(MaxPool2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer= &#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_6 (Conv2D) (None, 25, 25, 32) 544 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32) 0 _________________________________________________________________ flatten_4 (Flatten) (None, 4608) 0 _________________________________________________________________ dense_8 (Dense) (None, 128) 589952 _________________________________________________________________ dense_9 (Dense) (None, 10) 1290 ================================================================= Total params: 591,786 Trainable params: 591,786 Non-trainable params: 0 _________________________________________________________________ . # Early Stopping from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor=&#39;val_loss&#39;, patience=3) . model.fit(X_train, y_train, epochs=30, batch_size=128, validation_data=(X_test, y_test), callbacks = [early_stop], verbose=2) . Train on 60000 samples, validate on 10000 samples Epoch 1/30 60000/60000 - 25s - loss: 0.4568 - accuracy: 0.8421 - val_loss: 0.3588 - val_accuracy: 0.8753 Epoch 2/30 60000/60000 - 25s - loss: 0.3125 - accuracy: 0.8899 - val_loss: 0.3288 - val_accuracy: 0.8840 Epoch 3/30 60000/60000 - 22s - loss: 0.2672 - accuracy: 0.9041 - val_loss: 0.2898 - val_accuracy: 0.8953 Epoch 4/30 60000/60000 - 23s - loss: 0.2373 - accuracy: 0.9141 - val_loss: 0.2881 - val_accuracy: 0.8928 Epoch 5/30 60000/60000 - 25s - loss: 0.2156 - accuracy: 0.9209 - val_loss: 0.2590 - val_accuracy: 0.9052 Epoch 6/30 60000/60000 - 24s - loss: 0.1948 - accuracy: 0.9295 - val_loss: 0.2723 - val_accuracy: 0.9052 Epoch 7/30 60000/60000 - 23s - loss: 0.1764 - accuracy: 0.9352 - val_loss: 0.2630 - val_accuracy: 0.9075 Epoch 8/30 60000/60000 - 24s - loss: 0.1605 - accuracy: 0.9420 - val_loss: 0.2649 - val_accuracy: 0.9099 . &lt;tensorflow.python.keras.callbacks.History at 0x7fb658208cd0&gt; . losses = pd.DataFrame(model.history.history) . losses[[&#39;loss&#39;,&#39;val_loss&#39;]].plot() . &lt;AxesSubplot:&gt; . losses[[&#39;accuracy&#39;,&#39;val_accuracy&#39;]].plot() . &lt;AxesSubplot:&gt; . Evaluate Model . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . model.evaluate(X_test, y_test, verbose=0) . [0.2649084388911724, 0.9099] . from sklearn.metrics import classification_report, confusion_matrix . predictions = model.predict_classes(X_test) predictions . array([9, 2, 1, ..., 8, 1, 5]) . y_test . array([[0., 0., 0., ..., 0., 0., 1.], [0., 0., 1., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 1., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) . y_test.argmax(axis=1) . array([9, 2, 1, ..., 8, 1, 5]) . print(classification_report(y_test.argmax(axis=1), predictions)) . precision recall f1-score support 0 0.80 0.91 0.85 1000 1 0.97 0.99 0.98 1000 2 0.86 0.88 0.87 1000 3 0.91 0.93 0.92 1000 4 0.85 0.87 0.86 1000 5 0.99 0.96 0.97 1000 6 0.82 0.63 0.71 1000 7 0.94 0.98 0.96 1000 8 0.98 0.98 0.98 1000 9 0.98 0.96 0.97 1000 accuracy 0.91 10000 macro avg 0.91 0.91 0.91 10000 weighted avg 0.91 0.91 0.91 10000 . print(confusion_matrix(y_test.argmax(axis=1), predictions)) . [[912 1 7 13 4 1 56 1 5 0] [ 2 989 1 5 1 0 1 0 1 0] [ 23 2 875 10 54 1 34 0 1 0] [ 14 17 5 932 18 0 11 0 3 0] [ 1 3 56 33 871 1 34 0 1 0] [ 0 0 0 0 0 962 0 29 0 9] [178 3 72 30 79 0 631 0 7 0] [ 0 0 0 0 0 2 0 984 0 14] [ 5 0 3 4 0 1 0 3 984 0] [ 0 0 0 0 0 6 0 34 1 959]] . Predict specific object . plt.imshow(X_test[0]); . X_test[0].shape . (28, 28, 1) . model.predict(X_test[0].reshape(1,28,28,1)).argmax() # 9 is an Ankle Boot . 9 .",
            "url": "https://sams101.github.io/DataScience/keras/cnn/tensorflow/classification/python/2020/10/04/Keras-CNN-Fashion-MNIST-multiclass.html",
            "relUrl": "/keras/cnn/tensorflow/classification/python/2020/10/04/Keras-CNN-Fashion-MNIST-multiclass.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Keras ANNs Basics - Regression - Diamond Prices",
            "content": "import pandas as pd import seaborn as sns import matplotlib.pyplot as plt . df = pd.read_csv(&#39;DATA/fake_reg.csv&#39;) . df.head() . price feature1 feature2 . 0 461.527929 | 999.787558 | 999.766096 | . 1 548.130011 | 998.861615 | 1001.042403 | . 2 410.297162 | 1000.070267 | 998.844015 | . 3 540.382220 | 999.952251 | 1000.440940 | . 4 546.024553 | 1000.446011 | 1000.338531 | . sns.pairplot(df); . df.head(2) . price feature1 feature2 . 0 461.527929 | 999.787558 | 999.766096 | . 1 548.130011 | 998.861615 | 1001.042403 | . # Convert Pandas to Numpy for Keras from sklearn.model_selection import train_test_split # Features X = df[[&#39;feature1&#39;, &#39;feature2&#39;]].values # Label y = df[&#39;price&#39;].values # Slit X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) . X_train.shape . (700, 2) . y_test.shape . (300,) . # Normalise and Scale data from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() # Fit to Training data scaler.fit(X_train) # Transform Train and test data based on Fit X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) . import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation . # Create Model model = Sequential() model.add(Dense(4, activation=&#39;relu&#39;)) model.add(Dense(4, activation=&#39;relu&#39;)) model.add(Dense(4, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;mse&#39;) . model.fit(X_train, y_train, epochs=100); . Train on 700 samples Epoch 1/100 700/700 [==============================] - 1s 2ms/sample - loss: 256734.2171 Epoch 2/100 700/700 [==============================] - 0s 100us/sample - loss: 256585.3193 Epoch 3/100 700/700 [==============================] - 0s 99us/sample - loss: 256419.2663 Epoch 4/100 700/700 [==============================] - 0s 99us/sample - loss: 256235.6121s - loss: 256104.81 Epoch 5/100 700/700 [==============================] - 0s 90us/sample - loss: 256019.6986 Epoch 6/100 700/700 [==============================] - 0s 137us/sample - loss: 255769.5409 Epoch 7/100 700/700 [==============================] - 0s 120us/sample - loss: 255486.2454 Epoch 8/100 700/700 [==============================] - 0s 100us/sample - loss: 255165.6379 Epoch 9/100 700/700 [==============================] - 0s 135us/sample - loss: 254808.9467 Epoch 10/100 700/700 [==============================] - 0s 120us/sample - loss: 254418.8767 Epoch 11/100 700/700 [==============================] - 0s 117us/sample - loss: 253993.2737 Epoch 12/100 700/700 [==============================] - 0s 108us/sample - loss: 253532.9037 Epoch 13/100 700/700 [==============================] - 0s 127us/sample - loss: 253039.5647 Epoch 14/100 700/700 [==============================] - 0s 120us/sample - loss: 252513.3096 Epoch 15/100 700/700 [==============================] - 0s 116us/sample - loss: 251953.1364 Epoch 16/100 700/700 [==============================] - 0s 99us/sample - loss: 251354.9470 Epoch 17/100 700/700 [==============================] - 0s 123us/sample - loss: 250701.4620 Epoch 18/100 700/700 [==============================] - 0s 112us/sample - loss: 249991.8088 Epoch 19/100 700/700 [==============================] - 0s 120us/sample - loss: 249224.1071 Epoch 20/100 700/700 [==============================] - 0s 118us/sample - loss: 248392.8379 Epoch 21/100 700/700 [==============================] - 0s 131us/sample - loss: 247503.0635 Epoch 22/100 700/700 [==============================] - 0s 103us/sample - loss: 246550.7026 Epoch 23/100 700/700 [==============================] - 0s 93us/sample - loss: 245525.2023 Epoch 24/100 700/700 [==============================] - 0s 114us/sample - loss: 244424.7712 Epoch 25/100 700/700 [==============================] - 0s 120us/sample - loss: 243256.8364 Epoch 26/100 700/700 [==============================] - 0s 114us/sample - loss: 242005.4338 Epoch 27/100 700/700 [==============================] - 0s 124us/sample - loss: 240694.4196 Epoch 28/100 700/700 [==============================] - 0s 104us/sample - loss: 239295.4205 Epoch 29/100 700/700 [==============================] - 0s 119us/sample - loss: 237815.9690 Epoch 30/100 700/700 [==============================] - 0s 117us/sample - loss: 236250.1349 Epoch 31/100 700/700 [==============================] - 0s 123us/sample - loss: 234589.2504 Epoch 32/100 700/700 [==============================] - 0s 91us/sample - loss: 232837.4500 Epoch 33/100 700/700 [==============================] - 0s 123us/sample - loss: 230987.5181 Epoch 34/100 700/700 [==============================] - 0s 112us/sample - loss: 229022.9837 Epoch 35/100 700/700 [==============================] - 0s 117us/sample - loss: 226955.6740 Epoch 36/100 700/700 [==============================] - 0s 118us/sample - loss: 224790.2327 Epoch 37/100 700/700 [==============================] - 0s 127us/sample - loss: 222513.4288 Epoch 38/100 700/700 [==============================] - 0s 120us/sample - loss: 220123.7206 Epoch 39/100 700/700 [==============================] - 0s 111us/sample - loss: 217604.6604 Epoch 40/100 700/700 [==============================] - 0s 104us/sample - loss: 214968.2704 Epoch 41/100 700/700 [==============================] - 0s 120us/sample - loss: 212223.9364 Epoch 42/100 700/700 [==============================] - 0s 116us/sample - loss: 209351.5223 Epoch 43/100 700/700 [==============================] - 0s 119us/sample - loss: 206372.1771 Epoch 44/100 700/700 [==============================] - 0s 117us/sample - loss: 203244.3596 Epoch 45/100 700/700 [==============================] - 0s 124us/sample - loss: 200001.6553 Epoch 46/100 700/700 [==============================] - 0s 133us/sample - loss: 196621.6040 Epoch 47/100 700/700 [==============================] - 0s 119us/sample - loss: 193118.5028 Epoch 48/100 700/700 [==============================] - 0s 135us/sample - loss: 189493.1879 Epoch 49/100 700/700 [==============================] - 0s 146us/sample - loss: 185738.1114 Epoch 50/100 700/700 [==============================] - 0s 127us/sample - loss: 181815.4626 Epoch 51/100 700/700 [==============================] - 0s 139us/sample - loss: 177811.2594 Epoch 52/100 700/700 [==============================] - 0s 124us/sample - loss: 173674.9202 Epoch 53/100 700/700 [==============================] - 0s 132us/sample - loss: 169387.5123 Epoch 54/100 700/700 [==============================] - 0s 149us/sample - loss: 164988.9645 Epoch 55/100 700/700 [==============================] - 0s 134us/sample - loss: 160489.6636 Epoch 56/100 700/700 [==============================] - 0s 123us/sample - loss: 155834.9291 Epoch 57/100 700/700 [==============================] - 0s 113us/sample - loss: 151046.0179 Epoch 58/100 700/700 [==============================] - 0s 118us/sample - loss: 146175.0710 Epoch 59/100 700/700 [==============================] - 0s 125us/sample - loss: 141198.4020 Epoch 60/100 700/700 [==============================] - 0s 124us/sample - loss: 136109.4471 Epoch 61/100 700/700 [==============================] - 0s 124us/sample - loss: 130940.7521 Epoch 62/100 700/700 [==============================] - 0s 135us/sample - loss: 125713.8463 Epoch 63/100 700/700 [==============================] - 0s 108us/sample - loss: 120369.5925 Epoch 64/100 700/700 [==============================] - 0s 116us/sample - loss: 114954.3468 Epoch 65/100 700/700 [==============================] - 0s 120us/sample - loss: 109446.0565 Epoch 66/100 700/700 [==============================] - 0s 127us/sample - loss: 103925.0006 Epoch 67/100 700/700 [==============================] - 0s 131us/sample - loss: 98349.1309 Epoch 68/100 700/700 [==============================] - 0s 124us/sample - loss: 92735.8169 Epoch 69/100 700/700 [==============================] - 0s 123us/sample - loss: 87106.1582 Epoch 70/100 700/700 [==============================] - 0s 115us/sample - loss: 81484.0130 Epoch 71/100 700/700 [==============================] - 0s 120us/sample - loss: 75883.2828 Epoch 72/100 700/700 [==============================] - 0s 110us/sample - loss: 70311.0703 Epoch 73/100 700/700 [==============================] - 0s 124us/sample - loss: 64779.8148 Epoch 74/100 700/700 [==============================] - 0s 121us/sample - loss: 59317.3782 Epoch 75/100 700/700 [==============================] - 0s 114us/sample - loss: 53911.8225 Epoch 76/100 700/700 [==============================] - 0s 129us/sample - loss: 48680.9900 Epoch 77/100 700/700 [==============================] - 0s 125us/sample - loss: 43579.7131 Epoch 78/100 700/700 [==============================] - 0s 126us/sample - loss: 38609.0669 Epoch 79/100 700/700 [==============================] - 0s 127us/sample - loss: 33854.9086 Epoch 80/100 700/700 [==============================] - 0s 126us/sample - loss: 29302.7500 Epoch 81/100 700/700 [==============================] - 0s 115us/sample - loss: 24985.3489 Epoch 82/100 700/700 [==============================] - 0s 120us/sample - loss: 20966.3412 Epoch 83/100 700/700 [==============================] - 0s 120us/sample - loss: 17273.1389 Epoch 84/100 700/700 [==============================] - 0s 125us/sample - loss: 13918.6046 Epoch 85/100 700/700 [==============================] - 0s 120us/sample - loss: 10918.2614 Epoch 86/100 700/700 [==============================] - 0s 122us/sample - loss: 8299.2842 Epoch 87/100 700/700 [==============================] - 0s 123us/sample - loss: 6104.3133 Epoch 88/100 700/700 [==============================] - 0s 118us/sample - loss: 4426.2176 Epoch 89/100 700/700 [==============================] - 0s 113us/sample - loss: 3218.5811 Epoch 90/100 700/700 [==============================] - 0s 129us/sample - loss: 2481.1472 Epoch 91/100 700/700 [==============================] - 0s 130us/sample - loss: 2149.1669 Epoch 92/100 700/700 [==============================] - 0s 113us/sample - loss: 2050.6529 Epoch 93/100 700/700 [==============================] - 0s 129us/sample - loss: 2018.1331 Epoch 94/100 700/700 [==============================] - 0s 123us/sample - loss: 1985.4679 Epoch 95/100 700/700 [==============================] - 0s 122us/sample - loss: 1957.8603 Epoch 96/100 700/700 [==============================] - 0s 128us/sample - loss: 1924.4281 Epoch 97/100 700/700 [==============================] - 0s 129us/sample - loss: 1892.2963 Epoch 98/100 700/700 [==============================] - 0s 134us/sample - loss: 1862.4811 Epoch 99/100 700/700 [==============================] - 0s 131us/sample - loss: 1831.3929 Epoch 100/100 700/700 [==============================] - 0s 130us/sample - loss: 1805.8847 . Evaluation . model.history.history . {&#39;loss&#39;: [256734.21714285715, 256585.31928571427, 256419.26625, 256235.61214285714, 256019.69857142857, 255769.54089285716, 255486.24535714285, 255165.63785714286, 254808.94669642858, 254418.87669642858, 253993.27366071427, 253532.90375, 253039.56473214287, 252513.30955357142, 251953.13642857142, 251354.94696428571, 250701.46196428573, 249991.80875, 249224.10705357144, 248392.83785714285, 247503.06348214287, 246550.7025892857, 245525.20232142857, 244424.7711607143, 243256.83642857143, 242005.43375, 240694.41955357144, 239295.42053571428, 237815.96901785713, 236250.1349107143, 234589.25044642857, 232837.45, 230987.518125, 229022.9836607143, 226955.67401785715, 224790.23267857142, 222513.4288392857, 220123.720625, 217604.66044642858, 214968.27044642856, 212223.93642857144, 209351.52232142858, 206372.17705357142, 203244.35955357144, 200001.65526785713, 196621.60401785714, 193118.50276785714, 189493.18794642857, 185738.11142857143, 181815.46258928571, 177811.259375, 173674.92017857142, 169387.51232142857, 164988.9644642857, 160489.66357142857, 155834.92910714285, 151046.01794642856, 146175.07098214285, 141198.4019642857, 136109.44705357144, 130940.75205357143, 125713.84629464286, 120369.59245535714, 114954.34678571428, 109446.05647321428, 103925.00058035714, 98349.1309375, 92735.816875, 87106.15816964286, 81484.01299107143, 75883.28279017858, 70311.07026785714, 64779.814776785715, 59317.37823660715, 53911.82245535714, 48680.989955357145, 43579.71308035714, 38609.066852678574, 33854.90863839286, 29302.749988839285, 24985.348939732143, 20966.341183035714, 17273.138872767857, 13918.604561941964, 10918.261395089286, 8299.284235491072, 6104.3133203125, 4426.217624162947, 3218.5810560825894, 2481.1472181919644, 2149.166927315848, 2050.652879464286, 2018.1330650111606, 1985.4679157366072, 1957.8602859933035, 1924.4280615234375, 1892.296298828125, 1862.4810923549107, 1831.3928627232142, 1805.884675641741]} . loss = model.history.history[&#39;loss&#39;] . sns.lineplot(x= range(len(loss)), y=loss) plt.title(&#39;Training Loss per Epoch&#39;); . model.metrics_names . [&#39;loss&#39;] . training_score = model.evaluate(X_train, y_train, verbose=0) test_score = model.evaluate(X_test, y_test, verbose=0) . test_predictions = model.predict(X_test) test_predictions[0:10] . array([[447.78525], [575.96216], [564.137 ], [530.7126 ], [396.1584 ], [545.59674], [484.13968], [490.73978], [517.7349 ], [486.22565]], dtype=float32) . pred_df = pd.DataFrame(y_test, columns=[&#39;Test Y&#39;]) pred_df . Test Y . 0 402.296319 | . 1 624.156198 | . 2 582.455066 | . 3 578.588606 | . 4 371.224104 | . ... ... | . 295 525.704657 | . 296 502.909473 | . 297 612.727910 | . 298 417.569725 | . 299 410.538250 | . 300 rows × 1 columns . test_predictions = pd.Series(test_predictions.reshape(300,)) test_predictions . 0 447.785248 1 575.962158 2 564.137024 3 530.712585 4 396.158386 ... 295 507.563782 296 465.209320 297 571.083984 298 468.226532 299 439.993561 Length: 300, dtype: float32 . test_predictions . 0 447.785248 1 575.962158 2 564.137024 3 530.712585 4 396.158386 ... 295 507.563782 296 465.209320 297 571.083984 298 468.226532 299 439.993561 Length: 300, dtype: float32 . pred_df = pd.concat([pred_df, test_predictions], axis=1) . pred_df.columns = [&#39;Test Y&#39;, &#39;Model Predictions&#39;] . pred_df . Test Y Model Predictions . 0 402.296319 | 447.785248 | . 1 624.156198 | 575.962158 | . 2 582.455066 | 564.137024 | . 3 578.588606 | 530.712585 | . 4 371.224104 | 396.158386 | . ... ... | ... | . 295 525.704657 | 507.563782 | . 296 502.909473 | 465.209320 | . 297 612.727910 | 571.083984 | . 298 417.569725 | 468.226532 | . 299 410.538250 | 439.993561 | . 300 rows × 2 columns . sns.scatterplot(x = &#39;Test Y&#39;, y=&#39;Model Predictions&#39;, data=pred_df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbdac0a6250&gt; . pred_df[&#39;Error&#39;] = pred_df[&#39;Test Y&#39;] - pred_df[&#39;Model Predictions&#39;] . sns.distplot(pred_df[&#39;Error&#39;], bins = 50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbdac0c1750&gt; . from sklearn.metrics import mean_absolute_error, mean_squared_error . mean_absolute_error(pred_df[&#39;Test Y&#39;], pred_df[&#39;Model Predictions&#39;]) . 33.24022047283535 . mean_squared_error(pred_df[&#39;Test Y&#39;], pred_df[&#39;Model Predictions&#39;]) . 1767.7946086328484 . test_score . 1767.7945979817707 . test_score ** 0.5 . 42.045149517890536 . Predict on new data . new_gem = [[998, 1000]] . scaler.transform(new_gem) . array([[0.14117652, 0.53968792]]) . new_gem = scaler.transform(new_gem) . model.predict(new_gem) . array([[429.36853]], dtype=float32) . Save Model . from tensorflow.keras.models import load_model . model.save(&#39;my_model.h5&#39;) . later_model = load_model(&#39;my_model.h5&#39;) . WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer. . Load Model . later_model.predict(new_gem) . array([[429.36853]], dtype=float32) .",
            "url": "https://sams101.github.io/DataScience/keras/ann/tensorflow/regression/python/2020/10/03/Keras-Regression-Simple-Diamond-Prices.html",
            "relUrl": "/keras/ann/tensorflow/regression/python/2020/10/03/Keras-Regression-Simple-Diamond-Prices.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Keras ANNs Intermediate - Regression - House Prices",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . df = pd.read_csv(&#39;DATA/kc_house_data.csv&#39;) . df.head() . id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view ... grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 0 7129300520 | 10/13/2014 | 221900.0 | 3 | 1.00 | 1180 | 5650 | 1.0 | 0 | 0 | ... | 7 | 1180 | 0 | 1955 | 0 | 98178 | 47.5112 | -122.257 | 1340 | 5650 | . 1 6414100192 | 12/9/2014 | 538000.0 | 3 | 2.25 | 2570 | 7242 | 2.0 | 0 | 0 | ... | 7 | 2170 | 400 | 1951 | 1991 | 98125 | 47.7210 | -122.319 | 1690 | 7639 | . 2 5631500400 | 2/25/2015 | 180000.0 | 2 | 1.00 | 770 | 10000 | 1.0 | 0 | 0 | ... | 6 | 770 | 0 | 1933 | 0 | 98028 | 47.7379 | -122.233 | 2720 | 8062 | . 3 2487200875 | 12/9/2014 | 604000.0 | 4 | 3.00 | 1960 | 5000 | 1.0 | 0 | 0 | ... | 7 | 1050 | 910 | 1965 | 0 | 98136 | 47.5208 | -122.393 | 1360 | 5000 | . 4 1954400510 | 2/18/2015 | 510000.0 | 3 | 2.00 | 1680 | 8080 | 1.0 | 0 | 0 | ... | 8 | 1680 | 0 | 1987 | 0 | 98074 | 47.6168 | -122.045 | 1800 | 7503 | . 5 rows × 21 columns . df.isnull().sum() . id 0 date 0 price 0 bedrooms 0 bathrooms 0 sqft_living 0 sqft_lot 0 floors 0 waterfront 0 view 0 condition 0 grade 0 sqft_above 0 sqft_basement 0 yr_built 0 yr_renovated 0 zipcode 0 lat 0 long 0 sqft_living15 0 sqft_lot15 0 dtype: int64 . df.describe().transpose() . count mean std min 25% 50% 75% max . id 21597.0 | 4.580474e+09 | 2.876736e+09 | 1.000102e+06 | 2.123049e+09 | 3.904930e+09 | 7.308900e+09 | 9.900000e+09 | . price 21597.0 | 5.402966e+05 | 3.673681e+05 | 7.800000e+04 | 3.220000e+05 | 4.500000e+05 | 6.450000e+05 | 7.700000e+06 | . bedrooms 21597.0 | 3.373200e+00 | 9.262989e-01 | 1.000000e+00 | 3.000000e+00 | 3.000000e+00 | 4.000000e+00 | 3.300000e+01 | . bathrooms 21597.0 | 2.115826e+00 | 7.689843e-01 | 5.000000e-01 | 1.750000e+00 | 2.250000e+00 | 2.500000e+00 | 8.000000e+00 | . sqft_living 21597.0 | 2.080322e+03 | 9.181061e+02 | 3.700000e+02 | 1.430000e+03 | 1.910000e+03 | 2.550000e+03 | 1.354000e+04 | . sqft_lot 21597.0 | 1.509941e+04 | 4.141264e+04 | 5.200000e+02 | 5.040000e+03 | 7.618000e+03 | 1.068500e+04 | 1.651359e+06 | . floors 21597.0 | 1.494096e+00 | 5.396828e-01 | 1.000000e+00 | 1.000000e+00 | 1.500000e+00 | 2.000000e+00 | 3.500000e+00 | . waterfront 21597.0 | 7.547345e-03 | 8.654900e-02 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 1.000000e+00 | . view 21597.0 | 2.342918e-01 | 7.663898e-01 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 4.000000e+00 | . condition 21597.0 | 3.409825e+00 | 6.505456e-01 | 1.000000e+00 | 3.000000e+00 | 3.000000e+00 | 4.000000e+00 | 5.000000e+00 | . grade 21597.0 | 7.657915e+00 | 1.173200e+00 | 3.000000e+00 | 7.000000e+00 | 7.000000e+00 | 8.000000e+00 | 1.300000e+01 | . sqft_above 21597.0 | 1.788597e+03 | 8.277598e+02 | 3.700000e+02 | 1.190000e+03 | 1.560000e+03 | 2.210000e+03 | 9.410000e+03 | . sqft_basement 21597.0 | 2.917250e+02 | 4.426678e+02 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 5.600000e+02 | 4.820000e+03 | . yr_built 21597.0 | 1.971000e+03 | 2.937523e+01 | 1.900000e+03 | 1.951000e+03 | 1.975000e+03 | 1.997000e+03 | 2.015000e+03 | . yr_renovated 21597.0 | 8.446479e+01 | 4.018214e+02 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 2.015000e+03 | . zipcode 21597.0 | 9.807795e+04 | 5.351307e+01 | 9.800100e+04 | 9.803300e+04 | 9.806500e+04 | 9.811800e+04 | 9.819900e+04 | . lat 21597.0 | 4.756009e+01 | 1.385518e-01 | 4.715590e+01 | 4.747110e+01 | 4.757180e+01 | 4.767800e+01 | 4.777760e+01 | . long 21597.0 | -1.222140e+02 | 1.407235e-01 | -1.225190e+02 | -1.223280e+02 | -1.222310e+02 | -1.221250e+02 | -1.213150e+02 | . sqft_living15 21597.0 | 1.986620e+03 | 6.852305e+02 | 3.990000e+02 | 1.490000e+03 | 1.840000e+03 | 2.360000e+03 | 6.210000e+03 | . sqft_lot15 21597.0 | 1.275828e+04 | 2.727444e+04 | 6.510000e+02 | 5.100000e+03 | 7.620000e+03 | 1.008300e+04 | 8.712000e+05 | . sns.distplot(df[&#39;price&#39;]); . sns.countplot(df[&#39;bedrooms&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad44c38b90&gt; . plt.figure(figsize = (12,8)) sns.scatterplot(x = &#39;price&#39;, y=&#39;sqft_living&#39;, data=df); . sns.boxplot(x = &#39;bedrooms&#39;, y=&#39;price&#39;, data=df); . Geographical Properties . sns.scatterplot(x=&#39;price&#39;, y=&#39;long&#39;, data=df); . sns.scatterplot(x=&#39;price&#39;, y=&#39;lat&#39;, data=df); . plt.figure(figsize=(12,8)) sns.scatterplot(x= &#39;long&#39;, y=&#39;lat&#39;, data=df, hue=&#39;price&#39;); . df.sort_values(&#39;price&#39;, ascending=False).head(3) . id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view ... grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 7245 6762700020 | 10/13/2014 | 7700000.0 | 6 | 8.00 | 12050 | 27600 | 2.5 | 0 | 3 | ... | 13 | 8570 | 3480 | 1910 | 1987 | 98102 | 47.6298 | -122.323 | 3940 | 8800 | . 3910 9808700762 | 6/11/2014 | 7060000.0 | 5 | 4.50 | 10040 | 37325 | 2.0 | 1 | 2 | ... | 11 | 7680 | 2360 | 1940 | 2001 | 98004 | 47.6500 | -122.214 | 3930 | 25449 | . 9245 9208900037 | 9/19/2014 | 6890000.0 | 6 | 7.75 | 9890 | 31374 | 2.0 | 0 | 4 | ... | 13 | 8860 | 1030 | 2001 | 0 | 98039 | 47.6305 | -122.240 | 4540 | 42730 | . 3 rows × 21 columns . len(df) * 0.01 . 215.97 . non_top_1_perc = df.sort_values(&#39;price&#39;, ascending=False).iloc[216:] . plt.figure(figsize=(12,8)) sns.scatterplot(x=&#39;long&#39;, y=&#39;lat&#39;, data=non_top_1_perc, hue=&#39;price&#39;, palette=&#39;RdYlGn&#39;, edgecolor=None, alpha=0.2); . sns.boxplot(x=&#39;waterfront&#39;, y=&#39;price&#39;, data=df); . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 21597 entries, 0 to 21596 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 id 21597 non-null int64 1 date 21597 non-null object 2 price 21597 non-null float64 3 bedrooms 21597 non-null int64 4 bathrooms 21597 non-null float64 5 sqft_living 21597 non-null int64 6 sqft_lot 21597 non-null int64 7 floors 21597 non-null float64 8 waterfront 21597 non-null int64 9 view 21597 non-null int64 10 condition 21597 non-null int64 11 grade 21597 non-null int64 12 sqft_above 21597 non-null int64 13 sqft_basement 21597 non-null int64 14 yr_built 21597 non-null int64 15 yr_renovated 21597 non-null int64 16 zipcode 21597 non-null int64 17 lat 21597 non-null float64 18 long 21597 non-null float64 19 sqft_living15 21597 non-null int64 20 sqft_lot15 21597 non-null int64 dtypes: float64(5), int64(15), object(1) memory usage: 3.5+ MB . df = df.drop(&#39;id&#39;, axis=1) df.head(3) . date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 0 10/13/2014 | 221900.0 | 3 | 1.00 | 1180 | 5650 | 1.0 | 0 | 0 | 3 | 7 | 1180 | 0 | 1955 | 0 | 98178 | 47.5112 | -122.257 | 1340 | 5650 | . 1 12/9/2014 | 538000.0 | 3 | 2.25 | 2570 | 7242 | 2.0 | 0 | 0 | 3 | 7 | 2170 | 400 | 1951 | 1991 | 98125 | 47.7210 | -122.319 | 1690 | 7639 | . 2 2/25/2015 | 180000.0 | 2 | 1.00 | 770 | 10000 | 1.0 | 0 | 0 | 3 | 6 | 770 | 0 | 1933 | 0 | 98028 | 47.7379 | -122.233 | 2720 | 8062 | . Feature Engineering . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;]) . df[&#39;month&#39;] = df[&#39;date&#39;].apply(lambda date:date.month) df[&#39;year&#39;] = df[&#39;date&#39;].apply(lambda date:date.year) . sns.boxplot(x=&#39;year&#39;, y=&#39;price&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad4445b250&gt; . sns.boxplot(x=&#39;month&#39;, y=&#39;price&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad44d52c50&gt; . df.groupby(&#39;month&#39;).mean()[&#39;price&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad45833510&gt; . df.groupby(&#39;year&#39;).mean()[&#39;price&#39;].plot(); . df = df.drop(&#39;date&#39;, axis=1) . df.columns . Index([&#39;price&#39;, &#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft_living&#39;, &#39;sqft_lot&#39;, &#39;floors&#39;, &#39;waterfront&#39;, &#39;view&#39;, &#39;condition&#39;, &#39;grade&#39;, &#39;sqft_above&#39;, &#39;sqft_basement&#39;, &#39;yr_built&#39;, &#39;yr_renovated&#39;, &#39;zipcode&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;sqft_living15&#39;, &#39;sqft_lot15&#39;, &#39;month&#39;, &#39;year&#39;], dtype=&#39;object&#39;) . df[&#39;zipcode&#39;].value_counts() . 98103 602 98038 589 98115 583 98052 574 98117 553 ... 98102 104 98010 100 98024 80 98148 57 98039 50 Name: zipcode, Length: 70, dtype: int64 . df = df.drop(&#39;zipcode&#39;, axis=1) . Create Train and Test splits . # Features X = df.drop(&#39;price&#39;, axis=1).values y = df[&#39;price&#39;].values . from sklearn.model_selection import train_test_split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) . Scaling . from sklearn.preprocessing import MinMaxScaler . scaler = MinMaxScaler() . X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) . X_train.shape . (15117, 19) . X_test.shape . (6480, 19) . Create Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation from tensorflow.keras.optimizers import Adam . model = Sequential() model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dense(1)) model.compile(optimiser=&#39;adam&#39;, loss=&#39;mse&#39;) . Train Model . model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=128, epochs=600, verbose=0) . &lt;tensorflow.python.keras.callbacks.History at 0x7fad34105d50&gt; . losses = pd.DataFrame(model.history.history) . losses.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fad3414f950&gt; . Evaluation on Test Data . from sklearn.metrics import mean_squared_error, mean_absolute_error from sklearn.metrics import explained_variance_score . predictions = model.predict(X_test) . mean_absolute_error(y_test, predictions) . 76821.42352189429 . np.sqrt(mean_squared_error(y_test, predictions)) . 126278.81521004501 . explained_variance_score(y_test, predictions) . 0.8824346233371788 . df[&#39;price&#39;].mean() . 540296.5735055795 . df[&#39;price&#39;].median() . 450000.0 . # Our predictions plt.scatter(y_test, predictions); # Perfect predictions plt.plot(y_test, y_test, &#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fad33458890&gt;] . errors = y_test.reshape(6480, 1) - predictions . sns.distplot(errors); . Predicting on new house . single_house = df.drop(&#39;price&#39;, axis=1).iloc[0] single_house . bedrooms 3.0000 bathrooms 1.0000 sqft_living 1180.0000 sqft_lot 5650.0000 floors 1.0000 waterfront 0.0000 view 0.0000 condition 3.0000 grade 7.0000 sqft_above 1180.0000 sqft_basement 0.0000 yr_built 1955.0000 yr_renovated 0.0000 lat 47.5112 long -122.2570 sqft_living15 1340.0000 sqft_lot15 5650.0000 month 10.0000 year 2014.0000 Name: 0, dtype: float64 . single_house = scaler.transform(single_house.values.reshape(1,19)) . model.predict(single_house) . array([[267865.03]], dtype=float32) . df.iloc[0] . price 221900.0000 bedrooms 3.0000 bathrooms 1.0000 sqft_living 1180.0000 sqft_lot 5650.0000 floors 1.0000 waterfront 0.0000 view 0.0000 condition 3.0000 grade 7.0000 sqft_above 1180.0000 sqft_basement 0.0000 yr_built 1955.0000 yr_renovated 0.0000 lat 47.5112 long -122.2570 sqft_living15 1340.0000 sqft_lot15 5650.0000 month 10.0000 year 2014.0000 Name: 0, dtype: float64 .",
            "url": "https://sams101.github.io/DataScience/keras/tensorflow/ann/regression/python/2020/10/03/Keras-Regression-Complex-House-Prices.html",
            "relUrl": "/keras/tensorflow/ann/regression/python/2020/10/03/Keras-Regression-Complex-House-Prices.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Keras ANNs - Breast Cancer Classification",
            "content": "import pandas as pd import numpy as np . df = pd.read_csv(&#39;DATA/cancer_classification.csv&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 mean radius 569 non-null float64 1 mean texture 569 non-null float64 2 mean perimeter 569 non-null float64 3 mean area 569 non-null float64 4 mean smoothness 569 non-null float64 5 mean compactness 569 non-null float64 6 mean concavity 569 non-null float64 7 mean concave points 569 non-null float64 8 mean symmetry 569 non-null float64 9 mean fractal dimension 569 non-null float64 10 radius error 569 non-null float64 11 texture error 569 non-null float64 12 perimeter error 569 non-null float64 13 area error 569 non-null float64 14 smoothness error 569 non-null float64 15 compactness error 569 non-null float64 16 concavity error 569 non-null float64 17 concave points error 569 non-null float64 18 symmetry error 569 non-null float64 19 fractal dimension error 569 non-null float64 20 worst radius 569 non-null float64 21 worst texture 569 non-null float64 22 worst perimeter 569 non-null float64 23 worst area 569 non-null float64 24 worst smoothness 569 non-null float64 25 worst compactness 569 non-null float64 26 worst concavity 569 non-null float64 27 worst concave points 569 non-null float64 28 worst symmetry 569 non-null float64 29 worst fractal dimension 569 non-null float64 30 benign_0__mal_1 569 non-null int64 dtypes: float64(30), int64(1) memory usage: 137.9 KB . df.describe().transpose() . count mean std min 25% 50% 75% max . mean radius 569.0 | 14.127292 | 3.524049 | 6.981000 | 11.700000 | 13.370000 | 15.780000 | 28.11000 | . mean texture 569.0 | 19.289649 | 4.301036 | 9.710000 | 16.170000 | 18.840000 | 21.800000 | 39.28000 | . mean perimeter 569.0 | 91.969033 | 24.298981 | 43.790000 | 75.170000 | 86.240000 | 104.100000 | 188.50000 | . mean area 569.0 | 654.889104 | 351.914129 | 143.500000 | 420.300000 | 551.100000 | 782.700000 | 2501.00000 | . mean smoothness 569.0 | 0.096360 | 0.014064 | 0.052630 | 0.086370 | 0.095870 | 0.105300 | 0.16340 | . mean compactness 569.0 | 0.104341 | 0.052813 | 0.019380 | 0.064920 | 0.092630 | 0.130400 | 0.34540 | . mean concavity 569.0 | 0.088799 | 0.079720 | 0.000000 | 0.029560 | 0.061540 | 0.130700 | 0.42680 | . mean concave points 569.0 | 0.048919 | 0.038803 | 0.000000 | 0.020310 | 0.033500 | 0.074000 | 0.20120 | . mean symmetry 569.0 | 0.181162 | 0.027414 | 0.106000 | 0.161900 | 0.179200 | 0.195700 | 0.30400 | . mean fractal dimension 569.0 | 0.062798 | 0.007060 | 0.049960 | 0.057700 | 0.061540 | 0.066120 | 0.09744 | . radius error 569.0 | 0.405172 | 0.277313 | 0.111500 | 0.232400 | 0.324200 | 0.478900 | 2.87300 | . texture error 569.0 | 1.216853 | 0.551648 | 0.360200 | 0.833900 | 1.108000 | 1.474000 | 4.88500 | . perimeter error 569.0 | 2.866059 | 2.021855 | 0.757000 | 1.606000 | 2.287000 | 3.357000 | 21.98000 | . area error 569.0 | 40.337079 | 45.491006 | 6.802000 | 17.850000 | 24.530000 | 45.190000 | 542.20000 | . smoothness error 569.0 | 0.007041 | 0.003003 | 0.001713 | 0.005169 | 0.006380 | 0.008146 | 0.03113 | . compactness error 569.0 | 0.025478 | 0.017908 | 0.002252 | 0.013080 | 0.020450 | 0.032450 | 0.13540 | . concavity error 569.0 | 0.031894 | 0.030186 | 0.000000 | 0.015090 | 0.025890 | 0.042050 | 0.39600 | . concave points error 569.0 | 0.011796 | 0.006170 | 0.000000 | 0.007638 | 0.010930 | 0.014710 | 0.05279 | . symmetry error 569.0 | 0.020542 | 0.008266 | 0.007882 | 0.015160 | 0.018730 | 0.023480 | 0.07895 | . fractal dimension error 569.0 | 0.003795 | 0.002646 | 0.000895 | 0.002248 | 0.003187 | 0.004558 | 0.02984 | . worst radius 569.0 | 16.269190 | 4.833242 | 7.930000 | 13.010000 | 14.970000 | 18.790000 | 36.04000 | . worst texture 569.0 | 25.677223 | 6.146258 | 12.020000 | 21.080000 | 25.410000 | 29.720000 | 49.54000 | . worst perimeter 569.0 | 107.261213 | 33.602542 | 50.410000 | 84.110000 | 97.660000 | 125.400000 | 251.20000 | . worst area 569.0 | 880.583128 | 569.356993 | 185.200000 | 515.300000 | 686.500000 | 1084.000000 | 4254.00000 | . worst smoothness 569.0 | 0.132369 | 0.022832 | 0.071170 | 0.116600 | 0.131300 | 0.146000 | 0.22260 | . worst compactness 569.0 | 0.254265 | 0.157336 | 0.027290 | 0.147200 | 0.211900 | 0.339100 | 1.05800 | . worst concavity 569.0 | 0.272188 | 0.208624 | 0.000000 | 0.114500 | 0.226700 | 0.382900 | 1.25200 | . worst concave points 569.0 | 0.114606 | 0.065732 | 0.000000 | 0.064930 | 0.099930 | 0.161400 | 0.29100 | . worst symmetry 569.0 | 0.290076 | 0.061867 | 0.156500 | 0.250400 | 0.282200 | 0.317900 | 0.66380 | . worst fractal dimension 569.0 | 0.083946 | 0.018061 | 0.055040 | 0.071460 | 0.080040 | 0.092080 | 0.20750 | . benign_0__mal_1 569.0 | 0.627417 | 0.483918 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.00000 | . import seaborn as sns import matplotlib.pyplot as plt . sns.countplot(x=&#39;benign_0__mal_1&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7edfc6250&gt; . plt.figure(figsize=(12,8)) sns.heatmap(df.corr()); . df.corr()[&#39;benign_0__mal_1&#39;].sort_values() . worst concave points -0.793566 worst perimeter -0.782914 mean concave points -0.776614 worst radius -0.776454 mean perimeter -0.742636 worst area -0.733825 mean radius -0.730029 mean area -0.708984 mean concavity -0.696360 worst concavity -0.659610 mean compactness -0.596534 worst compactness -0.590998 radius error -0.567134 perimeter error -0.556141 area error -0.548236 worst texture -0.456903 worst smoothness -0.421465 worst symmetry -0.416294 mean texture -0.415185 concave points error -0.408042 mean smoothness -0.358560 mean symmetry -0.330499 worst fractal dimension -0.323872 compactness error -0.292999 concavity error -0.253730 fractal dimension error -0.077972 symmetry error 0.006522 texture error 0.008303 mean fractal dimension 0.012838 smoothness error 0.067016 benign_0__mal_1 1.000000 Name: benign_0__mal_1, dtype: float64 . df.corr()[&#39;benign_0__mal_1&#39;].drop(&#39;benign_0__mal_1&#39;).sort_values().plot(kind=&#39;bar&#39;); . Train Test Split . from sklearn.model_selection import train_test_split . X = df.drop(&#39;benign_0__mal_1&#39;, axis=1).values y = df[&#39;benign_0__mal_1&#39;].values . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101) . Scaling Data . from sklearn.preprocessing import MinMaxScaler . scaler = MinMaxScaler() . X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) . Creating the Model . # For a binary classification problem use model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Activation, Dropout . X_train.shape . (426, 30) . https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-net . model = Sequential() model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . Training the Model . https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network . https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch . model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), verbose=0); . model_loss = pd.DataFrame(model.history.history) . model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7d5acba10&gt; . Add Early Stopping . model = Sequential() model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=30, activation=&#39;relu&#39;)) model.add(Dense(units=1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(monitor = &#39;val_loss&#39;, mode = &#39;min&#39;, patience = 25, verbose = 1) . model.fit(x = X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test), callbacks = [early_stop], verbose = 0) . Epoch 00066: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x7fd7ba372050&gt; . model_loss = pd.DataFrame(model.history.history) model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7ba1513d0&gt; . Adding DroupOut Layers . from tensorflow.keras.layers import Dropout . model = Sequential() model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . model.fit(x = X_train, y = y_train, epochs=600, validation_data=(X_test, y_test), verbose = 0, callbacks = [early_stop]) . Epoch 00151: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x7fd7c0ca5510&gt; . model_loss = pd.DataFrame(model.history.history) model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7c1977250&gt; . Model Evaluation . predictions = model.predict_classes(X_test) . from sklearn.metrics import classification_report, confusion_matrix . print(classification_report(y_test, predictions)) . precision recall f1-score support 0 0.93 0.98 0.96 55 1 0.99 0.95 0.97 88 accuracy 0.97 143 macro avg 0.96 0.97 0.96 143 weighted avg 0.97 0.97 0.97 143 . print(confusion_matrix(y_test, predictions)) . [[54 1] [ 4 84]] . Tensorboard . from tensorflow.keras.callbacks import TensorBoard from datetime import datetime . datetime.now().strftime(&#39;%Y-%m-%d--%H%M&#39;) . &#39;2020-10-03--1554&#39; . pwd . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study&#39; . log_directory = &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study&#39; + &#39;/&#39; + &#39;logs/fit&#39; . log_directory . &#39;/Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study/logs/fit&#39; . # Create Tensorboard Callback # OPTIONAL: ADD A TIMESTAMP FOR UNIQUE FOLDER # timestamp = datetime.now().strftime(&quot;%Y-%m-%d--%H%M&quot;) # log_directory = log_directory + &#39;/&#39; + timestamp board = TensorBoard(log_dir = log_directory, histogram_freq = 1, write_graph=True, write_images=True, update_freq=&#39;epoch&#39;, profile_batch=2, embeddings_freq=1) . model = Sequential() model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(30, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;) . model.fit(x = X_train, y = y_train, epochs =600, validation_data= (X_test, y_test), callbacks = [early_stop, board], verbose = 0) . Epoch 00096: early stopping . &lt;tensorflow.python.keras.callbacks.History at 0x7fd7ad557dd0&gt; . model_loss = pd.DataFrame(model.history.history) model_loss.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd7aec6dc10&gt; . print(log_directory) . /Users/samtreacy/OneDrive - TietoEVRY/00_Analysis/Jupyter/Tensorflow_Cert/Summaries_to_Study/logs/fit . Run this code in Terminal to view Tensorboard . Use cd at your command line to change directory to the file path reported back by pwd or your current .py file location. . Then run this code at your command line or terminal . tensorboard --logdir logs/fit .",
            "url": "https://sams101.github.io/DataScience/keras/ann/classification/python/2020/10/03/Keras-Breast-Cancer-Classifification.html",
            "relUrl": "/keras/ann/classification/python/2020/10/03/Keras-Breast-Cancer-Classifification.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Matplotlib and Seaborn Basics",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt . x = [0,1,2] y = [100,200,300] . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7ffd3f86b4d0&gt;] . # add ; to hide matplotlib output text plt.plot(x,y); . Basic Tools . housing = pd.DataFrame({&#39;rooms&#39;:[1,1,2,2,2,3,3,3,], &#39;price&#39;:[100,120, 190, 200,230,310,330,305]}) housing . rooms price . 0 1 | 100 | . 1 1 | 120 | . 2 2 | 190 | . 3 2 | 200 | . 4 2 | 230 | . 5 3 | 310 | . 6 3 | 330 | . 7 3 | 305 | . plt.scatter(housing[&#39;rooms&#39;], housing[&#39;price&#39;]) . &lt;matplotlib.collections.PathCollection at 0x7ffd3ed8a1d0&gt; . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7ffd4031b450&gt;] . plt.plot(x,y, color=&#39;red&#39;) plt.title(&#39;Title&#39;) plt.xlabel(&#39;X Label&#39;) plt.ylabel(&#39;Y Label&#39;) plt.xlim(0,2) plt.ylim(100,300) . (100.0, 300.0) . plt.plot(x,y, color=&#39;red&#39;, marker=&#39;o&#39;, markersize=20, linestyle=&#39;--&#39;) plt.xlim(0,2) plt.ylim(100,300) plt.title(&#39;Title&#39;) plt.xlabel(&#39;X Label&#39;) plt.ylabel(&#39;Y Label&#39;) . Text(0, 0.5, &#39;Y Label&#39;) . Seaborn . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . ls . 01 Numpy.ipynb Excel_Sample.xlsx example.csv 02 Pandas.ipynb Universities.csv output.csv 03 Matplotlib.ipynb african_econ_crises.csv DATA/ bank.csv . df = pd.read_csv(&#39;DATA/heart.csv&#39;) df.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . sns.distplot(df[&#39;age&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd422ff310&gt; . plt.figure(figsize=(12,8)) sns.distplot(df[&#39;age&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd423e3650&gt; . sns.distplot(df[&#39;age&#39;], kde=False); . sns.distplot(df[&#39;age&#39;], kde=False, bins=40, color=&#39;red&#39;) plt.xlim(40,70); . df.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . sns.countplot(x=&#39;sex&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd4324c6d0&gt; . sns.countplot(df[&#39;sex&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd43428dd0&gt; . sns.countplot(x=&#39;target&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd4350cf50&gt; . sns.countplot(x=&#39;cp&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd435e7910&gt; . sns.countplot(x=&#39;cp&#39;, hue=&#39;sex&#39;, data=df); . sns.countplot(x=&#39;cp&#39;, palette=&#39;terrain&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42cf9910&gt; . sns.boxplot(x=&#39;sex&#39;, y=&#39;age&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42a93a10&gt; . sns.boxplot(x=&#39;sex&#39;, y=&#39;age&#39;, hue=&#39;sex&#39;, data=df); . Scatter Plots . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42e99e10&gt; . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, hue=&#39;sex&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd43950c10&gt; . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, hue=&#39;sex&#39;, palette=&#39;Dark2&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42e3bd10&gt; . sns.scatterplot(x=&#39;chol&#39;, y=&#39;trestbps&#39;, size=&#39;age&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffd42e549d0&gt; . iris = pd.read_csv(&#39;DATA/iris.csv&#39;) . iris.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . sns.pairplot(iris); . sns.pairplot(iris, hue=&#39;species&#39;); . cd OneDrive - TietoEVRY/ . /Users/samtreacy/OneDrive - TietoEVRY . cd 00 Analysis/ . /Users/samtreacy/OneDrive - TietoEVRY/00 Analysis . diamonds = pd.read_csv(&#39;DATA/diamonds.csv&#39;) . diamonds[&#39;cut&#39;].unique() . array([&#39;Ideal&#39;, &#39;Premium&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Fair&#39;], dtype=object) . cut_order = list(diamonds[&#39;cut&#39;].unique()) . cut_order . [&#39;Ideal&#39;, &#39;Premium&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Fair&#39;] . plt.figure(figsize=(12,8)) sns.boxplot(x=&#39;cut&#39;, y=&#39;price&#39;, data=diamonds, order=cut_order,palette=&#39;cool&#39; ); .",
            "url": "https://sams101.github.io/DataScience/matplot/seaborn/python/2020/10/02/Matplotlib_Seaborn.html",
            "relUrl": "/matplot/seaborn/python/2020/10/02/Matplotlib_Seaborn.html",
            "date": " • Oct 2, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sams101.github.io/DataScience/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your page/pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sams101.github.io/DataScience/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sams101.github.io/DataScience/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}